Name,also called or related to,Description,Advantages over other metrics,Question answered,Designed for Reproducibility,Type of reproducibility named in included paper(s),Type of Reproducibility investigated,Related iRISE glossary term,Scenario of application,Purpose of Metric,Purpose of Metric - explanaition,Type of metric,Type of assessment,Implementation of Metric,Data Input,Assumptions,Limitations,First mention in,Discussed in,Used in
Bayes Factor: Independent Jeffreys-Zellner-Siow BF test,default BF,"This test compares the null hypothesis that the effect size is zero against an alternative hypothesis that the effect is not zero. Suppose $H_0: \theta = 0$ and $H_1: \theta \sim \mbox{Cauchy}(0, 1)$, then the Bayes factor is defined as $$B_{10} = \frac{f(Y \  | \ H_1)}{f(Y \  | \ H_0)},$$ where $f(Y  \  | \ H_i)$ is the marginal likelihood of the data $Y$ under hypothesis $H_i$ with $i \in \{0, 1\}$. $B_{10}$ higher than 1 indicate support for $H_1$, whereas lower than 1 indicate support for $H_0$. In the replication setting, the Bayes factor is used to test the absence or presence of an effect in the replication study. Note that the Jeffreys-Zellner-Siow prior is a prior that is specifically designed for the t-test / linear regression setting (normal data with unknown mean and variance).",(1) Can quantify the evidence for the null hypothesis (2) Can be specifically useful in detecting effects that are present but much smaller in magnitude than those from the original experiment,"""What is the evidence for the effect being present or absent in light of a replication attempt, given that we know relatively little about the expected effect size beforehand?""",No,Replication,Different data - same analysis,Reproducibility (direct and conceptual replication),Two exchangeable studies: one original and one replication,To quantify (continuous) ,Quantifies support for either hypothesis,A formula,Quantitative and objective,"Unclear implementation and/or hard to implement from scratch, but authors Ready-to-use open source tool: provide R code \href{https://www.josineverhagen.com/?page_id=76}{https://www.josineverhagen.com/?page_id=76}",Results - numbers and tables,,(1) Disregards results from original study,,"\cite{verhagen_bayesian_2014, verhagen_bayesian_2014-1, heirene_call_2021, muradchanian_how_2021}",\cite{wagenmakers_registered_2016}
Bayes Factor: Equality-of-effect-size BF test,,"This test compares the null hypothesis that the effect sizes from two experiments ($o$ and $r$ for original and replication) are equal against an alternative hypothesis that they are not. Suppose $H_0: \theta_o = \theta_r$ and $H_1: \theta_o \neq \theta_r$, then the equality-of-effect-size Bayes factor is defined as $$B_{01} = \frac{f(Y_o, Y_r\  | \ H_0)}{f(Y_o, Y_r \  | \ H_1)},$$ where $f(Y_o, Y_r  \  | \ H_i)$ is the marginal likelihood of the data under hypothesis $H_i$ with $i \in \{0, 1\}$. $B_{01}$ higher than 1 indicate support for $H_0$ and is indicative of a successful replication.",(1) Can quantify the evidence for both hypotheses,"""What is the evidence for the effect size in the replication attempt being equal vs. unequal to the effect size in the original study?""",No,Replication,Different data - same analysis,Reproducibility (direct and conceptual replication),Two exchangeable studies: one original and one replication,To quantify (continuous) ,"Quantifies support for successful replication, null hypothesis",A formula,Quantitative and objective,"Unclear implementation and/or hard to implement from scratch, but Ready-to-use open source tool: authors provide R code \href{https://www.josineverhagen.com/?page_id=76}{https://www.josineverhagen.com/?page_id=76}",Results - numbers and tables,(1) There is one true underlying effect size from which the effect sizes in the original and replication study deviate with a certain variance,"(1) Not useful for a skeptic who has grounds to doubt the findings from an original experiment, because the test ignores the possibility that the effect is absent.",\cite{bayarri_bayesian_2002},"\cite{verhagen_bayesian_2014, verhagen_bayesian_2014-1}",
Bayes Factor: Fixed-effect meta-analysis BF Test,Meta-analytic BF,"The meta-analytic Bayes factor quantifies the evidence provided by the data of several experiments/studies for the hypothesis that the true effect is present ($H_1$) versus absent ($H_0$): $$B_{10} = \frac{f(Y_1, ..., Y_M \  | \ H_1)}{f(Y_1, ... Y_M \  | \ H_0)},$$  where $f(\dots \ | \ H_i)$ is the marginal likelihood of the data under hypothesis $H_i$ with $i \in \{0, 1\}$. A high $B_{10}$ indicates that the evidence from the pooled data supports $H_1$. ",(1) Can quantify the evidence for the null hypothesis,"""When pooling all data, what is the evidence for the effect being present vs. absent?""",No,Replication,Different data - same analysis,Reproducibility (direct and conceptual replication),A series of exchangeable studies: one original and many replications; many replications without an original,To quantify (continuous) ,Quantifies the evidence from the pooled data in support of either hypothesis,A formula,Quantitative and objective,"Unclear implementation and/or hard to implement from scratch, but Ready-to-use open source tool: authors provide R code \href{https://www.josineverhagen.com/?page_id=76}{https://www.josineverhagen.com/?page_id=76}",Results - numbers and tables,(1) Treats a series of results as exchangeable (2) There is a single true underlying effect size and the observed effect sizes all fluctuate around this true value (3) All experiments are independent,"(1) Not useful for a sceptic who has grounds to doubt the findings from an original experiment, because this test assumes that  that the data from the original experiment and the replication experiment are exchangeable",\cite{rouder_default_2012},"\cite{verhagen_bayesian_2014, verhagen_bayesian_2014-1, muradchanian_how_2021}",\cite{coretta_multidimensional_2023}
Replication Bayes factor,,"The replication Bayes factor tests the proponent's replication hypothesis $H_r: \theta \sim $posterior distribution from original study vs the null hypothesis $H_0: \theta = 0$ of a skeptic who has reason to doubt the presence of an effect: $$B_{r0} = \frac{f(Y_r \  | \ H_r)}{f(Y_r \  | \ H_0)},$$ where $f(Y_r \ | \ H_i)$ is the marginal likelihood of the data under hypothesis $H_i$ with $i \in \{0, 1\}$.The higher the $B_{r0}$ the more evidence for the replication hypothesis.","(1) Summarizes the proponent's beliefs by an entire distribution of effects, not just a point estimate (2) No need for prior distribution specification, only original $t$-statistic and degrees of freedom are used","""What is the evidence for the effect from the replication attempt being comparable to what was found in the original study, or absent?"" - ""Are the replication results more consistent with the original study or with a null effect?"" ",Yes,Replication; Replicability; Repeatability,Different data - same analysis,Reproducibility (direct and conceptual replication),One original and one replication study,To quantify (continuous) ,Quantifies the evidence for the replication hypothesis,A formula,Quantitative and objective,"Unclear implementation and/or hard to implement from scratch, but Ready-to-use open source tool: authors provide R code \href{https://www.josineverhagen.com/?page_id=76}{https://www.josineverhagen.com/?page_id=76}",Results - numbers and tables,(1) $H_r$ has to be specified precisely (2) assumes that the data from both experiments are structurally comparable,"(1) When the original experiment did not find an effect, there is no “proponent” (2) may suffer from the replication-paradox, where a method flags replication success even though the sign of the original and replication study go in the opposite direction",\cite{verhagen_bayesian_2014},"\cite{zwaan_making_2018, dixon_assessing_2020, heirene_call_2021, muradchanian_how_2021, baig_bayesian_2022}",\cite{wagenmakers_registered_2016}
Significance criterion,"vote counting, two-trials rule, regulatory agreement","For an original-replication study pair, replication success is concluded when both original study and replication study find a statistically significant effect, in the same direction. This can be done either with directional two-sided hypothesis tests, or via a one-sided test. For a continuous assessment of reproducibility, $\max(p_o, p_r)$ can be used, where $p_o$ and $p_r$ are the $p$-values from the original and replication, respectively. ",(1) Simplicity,"""Do the original and replication study both find a statistical significant effect in the same direction?""",No,Replication,Different data - same analysis; Same data - different analysis; Different data - different analysis,"Reproducibility (direct and conceptual replication), repeatability, robustness, generalizability, translatability","One original and one replication study; or several original-replication study pairs, or several replications","To classify (binary, yes or no); To quantify (continuous)",Classification for an original-replication study pair; Quantification if aggregated over several original-replication study pairs,A formula,Quantitative and objective,Easy to implement,Results - numbers and tables,"(1) Same population, there is a single true underlying effect size","(1) Ignores the magnitude of the effect (2) Not applicable for ""null"" findings \citep{pawel_replication_2024} (3) A difference in significance does not always indicate that the difference is significant \citep{gelman_difference_2006, nieuwenhuis_erroneous_2011} (4) Probability that the method flags replication failure when effects estimates are the same can be quite high",,"\cite{anderson_theres_2016, heirene_call_2021, schauer_evaluation_2021, schauer_reconsidering_2021, bonett_design_2021, fletcher_how_2021, muradchanian_how_2021, steiner_correspondence_2023}","\cite{irvine_law_2018, naudet_data_2018, boyce_eleven_2023, hoogeveen_many-analysts_2023, botvinik-nezer_variability_2020, huntington-klein_influence_2021, errington_investigating_2021, soto_how_2019, schweinsberg_same_2021, balli_interaction_2013, arroyo-araujo_systematic_2022, silberzahn_many_2018, low_comparison_2017, arroyo-araujo_reproducibility_2019, camerer_evaluating_2018, fisar_reproducibility_2024, camerer_evaluating_2016, cheung_registered_2016, open_science_collaboration_estimating_2015, wagenmakers_registered_2016, chang_leveraging_2024, hanousek_rise_2008, cova_estimating_2021, ebersole_many_2016, klein_investigating_2014, brauer_data_2007, wang_emulation_2023, klein_many_2018, amaral_brazilian_2019, breznau_observing_2022, marcoci_predicting_2024}"
Difference in effect size,"Q-statistic, (meta-analytic) Q-test, difference test, Tukey’s post-hoc honest significant difference test","The original and replication effect sizes can be compared by calculating their difference together with its confidence interval. They can further be compared in a significance testing paradigm using the Q-statistic or difference test. Alternatively, when there is data for several original-replication study pairs, a paired t-test and/or Wilcoxon test can be applied on the effect size estimates for the original and replication studies. Tukey’s post-hoc honest significant difference test can be used to answer the question of how many replications produced results that were statistically indistinguishable from one another.",(1) Avoid dichotomous approach to thinking about study outcomes in which an effect is either true or false,"""To which degree do the effects from a replication study mirror the original""",No,Replication; Replicability; Direct replication,Different data - same analysis; Same data - different analysis; Different data - different analysis,"Reproducibility (direct and conceptual replication), repeatability, robustness, generalizability, translatability",One original and one replication study; or several replications (meta-analytic Q-test),"To quantify (continuous); To classify (binary, yes or no)",Classification for an original-replication study pair; Quantification of evidence against the null ,A formula,Quantitative and objective,Easy to implement; Ready-to-use open source tool: code provided on \href{https://supp.apa.org/psycarticles/supplemental/met0000189/met0000189_supp.html}{https://supp.apa.org/psycarticles/supplemental/met0000189/met0000189_supp.html},Results - numbers and tables,(1) All effect sizes are on the same scale; for Q-statistic they are normally distributed (2) Q-statistic defines replication in terms of heterogeneity of effects among a series of $k$ studies in which no particular study is privileged as different from all the others.,"(1) Caution when biases and QRPs might have inflated original effects (2) When applying over several original-replication study pairs, there can be inconsistency in definition of replication, as all findings can fail to replicate, but averaging across findings ignores this. (3) The null hypothesis of the Q test is that the studies replicate, and hypothesis tests cannot prove that the null hypothesis is true. (4) Q-test has low power to detect replication success (5) Evaluation of replication via estimation is not sufficiently sensitive to obtain unambiguous conclusions when there is only a single replication study",\cite{hedges_statistical_2019} (Q-statistic for reproducibility),"\cite{mathur_challenges_2019, hedges_more_2019, schauer_assessing_2020, hedges_design_2021, heirene_call_2021, schauer_evaluation_2021, schauer_reconsidering_2021, bonett_design_2021, fletcher_how_2021, steiner_correspondence_2023}","\cite{irvine_law_2018, milcu_genotypic_2018, naudet_data_2018, hoogeveen_many-analysts_2023, ebersole_many_2020, huntington-klein_influence_2021, errington_investigating_2021, schweinsberg_same_2021, arroyo-araujo_systematic_2022, silberzahn_many_2018, camerer_evaluating_2018, hagger_multilab_2016, cheung_registered_2016, wang_reproducibility_2022, open_science_collaboration_estimating_2015, amini_comparison_2012, wagenmakers_registered_2016, coretta_multidimensional_2023, chang_leveraging_2024, hanousek_rise_2008, ebersole_many_2016, klein_many_2022, brauer_data_2007, wang_emulation_2023, bouwmeester_registered_2017, klein_many_2018, amaral_brazilian_2019, page_reprise_2021, breznau_observing_2022, marcoci_predicting_2024}"
Confidence interval: original effect in replication 95\% CI,Coverage,"For an original-replication study pair, this metric entails a binary check on whether the original effect size is included in the 95\% confidence interval of the replication effect size. When several original-replication study pairs are considered, coverage is calculated as the proportion of pairs in which the original effect was in the CI of the replication.",(1) Avoid dicotomous approach to thinking about study outcomes in which an effect is either true or false,"""Given an original effect size, (what is the probability that) does a repetition of the experiment, with an independent sample of participants, produce(s) a CI that overlaps with the original effect?""",Yes,Replication,Different data - same analysis,Reproducibility (direct and conceptual replication),One original and one replication study; or one original and many replication studies,"To quantify (continuous); To classify (binary, yes or no)",Classification for an original-replication study pair. Quantification possible with more than one replication.,A formula,Quantitative and objective,Easy to implement,Results - numbers and tables,"(1) Same population, there is a single true underlying effect size","(1) Ignores uncertainty in original study (2) Caution when biases, QRPs and low power might have inflated original effects (3) Probability that the method flags replication failure when effects estimates are the same can be quite high",\cite{brandt_replication_2014},"\cite{fletcher_how_2021, schauer_reconsidering_2021}","\cite{errington_investigating_2021, camerer_evaluating_2016, open_science_collaboration_estimating_2015, wagenmakers_registered_2016, cova_estimating_2021}"
Confidence interval: replication effect in original 95\%CI,Capture probability,"For an original-replication study pair, this metric entails a binary check on whether the replication effect size is included in the 95\% confidence interval of the original effect size. When several replication studies are performed the shares of replications in that interval is captured via the capture probability, which is defined as the percentage of replication means that (will) fall within a given original CI.",(1) Avoid dicotomous approach to thinking about study outcomes in which an effect is either true or false,"""Given an effect size and 95\% CI, (what is the probability that) does a repetition of the experiment, with an independent sample of participants, give(s) an effect that falls within the original CI?""",Yes,Replication,Different data - same analysis,Reproducibility (direct and conceptual replication),One original and one replication study; or one original and many replication studies,"To quantify (continuous); To classify (binary, yes or no); To predict ",Classification for an original-replication study pair. Quantification possible with more than one replication.,A formula,Quantitative and objective,Easy to implement,Results - numbers and tables,"(1) Same population, there is a single true underlying effect size",(1) Ignores uncertainty in replication study (2) If original power too low and relatively broad CI covers zero \citep{asendorpf_recommendations_2013} (3) Caution when biases and QRPs might have inflated original effects,\cite{brandt_replication_2014},"\cite{cumming_confidence_2006, heirene_call_2021}","\cite{errington_investigating_2021, chang_leveraging_2024, wang_emulation_2023}"
Prediction interval: replication effect in original 95\% prediction interval,,"Using the findings (effect size and variation) of the original study, and the expected variation of the replication study (linked to its sample size), compute the 95\% prediction interval. This can be used to predict the effect size of the replication study or, for a binary criterion of replication success, check whether the replication effect size is included in the prediction interval. \cite{schauer_reconsidering_2021} further show how the metric based on the prediction interval is related to the Q-test. ",(1) Accounts for some of the heterogeneity in study outcomes (2) Can incorporate the variation in both the original and the replication study,"""Do the findings from the replication study align with a reasonable expectation, given the observed variation in the original study and replication study?"" -  ""Are the replication estimates statistically consistent with the original estimates?""",Yes,Replication; Replicability,Different data - same analysis,Reproducibility (direct and conceptual replication),Original finding only; one original and one replication study; or one original and many replication studies,"To quantify (continuous); To classify (binary, yes or no); To predict ",Classification for an original-replication study pair. Quantification possible with more than one replication; Prediction possible for single original studies,A formula,Quantitative and objective,Easy to implement,Results - numbers and tables; sample size of the replication study,"(1) Same population, there is a single true underlying effect size",(1) Not sensitive to replication failures: a wide prediction interval can lead to large coverage probabilities (2) the test has low power to detect differences between effects,\cite{patil_what_2016},"\cite{heirene_call_2021, schauer_evaluation_2021, schauer_reconsidering_2021, fletcher_how_2021}","\cite{boyce_eleven_2023, errington_investigating_2021, camerer_evaluating_2016}, \cite{amaral_brazilian_2019} checked original effect in 95\% prediction interval of replications"
Small Telescopes,,"Based on the sample size and the statistical test performed in the original study, the effect that the original study has 33\% power to detect, $d_{33}$, is computed. If the effect size of the replication study is significantly different from $d_{33}$, a replication failure is concluded. ",(1) Combines significance testing and comparing effect sizes (2) Involves a smallest effect size of interest,"""Are the replication results consistent with an effect size big enough to have been detectable in the original?""",Yes,Replication; Replicability; Repeatability; Reproducibility,Different data - same analysis,Reproducibility (direct and conceptual replication),One original and one replication study,"To quantify (continuous); To classify (binary, yes or no)","Classification for an original-replication study pair. Quantification of the evidence that the replication effect size is smaller than a ""small effect""",A formula,Quantitative and objective,Ready-to-use open-source tool: R code provided via \href{https://osf.io/adweh/files/osfstorage}{https://osf.io/adweh/files/osfstorage},Results - numbers and tables,(1) Requires a replication samples size roughly 2.5 times the origninal to deterct the $d_{33}$. ,(1) Large sample size requirement might not be feasible (2) Does not directly provide positice evidence for replication success,\cite{simonsohn_small_2015},"\cite{zwaan_making_2018, heirene_call_2021, dixon_assessing_2020, muradchanian_how_2021, costigan_performing_2024}",
Meta-analysis,,"Fixed-effect or random-effects meta-analyses can be used to combine the results from an original and a replication study, or from several replication studies. In the pairwise scenario, a replication is often considered successful if the results of the meta-analysis align with the results of the original study (significance and direction of effect). When several replications are conducted of the same phenomenon, meta-analysis methodology can be used to assess the reproducibility of the finding. To account for potential heterogeneity between studies, random-effects models are used.",(1) Familiar to most researchers,"""Given an original-replication study pair, does the pooled effect align with that of the original study?"" - ""Given a set of replications, is the effect size reproducible across studies?""",No ,Replication,Different data - same analysis; Same data - different analysis; Different data - different analysis,"Reproducibility (direct and conceptual replication), repeatability, robustness, generalizability, translatability",One original and one replication study; or one original and many replication studies; or several replications,"To quantify (continuous); To classify (binary, yes or no)","Classification for an original-replication study pair, or several replications. ",A formula; A statistical model,Quantitative and objective,"Easy to implement: using standard statistical software; specifically for several replications, or MAs in general, see \href{https://github.com/menglix/repMeta}{https://github.com/menglix/repMeta}",Results - numbers and tables,(1) Original and replication study/ies are exchangable (2) Fixed-effect meta-analysis assumes that all studies have the same underlying true effect (3) Effect sizes are normally distributed,(1) For random effect: between study variance difficult to estimate when the number of available studies is limited,,"\cite{fabrigar_conceptualizing_2016, hedges_more_2019, heirene_call_2021, schauer_reconsidering_2021, bonett_design_2021, fletcher_how_2021, muradchanian_how_2021,mcshane_variation_2022}","\cite{milcu_genotypic_2018, ebersole_many_2020, errington_investigating_2021, hagger_multilab_2016, camerer_evaluating_2016, jaric_using_2024, wagenmakers_registered_2016, amaral_brazilian_2019}"
Equivalence testing,TOST (two one-sided tests),"An equivalence range is constructed based on an equivalence margin, or a smallest effect size of interest. When assessing the replication of an original ``null'' (non-significant) finding a successful replication would reject the null hypothesis of an effect being outside the equivalence region. Alternatively, when interested in assessing whether the original and the replication study find consistent or equivalent effects, one can test whether the difference in effect size falls within a region of equivalence. 
",(1) Involves a smallest effect size of interest (2) Allows the assessment of original null findings,"""For the replication of an original null finding, does the replication study find an effect that is equally negligible?""  - ""Are the results from the replication statistically equivalent to the results of the original study?""",No ,Replication,Different data - same analysis,Reproducibility (direct and conceptual replication),One original and one replication study,"To classify (binary, yes or no); To quantify (continuous)",Classification if checking that the replication effect is in equivalence margings. Quantification if combined with TOST $p$-values,A formula,Quantitative with some subjectivity (equivalence margins),Can be implemented in open-source software \citep{lakens_equivalence_2018},Results - numbers and tables,,(1) Needs setting of equivalence margins,,"\cite{anderson_theres_2016, heirene_call_2021, bonett_design_2021, steiner_correspondence_2023, mateu_towards_2024}",
Minimum effect testing,,"Based on the results of the original study, a minimal level of evidence required to support the original study is defined, as a range constituting the null hypothesis. A test is performed to see whether the replication effect size lies within the range ($H_0$) or outside ($H_1$).",(1) Involves a smallest effect size of interest,"""Is the replication effect size significantly different from a minimal effect size of interest, required to support the original study?""",No ,Replication,Different data - same analysis,Reproducibility (direct and conceptual replication),One original and one replication study,"To classify (binary, yes or no)",Classification if checking that the replication effect outside range. Quantification if combined test/$p$-values,A formula,Quantitative with some subjectivity (Smallest effect size of interest),Easy to implement,Results - numbers and tables,,(1) Needs the setting of a smallest effect size of interest,,\cite{heirene_call_2021},\cite{wang_reproducibility_2022}
Causal replication framework,,"The framework formalizes the conditions under which replication success can be expected, and allows for the causal interpretation of replication failures. These conditions are summarized into replication assumptions which are qualitatively or narratively assessed. Replication failure occurs when one or more of the causal replication framework assumptions are violated.",,"""How can a replication failure be interpreted, from a causal perspective""",Yes,Replication; Conceptual replication; Replicability,Different data - same analysis; Different data - different analysis,"Reproducibility (direct and conceptual replication), generalizability, translatability",One original and one replication study; or one original and many replication studies; or several replications,To explain; To interprete,Explanation and interpretation of replication failures,A framework,Qualitative and subjective,"Unclear implementation: the authors give two examples showing how to use the framework via a Table, but as it is very subjective it remains unclear how to fill the table and what conclusions to draw from",Elements of the design of both original and replication studies,(1) Replication design has to allow for the testing of the framewoks assumptions,(1) Very subjective interpretation of assumptions,\cite{steiner_causal_2019},\cite{wong_design-based_2022},
Text-based machine learning model to estimate reproducibility,,"A machine learning model using an ensemble of random forest and logistic regression was trained on data from replication studies. This model can then use a paper's text and meta-data to predict its likelihood of replication, based on the significance criterion.  ",,"""Given the text of an original paper, what is the probability of replication success?""",Yes,Replicability,Different data - same analysis,Reproducibility (direct and conceptual replication),One original study; or several original studies,To predict,Prediction of the replication outcome of an study,An algorithm,Quantitative and objective,"Hard and unclear implementation, and costly to implement, and ""The
data are available upon request""",Text; Some demographics or meta-data,(1) Expertise in machine learning techniques,"(1) So far, only test on ""top-tier"" journal publications (2) Limitations of (generalizability of) training data",\cite{yang_estimating_2020},"\cite{youyou_discipline-wide_2023, nordling_literature_2022}",\cite{alipourfard_systematizing_2024}
Prediction market,,"Based on original results and information on the design of planned replication studies, participants in a prediction market trade contracts on the possible outcome of a replication study. The contracts pay a certain amount of money if the replication is successful. The traded contracts then allow the price to be interpreted as the predicted probability of the outcome occurring.",,"""What do the participants in a prediction market predict as the probability that the original findings will replicate?""",Yes,Reproducibility; Replication,Different data - same analysis,Reproducibility (direct and conceptual replication),One original study with a planned replication; or several original studies with planned replications,"To quantify (continuous); To classify (binary, yes or no); To predict",Mainly to predict the reproducibility of results,A study,Quantitative and subjective,"Clear implementation, but hard to implement",Text; Information on the design of the original and the replication studies,(1) Based on replication success definition being that a replication finds a statistically significant effect in the same direction as the original,(1) Cost and time consuming (2) Replications have to be performed to ensure that the outcome can be traded on,\cite{dreber_using_2015},,"\cite{botvinik-nezer_variability_2020, camerer_evaluating_2016}"
"Presence/Absence of elements ensuring reproducibility,via proxies",Framework for evaluating rigor and reproducibility,"An original paper is checked for the presence or absence of certain design and reporting elements that are crucial for its reproducibility. This is often achieved using checklists or reporting guidelines which summarize the community standards. The elements of theses checklists or guidelines are usually integrated in a study, survey or questionnaire.",,"""Do the design, methods and reporting of the original paper align with community standards of reproducible and transparent research?""",Yes,Reproducibility ,Same data - same analysis; Same data - different analysis; Different data - same analysis; Different data - different analysis,"Proxy, reproducibility (direct and conceptual replication), computational reproducibility, repeatability, robustness, generalizability, translatability",One original study,"To quantify (continuous); To classify (binary, yes or no); To predict","Can be done by calculating shares, or evaluating the reproducibility on a categorical scale",A survey or questionnaire,"Quantitative with subjective element, due to the choice of elements to go in the checklist",Easy to implement,Text; Results - numbers and tables; Results - figures; Some demographics or meta-data,(1) Such systems depend on community standards of a research field,"(1) Might be designed to be as objective as possible, but can remain subjective to a certain degree",,"\cite{gonzalez-barahona_reproducibility_2012, nordling_literature_2022, belbasis_reproducibility_2022, hildebrandt_rigor_2020}",
"Quantified reproducibility assessment, QRA",,"The method is based on the concepts and definitions of metrology. For QRA, the precision of measurements done in replications across varying conditions is assessed. ",(1) Produces degree-of-reproducibility scores that are comparable across multiple replications of different original studies,"""After performing multiple measurements of an object, what is the precision of the measured quantity obtained?""",Yes,Repeatability; Reproducibility ,Same data - same analysis; Different data - same analysis; Same data - different analysis; Different data - different analysis,"Reproducibility (direct), repeatability",One original study and many replication studies,"To quantify (continuous); To classify (binary, yes or no)",,A formula; A statistical model,Quantitative and objective,Ready-to-use open-source tool; Easy to implement; Code and data available: \href{https://github.com/asbelz/coeff-var}{https://github.com/asbelz/coeff-var},Results - numbers and tables,(1) Assumes measured quantity values are normally distributed (2) Needs to specifc a method for computing precision; specified conditions of measurement; and a procedure for carrying out reproducibility assessments.,(1) Designed for studies in natural language (NLP) processing,\cite{belz_quantified_2022},"\cite{nordling_literature_2022, belz_metrological_2022}",
Jaccard similarity coefficient,Coefficient of similarity,"The percent overlap of activation between two fMRI studies ($j$ and $l$) is defined as $$w_{j,l} = \frac{V_{j,l}}{V_j + V_l - V_{j,l}},$$ where $V_j$ and $V_l$ are the number of voxels identified as activated in either experiment and $V_{j,l}$ is the number of voxels identified as activated in both experiments. \cite{wang_replicability_2022} suggest using a measure that is closely related to the Jaccard coefficient to measure reproducibility in omics data analysis.",,"""By what extent do the results of two (or more) fMRI experiments overlap?""",No,Reproducibility; Reliability; Replication,Same data - same analysis,"Computational reproducibility, repeatability",One original study and one replication study; or several replications,To quantify (continuous),Quantification of reproducibility,A formula,Quantitative and objective,Unclear implementation,Results - numbers and tables,,"(1) Specifically designed for fmRI studies, but also omics data analysis",,"\cite{maitra_re-defined_2010, wang_replicability_2022}",\cite{veronese_reproducibility_2021}
Sceptical $p$-value,"versions: nominal sceptical $p$-value, golden sceptical $p$-value, controlled sceptical $p$-value","Replication success is declared if the replication study is in conflict with a sceptical prior that would make the original study non significant. The sceptical $p$-value quantifies the prior-data conflict. \cite{held_new_2020} introduced the nominal $p$-value. Two more re-calibrations have been proposed since. The nominal $p$-value might be too stringent as it needs both original and replication study to be significant at level $\alpha$. With the golden re-calibration it is possible to establish replication success, original and replication study do not both necessarily need to be significant at level $\alpha$, provided that the replication effect estimate does not shrink compared to the original one. The controlled $p$-value was introduced to guarantee overall type I error control at $\alpha^2$ and is closely related to the significance criterion.",(1) Assess replication success quantitatively (2) Golden version can flag replication success if the original or replication $p$-value does not meet the significance threshold. (3) The controlled sceptical $p$-value guarantees exact overall type one error control. (4) the method penalizes shrinkage of replication effect estimate compared to the original one,"""To what extent are the results of a replication study in conflict with the beliefs of a sceptic of the original study?""",Yes,Replication; Replicability,Different data - same analysis,Reproducibility (direct and conceptual replication),One original study and one replication study,"To quantify (continuous); To classify (binary, yes or no)",To classify replication success using the sceptical $p$-value (any versions) and comparing it to a certain significance level $\alpha$. The sceptical $p$-value can also be used as a continuous measure or replication success.,A formula,Quantitative and objective,Ready-to-use open-source tool: ReplicationSuccess R-package,Results - numbers and tables,"(1) normality assumption of effect size (2) golden: for borderline significant original studies, replication success can only be achieved if the replication effect estimate is larger than the original one (3) the original and replication study are assumed to be not exchangeable","(1) exact overall type I error control comes at a certain price: the explicit penalization of small relative effect sizes in the nominal or golden versions of the sceptical $p$-value is lost and replication success may occur even for large shrinkage of the replication effect estimate, if the relative sample size c is large enough.",\cite{held_new_2020},"\cite{muradchanian_how_2021, held_assessment_2022, micheloud_assessing_2023}",
Modified Brinley plot,,"The plot summarizes the results for several replications including a comparison (A vs. B) by plotting the means of one phase (A, baseline) against the mean of the second phase (B, intervention) for each comparison. An identity line (diagonal with intercept = 0, slope = 1) is included to represent the lack of difference between means. A desired post-intervention level and a desired amount of change after introducing the intervention is specified to define an area of the plot in which the dots should fall if they all meet both requirements. The share of points in the area gives the degree of replication.",(1) Allows the representation of the results for several comparisons within participants and across participants on the same plot,"""Given a pre-specified desired effect and multiple replications, what is the share of replications that, represented graphically, achieve the desired effect?""",No,Replication; Consistency,Same data - same analysis; Same data - different analysis; Different data - same analysis; Different data - different analysis,"Reproducibility (direct and conceptual replication), repeatability, robustness, generalizability, translatability",Several replications,"To quantify (continuous); To classify (binary, yes or no); To illustrate","The authors say that the graph can be used to quantify replicability, but mostly  used to decide whether or not the points are in the designated area or not (so dichotomous)",A graph,Quantitative with subjective input needed to define improvement and post intervention level margins,Ready-to-use tool: R shiny app(\href{https://manolov shinyapps.io/Brinley}{https://manolov shinyapps.io/Brinley} without underlying source code,Results - numbers and tables,,(1) Loss of Information about time and variability (2) The quantifications proposed are purely descriptive,\cite{manolov_assessing_2020},\cite{manolov_proposal_2022},
Likelihood-based approach for reproducibility,Likelihood-ratio,"The design of the original study is used to derive an estimate of a theoretically interesting effect size, $d_{\mbox{tie}}$. A likelihood ratio is then calculated to contrast the match of two models to the data from the replication attempt: a model based on the derived $d_{\mbox{tie}}$, and a null model. More specifically, a null model assuming no effect and a replication model that assumes the effect is $d_{\mbox{tie}}$.  The magnitude of the likelihood ratio describes the strength of the evidence in favor of one or the other model. Very large ratios in favor of $d_{\mbox{tie}}$ would be considered strong evidence for replication. Symmetrically, very large ratios in favor of the null model would be strong evidence against replication.",(1) Provides clear inferences concerning replication without the need for large samples (2) Provides a continuous index of strength of evidence for or against replication,"""Given a theoretically interesting effect size derived from the original study, what is the evidence for or against replicating this effect?""",Yes,Replication,Different data - same analysis,Reproducibility (direct and conceptual replication),One original study and one replication study,"To quantify (continuous); To classify (binary, yes or no)","Quantification as the technique provides an effective way to describe the evidence for or against replicating a theoretically interesting effect size. Classification by using a threshold value for ""clear evidence"" on the ratio.",A formula; A statistical model,Quantitative and objective,Ready-to-use open-source tool; Easy to implement: The authors provide R-code from \href{https://osf.io/rdcmu/}{https://osf.io/rdcmu/},"Results - numbers and tables, and sample size of original study","(1) independent and normally distributed data (2) To define the theoretically interesting effect size, the method assumes that “good evidence” corresponds to an adjusted likelihood ratio of 8:1 (3) the original study was sufficiently powered",(1) Currently limited to the comparison of two conditions,\cite{dixon_assessing_2020},,
Bayesian mixture model for reproducibility rate,,"It is a model for the $p$-values from the original results and the replications, in order to assess the reproducibility rate and to investigate whether some characteristics of the studies are associated with how likely they reproduce. In the mixture model each pair of $p$-values (original and replication) comes from a mixture distribution were one component describes the $p$-value behavior under the null hypothesis and the second under the alternative. All included original studies claim a significant result, the weight given to the second component of the mixture can be seen as a reproducibility rate. As such, the model is linked to the significance criterion.",,"""Given the results ($p$-values) from a set of original and replication studies, what is the rate of reproducibility, and how is it related to certain aspects of the experiments?""",Yes,Reproducibility; replication; reliability,Different data - same analysis,Reproducibility (direct and conceptual replication),Several pairs of original and replication studies,To quantify (continuous); To explain,Quantification of reproducibility of a whole replication projects with several study pairs. Explanation of reproducibility possible by adding covariates to the model.,A statistical model,Quantitative and objective,"Hard to implement: Authors mention STAN within R, but no code is provided",Results - numbers and tables,(1) the $p$-value distribution is the same in the original and replication studies; the distribution is supposed to be uniform ,(1) Tested only in a scenario where all originals were significant (2) Closely linked to significance criterion (as based on $p$-values),\cite{pauli_statistical_2019},,
Unified framework for estimating the credibility of published research,,"The unified framework for estimating the credibility of published research examines four fundamental falsifiability-related dimensions: transparency of the methods and data, reproducibility of the results when the same data-processing and analytic decisions are reapplied, robustness of the results to different data-processing and analytic decisions, and reproducibility of the effect. This framework includes a standardized workflow in which the degree to which a finding has survived scrutiny is quantified along these four dimensions. More specifically, for method and data transparency: availability of design details, analytic choices, and underlying data; for analytic reproducibility: ability of reported results to be reproduced by repeating the same data processing and statistical analyses on the original data; for analytic robustness: robustness of results to different data-processing and data-analytic decisions; and for effect reproducibility: ability of the effect to be consistently observed in new samples, at a magnitude similar to that originally reported, when methodologies and conditions similar to those of the original study are used. The framework outlines the steps to investigate these four dimensions.",(1) combines various sources of evidence,"""For a specific published research work, what is the evidence for its credibility measured on four different dimensions: method and data transparency, analytic reproducibility, analytic robustness and effect reproducibility?""",Yes,Analytical reproducibility; Analytical robustness; Replicability; Effect replicability,Same data - same analysis; Same data - different analysis; Different data - same analysis; Different data - different analysis,"Proxy, reproducibility (direct and conceptual replication), computational reproducibility, repeatability, robustness",One original study and many replication studies,To quantify (continuous); To describe,"While the authors claim quantification of reproducibility is possible, the framework is more useful to describe the evidence from various sources",A framework including: A formula; A statistical model; A survey or questionnaire,"Qualitative (Quantitative elements, but the conclusion is a qualitative assessment)",Unclear implementation: authors only mention a prototype implementation,"Results - numbers and tables; Qualitative data, surveys or questionnaires",(1) needs all types of replication studies,,\cite{lebel_unified_2018},,
Reproducibility scale of workflow execution - Tonkaz,,"The metric is based on the idea of evaluating the reproducibility of results using biological feature values (e.g., number of reads, mapping rate, and variant frequency) representing their biological interpretation. The resulting reproducibility scale is a four point scale and goes from ``Fully Reproduced'' to ``Acceptable Differences'' to ``Unacceptable Differences'' to ``Not Reproduced''. The authors implemented an automated system to classify results on this scale. ",(1) does not only focus on execution of a workflow but also on the verification of results,"""Given a certain original research paper with results based on computation, can the workflow to generate the results be executed and verified?""",Yes,Reproducibility,Same data - same analysis; Same data - different analysis; Different data - same analysis; Different data - different analysis,"Proxy, reproducibility (direct and conceptual replication), computational reproducibility, repeatability, robustness",One original study,To classify (on a 4-point scale),"To automatically calculate the reproducibility scale of a file: Tonkaz first checks whether the files are identical using a checksum. If the files are identical, the reproducibility scale concludes ``Fully Reproduced''. If the files are not identical, Tonkaz compares the biological feature values of the files using a threshold value to determine whether the differences are acceptable or not and if they are acceptable, the reproducibility scale conclude ``Acceptable Difference''. If they are unacceptable, the reproducibility scale conclude ``Unacceptable Difference''. The default threshold value is set to 0.05 which can be changed according to the use case. If the file entity exists only in 1 of the 2 RO-Crates, the reproducibility scale value is “Not Reproduced.”",An algorithm,Qualitative and objective,"Ready-to-use open-source tool; Hard to implement: Authors refer to project on \href{https://github.com/sapporo-wes/sapporo-service}{https://github.com/sapporo-wes/sapporo-service}. Authors further suggest some general steps: (i) extract the statistics of biological features from the output, (ii) represent the statistics in a standardized format, and (iii) compare the statistics and report on the reproducibility scale.",Code or software; Original raw data,,"(1) Specific to bio-informatics research (2) Tonkaz has a challenge in the reproducibility of its function, as it is dependent on other systems",\cite{suetake_workflow_2022},,
Mean relative effect size,Percentage difference in effect size,"The mean relative effect size is defined as $\nu = \sum_{j=1}^m \frac{\theta_{2j}/\theta_{1j}}{m},$ where $\theta_{2j}$ and $\theta_{1j}$ are the effect sizes from either the original or the replication study and $m$ is the number of findings that were replicated. This value is usually used to assess by how much the effect size changed from original to replication study. Alternatively, the percentage difference can be used.",,"""What is the average ratio of replication study effects to original study effects?""",No,Replication; Replicability,Different data - same analysis; Same data - same analysis,"Reproducibility (direct and conceptual replication), computational or analytical reproducibility",Several pairs of original and replication studies,To quantify (continuous),"To quantify a proxy, i.e., the closeness of effect size estimates",A formula,Quantitative and objective,Easy to implement,Results - numbers and tables,,(1) Not defined for effects that are zero (2) Standard error is undefined,,\cite{schauer_evaluation_2021},"\cite{ebersole_many_2020, soto_how_2019, kirkby_quantitative_2023, veronese_reproducibility_2021, wang_reproducibility_2022, open_science_collaboration_estimating_2015}"
Correlation between effects,,"Replication is assessed in terms of the linear relationship between effect estimates, including numerically with the Pearson or Spearman correlation as well as visually with scatter-plots. For successful replications the correlation should be close to 1.",,"""Do the replication studies and the original studies produce effects that are correlated?""",No,Replication; Replicability,Different data - same analysis; Same data - same analysis,"Reproducibility (direct and conceptual replication), computational or analytical reproducibility",Several pairs of original and replication studies,To quantify (continuous),To quantify the correlation between original and replication effects,A formula,Quantitative and objective,Easy to implement,Results - numbers and tables,,"(1) There is inconsistency in the definition of replication: High correlation between effect parameters does not mean that they are similar in size (2) Bias: Reported correlation will
be downward biased",,\cite{schauer_evaluation_2021},\cite{wang_reproducibility_2022}
Fragility Index,Fragility quotient,"The fragility index was proposed to quantify the robustness of statistical significance of clinical studies with binary outcomes. It is defined as the minimal event status modifications that can alter statistical significance. If the original study result is statistically significant (with $p(0,0) < \alpha$), the fragility index is defined as $$FI =  \min_{p(f_0, f_1) \geq \alpha} |f_0| + |f_1|,$$ where $f_0$ and $f_1$ are the numbers of non-events changed to events in groups 0 and 1, respectively. If the original study result is non-significant (with $p(0,0) \geq \alpha$), the min is searched for all $f_0$ and $f_1$ with $p(f_0, f_1)< \alpha$.  A smaller value of FI indicates a more fragile results. The FI was extended to meta-analyses and network meta-analyses. One may use the relative measure, fragility quotient (FQ), to compare the multiple studies’ fragility. Specifically, $$FQ = \frac{FI}{n_0 + n_1} \times 100\%$$ where $n_0 + n_1$ is the total sample size of the study. Thus, the FQ represents the minimal percentage change of event status among all participants that can alter the significance (or non-significance), and it ranges within 0 and 10\%.",,"""Given the results of an original study were significant, what is the smallest change in the original data that is needed to deem the results non-significant? and vice-versa for original null results"" - ""How fragile are the original results to small changes in the underlying data?""",Yes,Fragility; Robustness; Reproducibility; Reliability,Same data - different analysis,"Repeatability, robustness",One original study,To quantify (continuous),The method quantifies fragility of results,A formula,Quantitative and objective,Ready-to-use open-source tool: Fragility package in R,Results - numbers and tables; Original raw data,"(1) the FI was originally restricted to RCTs with a balanced design (i.e., a 1:1 treatment allocation ratio), while this restriction could be relaxed, and a more general algorithm of the FI is available for RCTs with an unbalanced design (2) Coding skills: using the “fragility” package requires researchers to possess some coding skills; clinicians without coding training might not directly use this package for assessing the fragility of clinical studies.","(1) the existing literature lacks a guideline or rule of thumb to interpret the magnitude of the FI or FQ (i.e., the extent of fragility) (2) the FI may be highly associated with $p$-value and sample size (3) limitation of presented implementation: current version of the “fragility” package can only deal with clinical studies with binary outcomes ",\cite{walsh_statistical_2014},"\cite{lin_assessing_2022, lin_assessing_2023}",
Externally standardized residuals,,"For each $i = 1, \dots, n$, the replication effect size $i$ is compared to the weighted mean effect size of all replications excluding study $i$ via a standardized difference. These residuals can than inform on a failure to replicate. They tend to be ambiguous about successful replications. This metric is related to the measure of reproducibility of the studies included in a meta-analysis introduced by \cite{xiao_quantifying_2024}.",,"""Is the original study consistent with the replication(s)?"" - ""Are all studies included in a meta-analysis replicable?""",No,Replication,Different data - same analysis; Same data - different analysis,"Reproducibility (direct and conceptual replication), repeatability, robustness, generalizability, translatability",One original study and one replication; or one original study and many replications,To quantify (continuous); To classify (binary),The method quantifies the extent to which initial findings may be incongruous with the replications,A formula,Quantitative and objective,Unclear implementation,Results - numbers and tables,,(1) The test will never be conclusive about replication success (unless they have very high power).,\cite{schauer_assessing_2020},\cite{xiao_quantifying_2024},
Snapshot hybrid,Bayesian meta-analysis,"The method combines both the original and replication effect size to evaluate the common true effect size. It is a hybrid method because it only takes the statistical significance of the original study into account, whereas it considers evidence of the replication study as unbiased. The snapshot hybrid consists of three steps. First, the likelihood of the effect sizes of the original study and replication is calculated conditional on four hypothesized effect sizes (zero, small, medium, and large). Second, the posterior model probabilities of these four effect sizes are calculated using the likelihoods of step 1 and assuming equal prior model probabilities. Equal prior model probabilities are selected by default, because this refers to an uninformative prior distribution for the encompassing model. Third, when desired, the posterior model probabilities can be recalculated for other than equal prior model probabilities.","(1) Statistically combines the original study and replication, while at the same time taking the statistical significance of the original study into account (2) Quantifies the evidence that the true effect size is zero, small, medium or large","""After replicating an original study, what is the evidence for a null, small, medium or large effect?""",Yes,Replication,Different data - same analysis,Reproducibility (direct and conceptual replication),One original study and one replication study,To quantify (continuous),"Quantifies the evidence that true effect size is zero, small, medium or large",A formula; A statistical model ,Quantitative and objective,Ready-to-use open-source tool: Researchers can apply snapshot hybrid with the R function snapshot” in the “puniform” package (package can be installed with the following R code devtools::install_github(“RobbievanAert/puniform”). ,Results - numbers and tables,"(1) the same effect (i.e., fixed effect) has to be underlying the original study and replication (2) effect size in the original study and replication are assumed to be normally distributed (3)  the original study is required to be statistically significant. ",(1) only applicable with significant original studies (2) assumes assumes that the same true effect is underlying the original study and replication (3) two studies are not sufficient to estimate the amount of heterogeneity (4) currently limited to pairwise comparison of one original and one replication study (5) will be biased in in case of QRPs in the original study,\cite{van_aert_bayesian_2017},,
Bayesian Evidence Synthesis,variant: Meta-Analysis Model-based Assessment of replicability (MAMBA),"The approach assumes that multiple studies exist that investigate a common general theory. These studies might be so diverse in design and measurements, that the study-specific informative hypotheses reflecting the common theory can differ. First the evidence for or against the hypothesis of interest in each individual study is quantified. The evidence is then pooled over studies, providing a joint level of support for the general theory. The aggregation uses updated model probabilities, that is, the posterior odds after observing a first data set are used as the prior odds for the second study; and the posterior odds after inclusion of the second study are used as the prior odds for the third study. This process can be repeated for each additional replication study as presented in: $$\left ( \frac{P(H_1 \ | \ Y)}{P(H_2 \ | \ Y)}\right)^N = \frac{P(H_1)}{P(H_2)} \prod_{n=1}^N(B_{12})^n,$$ where $n = 1,\dots, N$ indicates the number of studies and $Y$ is the denotes the data. Note that the prior odds before the first study $P(H_1)/P(H_2)$ is often set to one, reflecting no preference for either hypothesis before any data was observed. A closely linked variant of this is the MAMBA, introduced for replicability for genome data.","(1) Allows to combine studies that are substantially different, but investigate a common general theory","""Given several conceptual replications with substantial diversity in data, design and methods but investigating the same theory, what is the evidence underlying a certain theory of interest?""",No,Replication; Replicability; Conceptual replication,Different data - different analysis,,Several substantially different replications investigating the same theory of interest,To quantify (continuous),Quantification of the evidence for a certain theory of interest,A formula; A statistical model ,Quantitative and objective,Ready-to-use open-source tool: the authors mention several R-packages that can be used for the implementation,Results - numbers and tables,"(1) assumes that multiple studies exist that investigate a common general theory but may be so diverse in design and measurements, that also the study-specific informative hypotheses reflecting this common theory can differ",(1) it is not a data pooling method; aggregation over multiple studies does not increase the power to find support for a hypothesized effect,\cite{klugkist_bayesian_2023},Variant for genome data in \cite{mcguire_model-based_2021},
Design analysis,,"Given that a study was performed that yielded an estimate $d$ with standard error $s$. Then a true effect size $D$ (the value that $d$ would take if observed in a very large sample) has to be considered. The random variable $d^{rep}$ is defined as the estimate that would be observed in a hypothetical replication study with a design identical to that used in the original study. A probability model for $d^{rep}$ then gives the following three summaries:  (1) The power: the probability that the replication $d^{rep}$ is larger (in absolute value) than the critical value that is considered to define “statistical significance” in this analysis; (2) The Type S error rate: the probability that the replicated estimate has the incorrect sign, if it is statistically significantly different from zero; (3) The exaggeration ratio (expected Type M error): the expectation of the absolute value of the estimate divided by the effect size, if statistically significantly different from zero.",(1) allows to put potentially misleading statistically significant results into context,"""Given the results of an original study and an effect of a hypothetical replication study, what is the probability of the estimate being in the wrong direction, and what is the factor by which the magnitude of the effect is overestimated?""",Yes,Replication; Hypothetical replication,Different data - same analysis,,One original study,To quantify (continuous); To analyse,Quantification of the probability of an estimate being in the wrong direction (Type S [sign] error) and the factor by which the magnitude of an effect might be overestimated (Type M [magnitude] error or exaggeration ratio) ,A formula; A statistical model; An analysis,Quantitative and objective,Ready-to-use open-source tool: the authors mention the R function retrodesign(),Results - numbers and tables,(1) assumes that the effect of a hypothetical replication study can be estimated,(1) Challenging to come up with reasonable estimates of plausible effect sizes based on external information,\cite{gelman_beyond_2014},,
Reproducibility Maps,,The fMRI images are colored depending on whether or not the truly active voxels were stongly reproducible or not ,,"""For fMRI research, how many and which of the truly active voxels were strongly reproduced?""",Yes,Reproducibility,Same data - same analysis; Same data - different analysis,,Several replications,To quantify (continuous); To classify (binary),,A graph,Quantitative and Qualitative,Hard to implement,Results - figures: fMRI,,(1) Needs many replications (2) Specific to fMRI research,\cite{liou_bridging_2003},,
Continuously cumulating meta-analytic approach,,"Continuously cumulating meta-analysis (CCMA) uses standard meta-analytic calculations in a continuing fashion after each new replication attempt completes. Instead of simply noting whether each individual replication attempt reached significance, CCMA combines the data from all studies that were completed so far and computes meta-analytic indexes to quantify the evidence ",(1) Combines evidence from subsequent replications in a continuing fashion,"""Given subsequent replications that were performed to date, what is the current evidence for an effect?""",Unsure,Replicability; Replication,Different data - same analysis,,One original study and several replication studies; or several replications,To quantify (continuous),Quantifies the evidence for an effect by combining the data of all replication studies that were completed so far,A formula; A statistical model,Quantitative and objective,Unclear implementation,Results - numbers and tables,,"(1) Potential bias in studies effect sizes and publication bias, will bias and influence the CCMA estimate",\cite{rosenthal_replication_1990},"\cite{braver_continuously_2014, anderson_theres_2016, fletcher_how_2021}",
Correspondence test,,"This measure combines a difference (related to the Q-test) and equivalence test in the same framework. The correspondence test allows for a more nuanced inference regarding replication success or failure based on whether the null hypothesis of either test can or cannot be rejected. The test has four possible outcomes: equivalence if the difference test is non-significant and the equivalence test is significant, difference if the difference test is significant and the equivalence test is non-significant, trivial difference if the difference test is significant and the equivalence test is significant and indeterminacy if the difference test or the equivalence test are significant . ",(1) It is a more severe test than the difference and equivalence tests on their own,"""To what extent does the effect size from the replication study differ or is equivalent to that of the original study?""",Yes,Replicability,Different data - same analysis,,One original study and one replication study,"To classify (binary, yes or no)",Classifies the correspondence of the two studies on a four point scale,A formula,Quantitative and Qualitative,Easy to implement,Results - numbers and tables,(1) Focuses on pairwise replications,,\cite{steiner_correspondence_2023},,
Z-curve,"Exact replication rate, p-curves","The Z-curve methodology is a method for estimating the expected replication rate, which can be defined as the predicted success rate of exact replication studies based on the mean power after selection for significance. An extension was proposed which estimates the expected discovery rate in addition, which is the estimate of a proportion that the reported statistically significant results constitute from all conducted statistical tests and can be used to detect and quantify the amount of selection bias. ",(1) Can work with any data distribution,"""Do all studies combined provide credible evidence for a phenomenon?""",Yes,Replication; Replicability,Different data - same analysis,,Several replications or originals,To quantify (continuous); To predict,Predicts the success rate of exact replication studies. Quantifies the evidence for a phenomenon,A formula; A graph,Quantitative and objective,Ready-to-use open-source tool; Easy to implement,Results - numbers and tables,(1) All studies investigate the same effect size,(1) Problem when included studies use different significance thresholds (2) Potential QRPs might cause mean power to be underestimated by the z-curve (3) Most useful with a large set of studies,"\cite{brunner_estimating_2020} (Z-curve), \cite{simonsohn_p-curve_2014} (P-curve)",\cite{bartos_z-curve_2022},
Cross-validation methods,"Jackknife, bootstrap","Internal cross-validation methodology are used to test result replicability, where the results received in one sub-sample of the raw data can be confirmed in the remaining data. The degree of shrinkage (validity shrinkage) is then estimated using the difference in $R^2$ between the sub-samples providing a theoretical basis to evaluate the reproducibility of result. The closer shrinkage is estimated to be zero, the greater the degree of stability and more confidence in the replicability/generalizability of the results. Alternatively, jackknife and bootstrap validation methods can be used. ",(1) cross-valudation can contribute important information regarding generalizability that is easily obtained,"""To what extent can the stability of a result be trusted, and to what extent can the result be generalized?""",No,Replicability; Results replicability; Generalisability; Internal replicability,Different data - same analysis,,One original study,To quantify (continuous); to predict,To quantify the degree of shrinkage related to reproducibility. To predict results of hypothetical replications,A formula,Quantitative and objective,Ready-to-use closed tool: SPSS syntax provided in \cite{guan_evaluating_2004}; Ready-to-use open-source tool: \href{https://qchelseasong.shinyapps.io/CrossValTutorial/}{https://qchelseasong.shinyapps.io/CrossValTutorial/},Raw original data,,(2) internal replicability analyses can not overcome the limits of the data at hand and the specific experimental procedures that the researcher followed - external analyses would be better (2) cross-validation is not a substitute for replication efforts.,\cite{thompson_pivotal_1994},"\cite{guan_evaluating_2004, song_making_2021}",
"Network Comparison Test, NCT",,This test was proposed to statistically evaluate the similarity of network models.,,"""Given two network structures, how similar are they to each other?""",No,Replicability; Stability; Reliability; Replication,Same data - same analysis; Different data - same analysis,,One original study and one replication study,To quantify (continuous); To classify (binary),"To quantify the degree of similarity between two networks. As it is a test, it can be dichotomized",A formula; A test,Quantitative and objective,Ready-to-use open-source tool: code is provided on \href{https://osf.io/akywf/}{https://osf.io/akywf/} and on \href{https://osf.io/6fk3v/}{https://osf.io/6fk3v/} refering to the bootnet R package,Results - numbers and tables,,"(1) True relationship between variables in the same sample is not known: In the absence of this knowledge, one cannot know for sure if differences in results are due to differences in sample characteristics, or to a flawed method (2) the guidelines for interpreting bootnet results encourage a false sense of confidence in the stability and interpretability of network characteristics (3) specific to network analyses",,"\cite{borsboom_false_2017, forbes_quantifying_2021}",
Leave-one-out error,,"A model is trained on all data without the $i$th data point, and tested on the $i$th data point. The leave-one-out error is then directly related to the average loss or error over all $i$. ",,"""Given a deep learning model, how generalizable are its results?""",No,Generalisability,Different data - same analysis,,One original study,To quantify (continuous); to predict,"To quantify generalization error, and to predict how well the deep learning model would generalize",A formula,Quantitative and objective,Ready-to-use open-source tool: Code available on \href{https://github.com/gregorbachmann/LeaveOneOut}{https://github.com/gregorbachmann/LeaveOneOut},Raw original data,,(1) Current discussion specific to deep learning models,,\cite{bachmann_generalization_2022},
Subjective reproducibility assessment,"Replication standard, assessment of feasibility","The replication teams are surveyed/asked to answer the question ``Did your results replicate the original effect?''. The teams can give a binary answer, or give a more nuanced interpretation on, for example, a Likert scale. Specific fields have specified their own categories for reproducibility assessment, as for example the replication standard in agent-based modeling:  ``numerical identity'', ``distributional equivalence'', and ``relational alignment''. For the reproducibility of simulation studies, agreement between results from the replication studies and the original studies was assessed in a qualitative manner and involved evaluating: whether numerical values from the replication studies were comparable to those in the original studies, whether trends in the results were moving in the same direction, and whether the performance rankings of different simulation scenarios matched those in the original studies \cite{luijken_replicability_2024}.",(1) Allows for the reproducibility assessment of complex designs,"""Does the replication team consider the replication as successful?"" - ""To what extent does the replication team trust in the reproducibility of a finding?""",Yes,Replicability; Replication,Different data - same analysis,,One original study and one replication study,To quantify (continuous); To classify (binary),"To classify into replication success and failure, or quantified on a more fine-grained scale.",A survey or questionnaire,Qualitative and subjective,Easy to implement,Results - numbers and tables; Results - figures; Text,(1) Clear instructions to assessors,(1) Inherently subjective and directly related to replicator expertise,,"\cite{wilensky_making_2007, fletcher_how_2021, luijken_replicability_2024}","\cite{irvine_law_2018, luijken_replicability_2024, naudet_data_2018, boyce_eleven_2023, botvinik-nezer_variability_2020, huntington-klein_influence_2021, van_dongen_multiple_2019, low_comparison_2017, luijken_replicability_2024, fisar_reproducibility_2024, open_science_collaboration_estimating_2015, bastiaansen_time_2020, cova_estimating_2021, chang_is_2022, page_reprise_2021, breznau_observing_2022}"
I squared - $I^2$,Estimation of effect variance,"I squared describes the percentage of total variation across studies (replications) that is due to heterogeneity rather than chance, and is calculated from basic results obtained from a typical meta-analysis: $$I^2 = 100\%\times (Q - \mbox{df})/Q,$$ where $Q$ is Cochran’s heterogeneity statistic and df the degrees of freedom. Any negative values of $I^2$ are set to zero so that it lies between 0 and 100\%. A value of 0\% indicates no observed heterogeneity, and larger values show increasing heterogeneity.",,"""Given a set of replications, to what extent is the total variation across study results due to heterogeneity?"" - ""How consistent are the results across replications?""",No,Consistency; Generalisability; Replication,Different data - same analysis; Different data - different analysis,,Several replications; one original and several replications,To quantify (continuous) ,To quantify (in)consistency in meta-analysis,A formula; A statistical model,Quantitative and objective,Ready-to-use open-source tool: metafor R-package and other tools,Results - numbers and tables,(1) Norrmal distribution of residuals,,\cite{higgins_measuring_2003},\cite{hedges_design_2021},"\cite{hagger_multilab_2016, klein_investigating_2014}"
Credibility analysis,"Reverse-Bayes, probability of credibility, probability of replicating an effect","The analysis of credibility uses the results of a study (specifically the confidence interval) and uses a Reverse-Bayes approach to find the prior that is required to generate credible evidence for the existence of an effect (i.e., a posterior that excludes no effect). The prior is then compared with internal or external evidence to assess if the finding is credible or not. ",(1) allows allows evidence deemed statistically significant/non-significant in the NHST framework to be assessed for its credibility in the Bayesian framework,"""How credible are the results of a study, in a Bayesian framework?""",Yes,Credibility; Replication,Different data - same analysis,,One original study,To quantify (continuous); To classify (binary),"To quantify the credibility of original findings, or to decide whether they are credible or not.",A formula,Quantitative and objective,Ready-to-use open-source tool: R code is provided via \href{https://gitlab.uzh.ch/samuel.pawel/Reverse-Bayes-Code}{https://gitlab.uzh.ch/samuel.pawel/Reverse-Bayes-Code},Results - numbers and tables,"(1) effects assumed to be normal distributed (can be transformed) (2) studies are assumed to be identically designed, having equal variance","(1) the use of prior evidence in credibility assessment: Critics of Bayesian methods frequently claim that the use of such evidence will lead to anarchy, with every clinician reaching different views about the same evidence (2) Normality assumption questionable for small studies",\cite{matthews_methods_2001},"\cite{held_assessment_2019, held_reversebayes_2022}",
"Consistency of original with replications, $P_{\mbox{orig}}$",,This metric is defined as the $p$-value for a null-hypothesis that the effect size of the original study and the effect size pf the replication study (or effect sizes of several replication studies) follow the same distribution.,"(1) account for all relevant sources of statistical uncertainty in many-to-one replication designs, including heterogeneity (2) allow for the assessment of many-to-one replication designs","""To what extent are the replication effect sizes consistent with the effect size of an original study?""",Yes,Consistency; Replication,Different data - same analysis; Different data - different analysis,,One original study and several replication studies,To quantify (continuous) ,"Quantification of consistency of the original study with the replication studies, while accounting for heterogeneity across the population effects",A formula,Quantitative and objective,Ready-to-use open-source tool: R packages Replicate and MetaUtility,Results - numbers and tables,(1) Replications are supposed to yield unbiased estimates (2) Assumes that the population effects are normally distributed (3) replications measure potentially heterogeneous effects,,\cite{mathur_new_2019},"\cite{mathur_challenges_2019, mathur_new_2020}","\cite{boyce_eleven_2023, ebersole_many_2020}"
"Proportion of population effects agreeing in direction with the original, $\hat{P}_{>0}$",,"This metric assesses the strength of evidence of the replication effect sizes going in the same direction as the original effect size, by estimating the proportion of population effects agreeing in direction with the original effect estimate. It can be generalized by ensuring that they do not only agree in direction but are also stronger than a chosen threshold.","(1) account for all relevant sources of statistical uncertainty in many-to-one replication designs, including heterogeneity (2) allow for the assessment of many-to-one replication designs","""To what extent do the replication effect sizes agree with the sign found in the original study?""",Yes,Consistency; Replication,Different data - same analysis; Different data - different analysis,,One original study and several replication studies,To quantify (continuous) ,Quantifies the strenght of evidence,A formula,Quantitative and objective,Ready-to-use open-source tool: R packages Replicate and MetaUtility,Results - numbers and tables,(1) Replications are supposed to yield unbiased estimates,,\cite{mathur_new_2019},"\cite{mathur_challenges_2019, mathur_new_2020}",\cite{ebersole_many_2020}
RepliCATS,,"The process elicits expert predictions about the reproducibility of research. It is based on a modified Delphi technique and includes four steps represented in the acronym IDEA: ‘Investigate’, ‘Discuss’, ‘Estimate’ and ‘Aggregate’. Each individual is provided a scientific claim and the original research paper to read, and provide an estimate of whether or not the claim will replicate (Investigate). They then see the group’s judgments and reasoning, and can interrogate these (Discuss). Following this, each individual provides a second private assessment (Estimate). A mathematical aggregation of the individual estimates is taken as the final assessment (Aggregate).",,"""How reliable do experts believe the claims from an original finding are?""",Yes,Reliability; Replication; Replicability; Generalisability,Different data - same analysis,,One original study,To quantify (continuous); To predict,Predicts the reproducibility/reliability of a claim,A study; A survey or questionnaire,"Quantitative, qualitative and subjective","Clear implementation, but hard to implement","Qualitative data, surveys or questionnaires",(1) based on human-elicitations (2) the IDEA groups are deliberately constructed to be diverse,"(1) Validated for psychology papers only (2) recruitment burden (3) elicitation burden: both the quantity and quality of information provided by experts are thought to decline the longer the elicitation process takes, similar to participant fatigue in surveys and experiments",\cite{fraser_predicting_2023},,\cite{alipourfard_systematizing_2024}
RepeAT - Repeatability Assessment Tool,,"The tool was developed using a multi-phase method to determine components needed for reproducing biomedical data: a literature review generated a framework which was tested and refined. The RepeAT framework now contains 119 unique variables that were grouped into five categories which address different components for reproducible research: research design and aim, database and data collection methods, data mining and data cleaning, data analysis, data sharing and documentation.",,"""Does the presented research align with community standards of reproducible biomedical research, using electronic health records?""",Yes,Reproducibility,Same data - same analysis; Different data - same analysis,,One original study,To quantify (continuous),Quantifies the alignement of the research with community standards,A framework; A study; A survey or questionnaire,Quantitative; Subjective,Unclear implementation,"Results - numbers and tables; Qualitative data, surveys or questionnaires",,(1) This tool has been developed for research using electronic health records,\cite{mcintosh_repeat_2017},,
P interval,,"The p interval, or prediction interval for p, is an interval with a specified chance (usually 80\%) of including the $p$-value given by a replication.",(1) can act as a remedy against the replication fallacy: the false belief that 1-p is the probability a result will replicate or that a repetition of the experiment will give a statistically significant,"""Given the results of an original study, what is the range of $p$-values a replication (following the same design) would lie in with 80\% probability?""",Yes,Replication,Different data - same analysis,,One original study,To predict,To predict the $p$-value of a future replication study,A formula,Quantitative and objective,Clear implementation,Results - numbers and tables,(1) different assumptions give slightly different versions of the p interval,(1) heavily depends on assumptions,\cite{cumming_replication_2008},,
RipetaScore,,"The ripetaScore combines three aspects of trust for a total of 30 points: 1. using the “Trust in Research” criteria it is determined whether a paper is a research paper. Only then will the paper continue to be scored. 2. The paper is then evaluated for the presence of reproducibility quality indicators and it can receive up to 20 points. Another 10 points come from the trust in professionalism quality indicators. For the trust in reproducibility criteria, papers are primarily evaluated with regards to their data/code sharing practices, reporting of methods, and citing software. These criteria are all assessed via natural language processing.",(1) very scalable,"""Given certain trust in research, reproducibility and professionalism quality indicators, how high does a paper score?""",Yes,Reproducibility; Replicability,Same data - same analysis; Different data - same analysis,,One original study,To quantify (continuous),On a 30 point scale,An algorithm,"Quantitative, qualitative and subjective",Ready-to-use closed tool; Hard to implement: based on a service from a company - Ripeta which was purchased by DigitalScience in 2023,Text,(1) Ripeta comes with a cost,(1) Like any NLP model it is limited by the training data,\cite{sumner_ripetascore_2022},,
Bland-Altman Plot,Agreement measures,"When two measures are compared (for example replications and their original studies), the mean difference between the measures and standard deviations of the difference are used to define the limits of agreement. Then the average effect (average of replication and original effect) is plotted against the difference in effect size. The two measures can be used interchangeably if most of the points lie inside the limits of agreement. Other related agreement parameters can be used as well.",,"""Do the effects estimated in several original-replication study pairs agree with each other?"" - ""How good is the agreement between repeated measures/studies?""",No,Agreement; Repeatability,Same data - same analysis; Same data - different analysis; Different data - same analysis; Different data - different analysis,,Several pairs of original and replication studies,To quantify (continuous); To classify (binary),"Quantify the agreement, or decide whether there is agreement between studies",A formula; A graph,Quantitative and objective,"Clear implementation, easy to implement",Results - numbers and tables,(1) Normality assumption of data,,\cite{bland_statistical_1986},\cite{de_vet_when_2006},"\cite{wang_reproducibility_2022, page_reprise_2021}"
Sceptical Bayes Factor,Reverse-Bayes,"The sceptical Bayes factor combines reverse-Bayes analysis with Bayesian hypothesis testing. First, a sceptical prior is determined for the effect size such that the original finding is no longer convincing in terms of Bayes factors. Then, this prior is contrasted to an advocacy prior (the reference posterior of the effect size based on the original study). Replication success is flagged if the replication data favor the advocacy over the sceptical prior at a higher level than the original data favored the sceptical prior over the null hypothesis. The highest level for which replication success would be declared is then the sceptical Bayes factor. ",(1) Combines several notions of reproducibility: both studies are ensured to have sufficient evidence against the null and it penalises effect estimate incompatibility,"""In light of the replication data, at which level of evidence can an advocate of the original study convince a sceptic?""",Yes,Replicability; Replication,Different data - same analysis,,One original study and one replication study,To quantify (continuous); To classify (binary),Quantification using the sceptical Bayes factor directly. Classification possible using a threshold,A formula,Quantitative and objective,Ready-to-use open-source tool: R package BayesRep available on Github,Results - numbers and tables,(1) Normality of effect estimates,"(1) may suffer from the replication-paradox, where a method flags replication success even thought the sign of the original and replication study go in the opposite direction (2) Normality assumption might be violated with small samples",\cite{pawel_sceptical_2022},,
