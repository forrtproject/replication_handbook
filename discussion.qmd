
### **Discussion** {#discussion-1}

#### Defining and Determining Replication Success {#defining-and-determining-replication-success}

There is no strong consensus yet on what constitutes a replication success and some approaches can be biased (e.g., Schauer & Hedges, 2021\) or imprecise (Patil et al., 2016b). Like in classical null hypothesis significance testing (NHST), replication researchers face the trade-off between dichotomizing something that is not dichotomous (success vs. failure) and making a clear decision about the outcome. On the one hand this is a question about statistical choices and their interpretation, namely how to compare original and replication effect sizes (or *p*\-values) and how to interpret differences. On the other hand, it is a more complex question about how to interpret a mixed pattern of results, where some results are consistent across original and replication, while others are not. Here, it is important for replication researchers to specify which effects are of primary interest in their pre-registration, and how they will aggregate results, noting that requiring multiple effects to yield the same result will reduce statistical power.

Below, we briefly present different approaches to assessing replication success. For a review and a computational implementation of these and other replication success criteria, see also Heyard et al., 2025; Muradchanian et al., 2021; Röseler & Wallrich, 2024; Errington et al., 2021, Table 1).

##### Qualitative criteria {#qualitative-criteria}

Researchers often face the difficulty of making an overarching decision on replication success, failure, or inconclusiveness due to the decision relying on a complex interplay of differences between original study and replication, different original and replication results, and vague theories. We term cases where the decision has not yet been formalized *qualitative*. It is inferior to quantitative criteria in that it is not formally reproducible but it is superior to quantitative criteria in that it allows researchers to consider a large number of details.

##### Quantitative Criteria  {#quantitative-criteria}

With quantitative criteria we refer to those that can be automatically computed based on study details such as effect sizes and statistical tests. They are often a combination of whether the replication effect is different from the expected effect under the assumption of the null hypothesis, different from the original effect, and whether the aggregated effect is different from zero. We list different approaches and brief descriptions in Table 3.

**Table 3**

*Quantitative criteria to operationalize replication success*

| Reference | Type | Description |
| :---- | :---- | :---- |
| Brandt et al., 2014 | NHST | *Comparison of replication effect to 0 and to original effect* Success: different from the null (i.e. statistically significant), and similar to the original (i.e. in its 95% confidence interval) or larger and in the same direction  Informative failure to replicate: either not different from null, or in the opposite direction from the original, *and* significantly different from original (i.e. outside its 95% confidence interval) Practical failure to replicate: both significantly different from the null and significantly smaller than the original Inconclusive: neither significantly different from null nor the original |
| Anderson & Maxwell, 2016 | NHST (Bayesian) | *Consider 6 distinct goals To infer an effect* Conduct statistical test on replication effect, and conclude success when it is significant and in same direction as original *To infer a null effect* NHST: Conduct an equivalence test, showing that the hypothesis that the effect is very small is significant (e.g., one-sided *t-*test against *d* \= .1) Bayesian: Test whether posterior is in Region of Practical Equivalence (better for *multivariate* hypotheses) Bayesian: Bayes Factors quantify the relative support for alternative hypothesis and null hypothesis, and can thus provide evidence *for* the null *Quantify the size of an effect* Focus on increasing precision of effect size estimate, primary success criterion is to achieve a confidence interval width that allows for decision-making (e.g. to assess value for money) *To infer an effect across the two studies* Combine original and replication effects in meta-analysis to estimate a more precise and robust effect Accounts for the fact that a non-significant replication can *increase* the strength of evidence *for* an effect when pooled with original NB: Only appropriate when original effect size is deemed trustworthy *To assess whether replication and original are **in**consistent* Construct a confidence interval for the difference between effect sizes If confidence interval excludes 0, replication effect is incompatible with original effect (heterogeneous) *To assess whether replication and original are consistent* Construct a confidence interval for the difference between effect sizes Conduct an equivalence test on the difference in effect sizes to see whether we can conclude that it is small or uncertain  |
| Steiner et al., 2023 | NHST | Test difference and equivalence of original and replication effects Depending on the combination of outcomes, there is equivalence, difference, a trivial difference, or indeterminacy |
| Bonett, 2020 | NHST | Compute confidence intervals for original and replication effect as well as for the difference of the two effects Combination of the three confidence intervals determines the type of evidence across 9 categories (Bonett, 2020, Table 2\) |
| Verhagen and Wagenmakers, 2014 | Bayesian | Test the likelihood of the replication finding between two competing priors: a skeptic hypothesis suggesting and effect size of zero, and a proponent hypothesis, based on the posterior probability from the original study (i.e. a prior centred on the effect size obtained in the original study)  Results in a  weighted-likelihood ratio between the two hypotheses (i.e. a Bayes Factor)  |
| Held et al., 2022 | Bayesian | Establish a "sceptical prior" to be just strong enough to render the original study's finding non-significant, and test the replication effect against it.  Replication success is then declared if the replication study's data conflicts with this sceptical prior, which is equivalent to the relative effect size exceeding a minimum required threshold. |

If researchers are unsure about how to compare results for their replication study with originals, they can also browse the FORRT Replication Database (Röseler et al., 2024\) and look for replication studies from their area of research, though they might note that current practice is highly variable and often not sufficiently justified.  

#### Interpreting Divergent Results (Replication Failures) {#interpreting-divergent-results-(replication-failures)}

When replications succeed, the original claim gains further credence (as long as the methods are sound). However, when replications fail, many explanations and interpretations can be advanced, which need to be carefully considered and discussed in a report. While replication failure can highlight issues with statistical conclusion validity in the original studies (John et al., 2012; Nelson et al., 2018; Simmons et al., 2011), other explanations need to be considered, including issues with internal, external, and construct validity in both original and replication studies (Fabrigar et al., 2020; Vazire et al., 2022). For example, internal validity is threatened when attrition rates differ between experimental conditions in original or replication studies, creating potential confounds in the interpretation of treatment effects (Zhou & Fishbach, 2016). Construct validity is threatened when original or replication studies use unvalidated ad-hoc measures, fail to employ validated manipulations of the target construct, or when differences in sample characteristics between original and replication studies mean that manipulations and measures do not work as intended (Fabrigar et al., 2020; Fiedler et al., 2021; Flake & Fried, 2020). External validity is threatened when original findings do not generalize to the specifics of the replication study due to person and context differences between studies that moderate the effect. Thus, before making statements about the original finding’s robustness and generalizability, replication researchers need to critically discuss potential methodological shortcomings in both original studies and replication attempts that limit statistical conclusion, internal, external, and construct validity. 

##### Hidden Moderator Account {#hidden-moderator-account}

One challenge for replication researchers is  the identification of hidden/unknown confounds that may influence or bias the phenomenon under study. Each study has a set of potential extraneous or unknown moderator variables that is unique to it. These may seem trivial, such as the brightness of an experimental laboratory, or important, such as a cultural difference between the replicating and original studies. Yet even seemingly trivial differences *could* potentially change results. Often statistical and methodological choices are made to circumvent or attenuate these issues. However, for some paradigms, these variables could be unknown to the original researcher (Fiedler, 2011). These are referred to in the literature as unknown moderators, background variables, hidden moderators or fringe variables. While they are always a way to reject unpleasant replication results, they can potentially bias replications, which highlights that a single replication is never entirely conclusive (though it might raise enough doubts that researchers do not see the value in addressing the remaining uncertainty). It should be noted that the same argument *could* be applied to raise doubts about any original study, questioning whether the effect is really due to the hypothesised cause or due to some hidden moderator or background variable. Clearly a skeptic who stops at that level would not be taken very seriously, so that it is important to move conversations about replication failure beyond general suspicion of hidden moderators.

Bargh (2006) suggested that the evidence generated by empirical findings far outweighs the resources of (social) psychology to conceptualize and understand the mechanisms underlying their effects. Therefore, boundary conditions are not easily specified, which can impact both direct and conceptual replication success. Replication failure indicates that the original claim does not generalise to the setting of the replication. Whether that generalises to the setting of the original study needs to be considered in light of theory, and might be a legitimate matter of contention.

#### The Role of Differences for the Interpretation of Findings {#the-role-of-differences-for-the-interpretation-of-findings}

Each replication outcome should be evaluated in the light of its closeness, which is why all deviations with the respective reasons and, if possible, their potential impact on the results should be discussed. Existing theories may help assess whether a deviation should affect the outcomes. For example, most psychological theories are agnostic towards age so that a different distribution of participants’ age will be unproblematic in most cases. Researchers may choose to evaluate replications from both phenomenon-focused / inductive and theory-focused / deductive views. Different types of interpretations are listed in Figure 5 and integrated from previous accounts by Borgstede and Scholz (2021) and Freese and Peterson (2017, Figure 3).

**Figure 5**

*Interpretation of replication outcomes depend on similarity of closeness and results as well as the view (inductive vs. deductive).*

![][image5]

#### Comments from the Original Study’s Authors {#comments-from-the-original-study’s-authors}

If the replication results do not converge with the original results, replication researchers can reach out to the original study’s authors and ask for a comment that they can publish together with the replication report. A template for asking for a comment is in the appendix. Note that some journals (e.g., *Journal of Replications and Comments in Economics*) require such statements at the time of submission.
