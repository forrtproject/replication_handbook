---
title: "Discussion"
---

#### Defining and Determining Replication Success {#defining-and-determining-replication-success}

There is no strong consensus yet on what constitutes a replication success and some approaches can be biased (e.g., Schauer & Hedges, 2021) or imprecise (Patil et al., 2016b). Like in classical null hypothesis significance testing (NHST), replication researchers face the trade-off between dichotomizing something that is not dichotomous (success vs. failure) and making a clear decision about the outcome. On the one hand this is a question about statistical choices and their interpretation, namely how to compare original and replication effect sizes (or *p*-values) and how to interpret differences. On the other hand, it is a more complex question about how to interpret a mixed pattern of results, where some results are consistent across original and replication, while others are not. Here, it is important for replication researchers to specify which effects are of primary interest in their pre-registration, and how they will aggregate results, noting that requiring multiple effects to yield the same result will reduce statistical power.

Below, we briefly present different approaches to assessing replication success. For a review and a computational implementation of these and other replication success criteria, see also Heyard et al., 2025; Muradchanian et al., 2021; Röseler & Wallrich, 2024; Errington et al., 2021, Table 1).

**Table 3**

*Quantitative criteria to operationalize replication success*

```{r}
#| label: success criteria
#| echo: false
#| message: false
#| warning: false

library(knitr)
library(dplyr)

rs <- read.csv("data/metrics_table_heyardetal2025.csv")

# # snippets taken from https://github.com/rachelHey/reproducibility_metrics/blob/main/docs/index.Rmd 
# rs <- rs %>%
#   mutate(
#     References = paste0(
#       ifelse(
#         First.mention.in != "",
#         paste0("First mentioned in ", First.mention.in, ". "),
#         ""
#       ),
#       ifelse(
#         Discussed.in != "",
#         paste0("Discussed in ", Discussed.in, ". "),
#         ""
#       ),
#       ifelse(
#         Used.in != "",
#         paste0("Used in ", Used.in, "."),
#         ""
#       )
#     )
#   ) %>%
#   select(-First.mention.in, -Discussed.in, -Used.in)

# show only most relevant variables
relevant <- c("Name", "Question.answered", "Type.of.Reproducibility.investigated") # , "First.mention.in"

rs <- rs[, relevant]
names(rs) <- gsub("\\.", " ", names(rs)) # replace "." with " " in variable names

knitr::kable(rs, caption = "Success Criteria Table")
```

*Note*. For reference and descriptions see the full table by Heyard et al. (2025, Table 4) also available at [http://rachelheyard.com/reproducibility_metrics/#table](#0). Reused in accordance with the CC-BY license of the published article and the OSF-project ([https://osf.io/sbcy3](#0)).

#### Interpreting Divergent Results (Replication Failures) {#interpreting-divergent-results-(replication-failures)}

When replications succeed, the original claim gains further credence (as long as the methods are sound). However, when replications fail, many explanations and interpretations can be advanced, which need to be carefully considered and discussed in a report. While replication failure can highlight issues with statistical conclusion validity in the original studies (John et al., 2012; Nelson et al., 2018; Simmons et al., 2011), other explanations need to be considered, including issues with internal, external, and construct validity in both original and replication studies (Fabrigar et al., 2020; Vazire et al., 2022). For example, internal validity is threatened when attrition rates differ between experimental conditions in original or replication studies, creating potential confounds in the interpretation of treatment effects (Zhou & Fishbach, 2016). Construct validity is threatened when original or replication studies use unvalidated ad-hoc measures, fail to employ validated manipulations of the target construct, or when differences in sample characteristics between original and replication studies mean that manipulations and measures do not work as intended (Fabrigar et al., 2020; Fiedler et al., 2021; Flake & Fried, 2020). External validity is threatened when original findings do not generalize to the specifics of the replication study due to person and context differences between studies that moderate the effect. Thus, before making statements about the original finding’s robustness and generalizability, replication researchers need to critically discuss potential methodological shortcomings in both original studies and replication attempts that limit statistical conclusion, internal, external, and construct validity.

##### Hidden Moderator Account {#hidden-moderator-account}

One challenge for replication researchers is the identification of hidden/unknown confounds that may influence or bias the phenomenon under study. Each study has a set of potential extraneous or unknown moderator variables that is unique to it. These may seem trivial, such as the brightness of an experimental laboratory, or important, such as a cultural difference between the replicating and original studies. Yet even seemingly trivial differences *could* potentially change results. Often statistical and methodological choices are made to circumvent or attenuate these issues. However, for some paradigms, these variables could be unknown to the original researcher (Fiedler, 2011). These are referred to in the literature as unknown moderators, background variables, hidden moderators or fringe variables. While they are always a way to reject unpleasant replication results, they can potentially bias replications, which highlights that a single replication is never entirely conclusive (though it might raise enough doubts that researchers do not see the value in addressing the remaining uncertainty). It should be noted that the same argument *could* be applied to raise doubts about any original study, questioning whether the effect is really due to the hypothesised cause or due to some hidden moderator or background variable. Clearly a skeptic who stops at that level would not be taken very seriously, so that it is important to move conversations about replication failure beyond general suspicion of hidden moderators.

Bargh (2006) suggested that the evidence generated by empirical findings far outweighs the resources of (social) psychology to conceptualize and understand the mechanisms underlying their effects. Therefore, boundary conditions are not easily specified, which can impact both direct and conceptual replication success. Replication failure indicates that the original claim does not generalise to the setting of the replication. Whether that generalises to the setting of the original study needs to be considered in light of theory, and might be a legitimate matter of contention.

#### The Role of Differences for the Interpretation of Findings {#the-role-of-differences-for-the-interpretation-of-findings}

Each replication outcome should be evaluated in the light of its closeness, which is why all deviations with the respective reasons and, if possible, their potential impact on the results should be discussed. Existing theories may help assess whether a deviation should affect the outcomes. For example, most psychological theories are agnostic towards age so that a different distribution of participants’ age will be unproblematic in most cases. Researchers may choose to evaluate replications from both phenomenon-focused / inductive and theory-focused / deductive views. Different types of interpretations are listed in Figure 5 and integrated from previous accounts by Borgstede and Scholz (2021) and Freese and Peterson (2017, Figure 3).

**Figure 5**

*Interpretation of replication outcomes depend on similarity of closeness and results as well as the view (inductive vs. deductive).*

\![\]\[image5\]

#### Comments from the Original Study’s Authors {#comments-from-the-original-study’s-authors}

If the replication results do not converge with the original results, replication researchers can reach out to the original study’s authors and ask for a comment that they can publish together with the replication report. A template for asking for a comment is in the appendix. Note that some journals (e.g., *Journal of Replications and Comments in Economics*) require such statements at the time of submission.
