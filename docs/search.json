[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Handbook for Reproduction and Replication Studies",
    "section": "",
    "text": "Summary\nThe practice of repeatedly testing published results with the same data (reproduction) or new data (replication) is currently gaining traction in the social sciences, owing to multiple failures to reproduce and replicate published findings. Along with increased skepticism have come guidelines for the repeated testing of hypotheses from various disciplines and fields. This guide aims to enable researchers to conduct high-quality reproductions and replications across social science disciplines. First we summarize recent developments, then provide a comprehensive guide to carrying out reproductions and replications, and finally present an example for how guidance needs to be tailored for specific fields. Our guide covers the entire research process: choosing a target study, deciding between different types of reproductions and replications, planning and running the new study, analyzing the results, discussing outcomes in the light of potential differences, and publishing a report.\nKeywords: replication, repetitive research, reproducibility, meta-science, meta-research, open science, open research, open scholarship\n\n\n\n\nLast update: 2025-08-21",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "1  Background",
    "section": "",
    "text": "“The proof established by the test must have a specific form, namely, repeatability. The issue of the experiment must be a statement of the hypothesis, the conditions of test, and the results, in such form that another experimenter, from the description alone, may be able to repeat the experiment. Nothing is accepted as proof, in psychology or in any other science, which does not conform to this requirement.” – (Dunlap 1926)\nRepeatability is the cornerstone of many sciences: A majority of the scientific progress rests on the successful accumulation of evidence for claims through reproduction and replications to establish robust discoveries. Reproductions and replications, that is repeated testing of a hypothesis with the same (reproduction) or different (replication) data, are necessary.\nCumulative science without repetition is costly. The aim of this guide is to empower researchers to conduct high-quality reproductions and replications and thereby contribute to making their fields of research more cumulative and robust. Issues of replicability have been discussed across many disciplines, such as psychology ((Open Science Collaboration 2015)), economics ((Dreber and Johannesson 2024)), biology ((Errington et al. 2021)), marketing ((Urminsky and Dietvorst 2024)), linguistics ((McManus 2024)), computer science ((Hummel and Manner 2024)) and epidemiology ((Lash, Collin, and Van Dyke 2018)) and the number of replications has been rising sharply (see Figure 1).\nFigure 1\nNumber of replication studies by year of publication based on the FORRT Replication Database (FReD, (Röseler et al. 2024)) based on the version from July 16, 2025. Code to reproduce the figure: https://osf.io/dznrb.\n\nWhile the number of replication and reproduction studies has increased, the overall proportion of them is still very small, with reviews finding yearly replication rates of up to 1% ((Perry, Morris, and Lea 2022)). Moreover, much of the guidance on replications is being developed actively ((Clarke et al. 2024)) and in narrow parts of science, which leads to fragmentation, siloing, and potentially inconsistent information.\nHere we attempt to integrate useful guidelines (e.g., (Block and Kuckertz 2018; Jekel et al. 2020)) into a comprehensive overview that allows diverse fields to profit from each other. In sum, this guide provides information about the entire process of research allowing researchers at all career stages to plan, conduct, and publish reproduction and replication studies. We limit our scope to quantitative research, given that the concept of reproducibility and replicability itself is highly contested among qualitative researchers (see Makel, Plucker, and Hegarty (2012); Cole et al. (2024); Pownall (2022); Bennett (2021)).\n\n\n\n\nBennett, E. A. 2021. “Open Science from a Qualitative, Feminist Perspective: Epistemological Dogmas and a Call for Critical Examination.” Psychology of Women Quarterly 45 (4): 448–56. https://doi.org/10.1177/03616843211036460.\n\n\nBlock, J., and A. Kuckertz. 2018. “Seven Principles of Effective Replication Studies: Strengthening the Evidence Base of Management Research.” Management Review Quarterly 68 (4): 355–59. https://doi.org/10.1007/s11301-018-0149-3.\n\n\nClarke, B., P. Y. (K.) Lee, S. R. Schiavone, M. Rhemtulla, and S. Vazire. 2024. “The Prevalence of Direct Replication Articles in Top-Ranking Psychology Journals.” American Psychologist. https://doi.org/10.1037/amp0001385.\n\n\nCole, N. L., S. Ulpts, A. Bochynska, E. Kormann, M. Good, B. Leitner, and T. Ross-Hellauer. 2024. “Reproducibility and Replicability of Qualitative Research: An Integrative Review of Concepts, Barriers and Enablers.” https://doi.org/10.31222/osf.io/n5zkw_v1.\n\n\nDreber, A., and M. Johannesson. 2024. “A Framework for Evaluating Reproducibility and Replicability in Economics.” Economic Inquiry. https://doi.org/10.1111/ecin.13244.\n\n\nDunlap, K. 1926. “The Experimental Methods of Psychology.” In Psychologies of 1925, edited by C. Murchison, 331–51. Clark University Press. https://doi.org/10.1037/11020-022.\n\n\nErrington, T. M., M. Mathur, C. K. Soderberg, A. Denis, N. Perfito, E. Iorns, and B. A. Nosek. 2021. “Investigating the Replicability of Preclinical Cancer Biology.” eLife 10: e71601. https://doi.org/10.7554/eLife.71601.\n\n\nHummel, T., and J. Manner. 2024. “A Literature Review on Reproducibility Studies in Computer Science.” In Proceedings of the 16th ZEUS Workshop on Services and Their Composition (ZEUS 2024)(CEUR). Vol. 3673.\n\n\nJekel, M., S. Fiedler, R. Allstadt Torras, D. Mischkowski, A. R. Dorrough, and A. Glöckner. 2020. “How to Teach Open Science Principles in the Undergraduate Curriculum—the Hagen Cumulative Science Project.” Psychology Learning & Teaching 19 (1): 91–106. https://doi.org/10.1177/1475725719868149.\n\n\nLash, T. L., L. J. Collin, and M. E. Van Dyke. 2018. “The Replication Crisis in Epidemiology: Snowball, Snow Job, or Winter Solstice?” Current Epidemiology Reports 5: 175–83.\n\n\nMakel, M. C., J. A. Plucker, and B. Hegarty. 2012. “Replications in Psychology Research: How Often Do They Really Occur?” Perspectives on Psychological Science 7 (6): 537–42. https://doi.org/10.1177/1745691612460688.\n\n\nMcManus, K. 2024. “Replication Studies in Second Language Acquisition Research: Definitions, Issues, Resources, and Future Directions: Introduction to the Special Issue.” Studies in Second Language Acquisition 46 (5): 1299–319. https://doi.org/10.1017/S0272263124000652.\n\n\nOpen Science Collaboration. 2015. “Estimating the Reproducibility of Psychological Science.” Science 349 (6251): aac4716. https://doi.org/10.1126/science.aac4716.\n\n\nPerry, T., R. Morris, and R. Lea. 2022. “A Decade of Replication Study in Education? A Mapping Review (2011–2020).” Educational Research and Evaluation 27 (1-2): 12–34. https://doi.org/10.1080/13803611.2021.2022315.\n\n\nPownall, M. 2022. “Is Replication Possible for Qualitative Research?” https://doi.org/10.31234/osf.io/dwxeg.\n\n\nRöseler, L., L. Kaiser, C. Doetsch, N. Klett, C. Seida, A. Schütz, and Y. and Zhang. 2024. “The Replication Database: Documenting the Replicability of Psychological Science.” Journal of Open Psychology Data 12 (1): 8. https://doi.org/10.5334/jopd.101.\n\n\nUrminsky, O., and B. J. Dietvorst. 2024. “Taking the Full Measure: Integrating Replication into Research Practice to Assess Generalizability.” Journal of Consumer Research 51 (1): 157–68. https://doi.org/10.1093/jcr/ucae007.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "understanding.html",
    "href": "understanding.html",
    "title": "2  Understanding Replications and Reproductions",
    "section": "",
    "text": "2.1 Reproduction and Replication\nThe terms reproduction and replication are used in different ways between disciplines; for example, in psychology, studies using different data are commonly referred to as replications and studies using the same data are referred to as reproductions, whereas in other fields, such as computational science or economics, these terms may be used in the opposite manner or treated interchangeably (see (Miłkowski, Hensel, and Hohol 2018; Ankel-Peters, Fiala, and Neubauer 2023)). In this paper, replication is used to refer to efforts involving the analysis of different data, and reproduction to efforts involving the same data. The different data do not necessarily need to be from a different sample but can also constitute distinct (non-overlapping) subsets from the same sample (e.g., incidental or panel data; (Huang and Huang 2024)).\nReproduction and replication should always be considered together and if possible, reproduction should come before replication. This is because, at the early stages of research, reproduction is much more cost efficient; first confirming whether the findings are reproducible can clarify whether a replication is worthwhile. Furthermore, if the research procedure consists of “moving away” from a specific finding in terms of changing the analysis code, materials, and dataset to test its generalizability or boundary conditions, a numerical reproduction (using the same data and same code) is the closest possible repetition of a finding and a useful foundation for further steps. We discuss multiple cases to illustrate the relationship between reproduction and replication in Table 1:\nTable 1\nPossible combinations of reproduction and replication outcomes.\nNote. A similar distinction is made by The Turing Way Community (TheTuringWayCommunity2025?) but uses a less specific terminology for reproductions.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Understanding Replications and Reproductions</span>"
    ]
  },
  {
    "objectID": "understanding.html#reproduction-and-replication",
    "href": "understanding.html#reproduction-and-replication",
    "title": "2  Understanding Replications and Reproductions",
    "section": "",
    "text": "Case\nReproducible?\nReplicable?\nPossible interpretation\n\n\n\n\nA\nYes\nYes\nThe original finding is reproducible and generalizable.\n\n\nB\nYes\nNo\nThe original finding is reproducible but not generalizable.\n\n\nC\nNo\nYes\nThe original finding is not reproducible but replicators could determine a scenario where it holds.\n\n\nD\nNo\nNo\nThe original finding is neither reproducible nor generalizable.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Understanding Replications and Reproductions</span>"
    ]
  },
  {
    "objectID": "understanding.html#outcome",
    "href": "understanding.html#outcome",
    "title": "2  Understanding Replications and Reproductions",
    "section": "2.2 Outcome",
    "text": "2.2 Outcome\nCommon language often conflates outcome and study descriptions: researchers typically use the phrase “has been replicated” to refer to a replication attempt that has corroborated the findings of the original study, whereas “failed to replicate” or “could not be replicated” is used to refer to circumstances where a replication attempt has not corroborated the original results or has led to a different interpretation or conclusions (see also (Patil, Peng, and Leek 2016)).\nIn this article, when we state that a “study was reproduced/replicated” we mean that there has been a replication attempt, irrespective of its outcome. With “replicable” and “reproducible” we express that there was support for the original hypothesis. Note that the outcome of a replication/reproduction study is often not straightforward, but may depend on the success criteria applied. This is discussed in section Defining and Determining Replication Success.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Understanding Replications and Reproductions</span>"
    ]
  },
  {
    "objectID": "understanding.html#types-of-replication",
    "href": "understanding.html#types-of-replication",
    "title": "2  Understanding Replications and Reproductions",
    "section": "2.3 Types of replication",
    "text": "2.3 Types of replication\nWe heavily rely on the typology provided by Hüffmeier et al. (Hüffmeier, Mazei, and Schultze 2016) where different types of replications are defined by the closeness or similarity between original and replication study. Similarity cannot be evaluated without a theory about the concepts involved. For example, the concept of age can differ strongly between replications of historical, psychological, or biological studies, leading to different measures of the concept itself and thus different judgments about the similarity of an object’s age.\nUnder the assumption of a stable world and constant laws or regularities that are investigated by the social, behavioral, and cognitive sciences, a reproduction and replication study’s closeness to an original study is associated with replication ‘success’ (Hüffmeier, Mazei, and Schultze 2016; LeBel et al. 2018) (see the discussion section for an in-depth discussion of success criteria). The argument can be made from two different philosophical perspectives that we call inductive (phenomenon-focused, effects application, bottom-up) and deductive (theory-focused, theory application, top down; e.g., (Calder, Phillips, and Tybout 1981); (Borgstede and Scholz 2021)). From an inductive perspective, a replication that is very similar to an original study should lead to the same result whereas one that differs with respect to any criterion may lead to different results. This is a stance often taken by proponents of findings that failed to replicate (e.g., (Baumeister and Vohs 2016); (Syed 2023)), arguing that characteristics such as time or place are different and can be valid reasons for different results. From a deductive (theory-focused) view, the only changes that matter are those that affect the underlying theory. Consider for example a replication experiment that is identical in every aspect except for the season (summer instead of winter). If the theory that is tested is about color perception, the replication is likely judged to be close to the original study but if it is about participants’ current tea preferences, it is likely judged to be different from the original study in a theoretically relevant aspect.1\nA related dimension of closeness concerns contextual sensitivity—the extent to which the meaning of a questionnaire or the effect of a manipulation depends on time, culture, or population. As Van Bavel et al. (2016) demonstrate, studies on contextually sensitive topics were significantly less likely to replicate successfully in Open Science Collaboration (Open Science Collaboration 2015), even though methodological fidelity was high. This raises important questions about what constitutes a “close” replication: Should a study on celebrity attitudes, for example, use the same examples (which may be outdated and thus psychologically inert), or should it adapt to locally and temporarily salient figures to trigger the same cognitive or emotional responses? In such cases, strict methodological similarity might paradoxically undermine theoretical closeness, and thus the validity of the replication attempt. This tension highlights that procedural fidelity does not always equate to theoretical equivalence—particularly for studies involving social meaning, identity, or temporally anchored norms. LeBel et al. (LeBel et al. 2018) (Figure X) provide a taxonomy for classifying a replication study’s closeness for psychological research.\nFigure 2\n Support for the view that methodological features that are theoretically irrelevant such as the use of text versus image stimuli or the type of sample can have a strong impact on the results is provided by Landy et al. (Landy et al. 2020), who let different groups of researchers test identical hypotheses using different study designs. The groups arrived at entirely different and even opposite conclusions for similar hypotheses. The differences in the study designs were not predicted by the theories involved in the respective studies: A priori, none of the differences (e.g., within- vs. between-subjects design, picture vs. text stimuli) “should” have affected the conclusions. Note that other theories such as demand characteristics (Orne, 2017) could help in these cases. Moreover, this does not disconfirm the deductive perspective but may be a demonstration of the lack of specification of theories - as well as a reminder that statistical choices affect statistical power by changing the variance, and thus standardised effect sizes. In line with deviations from original studies mostly having uncertain consequences, close replications more directly test the credibility of original results, while conceptual replications that vary features of the design are concerned with generalizability.\nNote that Nosek and Errington (Nosek and Errington 2020) define replication as a study “for which any outcome would be considered diagnostic evidence about a claim from prior research”. This can lead to issues when the original claim is not clear on its boundary conditions. Conceptual replications that highlight limitations to the claim made clearly count, e.g. when the original claim was about a universal effect, and the replication shows that it does not hold in a specific country. Conversely, “replications” that go beyond the claim made, and test the transferability of a claim explicitly made about, e.g., maths education to science education may indeed serve to be framed differently, as they do not directly speak to the claim made originally. Where original authors failed to specify the scope of their claim, we would understand that they imply a broad/universally applicable relationship, which any attempts at generalisation help to corroborate or specify.\nIn terms of Schöch (Schöch 2023), who defines an overarching type of repetitive research based on multiple dimensions, replications are concerned with the same question as a previous study, use the same (close replication) or a similar (conceptual replication) method and use different data (otherwise they are reproductions).",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Understanding Replications and Reproductions</span>"
    ]
  },
  {
    "objectID": "understanding.html#types-of-reproduction",
    "href": "understanding.html#types-of-reproduction",
    "title": "2  Understanding Replications and Reproductions",
    "section": "2.4 Types of reproduction",
    "text": "2.4 Types of reproduction\nReproductions can be numerical reproductions, testing whether the same data, code and software lead to the same results, or robustness reproductions, extending the original analysis and exploring the central finding’s limits (Dreber and Johannesson 2024). Most reproductions would include both a numerical reproduction as baseline and then a robustness reproduction, unless the numerical reproduction is not possible due to a lack of code or software.\n\n\n\n\nAnkel-Peters, J., N. Fiala, and F. Neubauer. 2023. “Do Economists Replicate?” Journal of Economic Behavior & Organization 212: 219–32. https://doi.org/10.1016/j.jebo.2023.05.009.\n\n\nBaumeister, R. F., and K. D. Vohs. 2016. “Misguided Effort with Elusive Implications.” Perspectives on Psychological Science 11 (4): 574–75. https://doi.org/10.1177/1745691616652878.\n\n\nBorgstede, M., and M. Scholz. 2021. “Quantitative and Qualitative Approaches to Generalization and Replication–a Representationalist View.” Frontiers in Psychology 12: 605191. https://doi.org/10.3389/fpsyg.2021.605191.\n\n\nCalder, B. J., L. W. Phillips, and A. M. Tybout. 1981. “Designing Research for Application.” Journal of Consumer Research 8 (2): 197–207. https://doi.org/10.1086/208856.\n\n\nDreber, A., and M. Johannesson. 2024. “A Framework for Evaluating Reproducibility and Replicability in Economics.” Economic Inquiry. https://doi.org/10.1111/ecin.13244.\n\n\nHeroux, Michael A., Lorena A. Barba, Manish Parashar, Victoria Stodden, and Michela Taufer. 2018. “Toward a Compatible Reproducibility Taxonomy for Computational and Computing Sciences.” https://doi.org/10.2172/1481626.\n\n\nHuang, F. L., and A. B. Huang. 2024. “Replication Studies Using Secondary or Nonexperimental Datasets.” School Psychology Review, 1–15. https://doi.org/10.1080/2372966X.2024.2346781.\n\n\nHüffmeier, J., J. Mazei, and T. Schultze. 2016. “Reconceptualizing Replication as a Sequence of Different Studies: A Replication Typology.” Journal of Experimental Social Psychology 66: 81–92. https://doi.org/10.1016/j.jesp.2015.09.009.\n\n\nKing, G. 1995. “Replication, Replication.” PS: Political Science & Politics 28 (3): 444–52. https://doi.org/10.2307/420301.\n\n\nKöhler, T., and J. M. Cortina. 2021. “Play It Again, Sam! An Analysis of Constructive Replication in the Organizational Sciences.” Journal of Management 47 (2): 488–518. https://doi.org/10.1177/0149206319843985.\n\n\nLandy, J. F., M. L. Jia, I. L. Ding, D. Viganola, W. Tierney, A. Dreber, and and Crowdsourcing Hypothesis Tests Collaboration. 2020. “Crowdsourcing Hypothesis Tests: Making Transparent How Design Choices Shape Research Results.” Psychological Bulletin 146 (5): 451. https://doi.org/10.1037/bul0000220.\n\n\nLeBel, E. P., R. J. McCarthy, B. D. Earp, M. Elson, and W. Vanpaemel. 2018. “A Unified Framework to Quantify the Credibility of Scientific Findings.” Advances in Methods and Practices in Psychological Science 1 (3): 389–402. https://doi.org/10.1177/2515245918787489.\n\n\nMcManus, K. 2024. “Replication Studies in Second Language Acquisition Research: Definitions, Issues, Resources, and Future Directions: Introduction to the Special Issue.” Studies in Second Language Acquisition 46 (5): 1299–319. https://doi.org/10.1017/S0272263124000652.\n\n\nMiłkowski, M., W. M. Hensel, and M. Hohol. 2018. “Replicability or Reproducibility? On the Replication Crisis in Computational Neuroscience and Sharing Only Relevant Detail.” Journal of Computational Neuroscience 45 (3): 163–72. https://doi.org/10.1007/s10827-018-0702-z.\n\n\nNosek, B. A., and T. M. Errington. 2020. “What Is Replication?” PLoS Biology 18 (3): e3000691. https://doi.org/10.1371/journal.pbio.3000691.\n\n\nOpen Science Collaboration. 2015. “Estimating the Reproducibility of Psychological Science.” Science 349 (6251): aac4716. https://doi.org/10.1126/science.aac4716.\n\n\nPatil, P., R. D. Peng, and J. T. Leek. 2016. “A Statistical Definition for Reproducibility and Replicability.” BioRxiv, 066803. https://doi.org/10.1101/066803.\n\n\nSchmidt, S. 2009. “Shall We Really Do It Again? The Powerful Concept of Replication Is Neglected in the Social Sciences.” Review of General Psychology 13 (2): 90–100. https://doi.org/10.1037/a0015108.\n\n\nSchöch, C. 2023. “Repetitive Research: A Conceptual Space and Terminology of Replication, Reproduction, Revision, Reanalysis, Reinvestigation and Reuse in Digital Humanities.” International Journal of Digital Humanities 5 (2): 373–403. https://doi.org/10.1007/s42803-023-00073-y.\n\n\nSyed, M. 2023. “Replication or Generalizability? How Flexible Inferences Uphold Unfounded Universal Claims,” May. https://doi.org/10.31234/osf.io/znv5r.\n\n\nTsang, E. W., and K. M. Kwan. 1999. “Replication and Theory Development in Organizational Science: A Critical Realist Perspective.” Academy of Management Review 24 (4): 759–80. https://doi.org/10.2307/259353.\n\n\nUrminsky, O., and B. J. Dietvorst. 2024. “Taking the Full Measure: Integrating Replication into Research Practice to Assess Generalizability.” Journal of Consumer Research 51 (1): 157–68. https://doi.org/10.1093/jcr/ucae007.\n\n\nVoelkl, B., R. Heyard, D. Fanelli, K. E. Wever, L. Held, Z. Maniadis, and H. and Würbel. 2025. “Defining Reproducibility.” https://doi.org/10.17605/OSF.IO/BR9SP.\n\n\nVohs, K. D., B. J. Schmeichel, S. Lohmann, Q. F. Gronau, A. J. Finley, S. E. Ainsworth, J. L. Alquist, et al. 2021. “A Multisite Preregistered Paradigmatic Test of the Ego-Depletion Effect.” Psychological Science 32 (10): 1566–81. https://doi.org/10.1177/0956797621989733.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Understanding Replications and Reproductions</span>"
    ]
  },
  {
    "objectID": "understanding.html#footnotes",
    "href": "understanding.html#footnotes",
    "title": "2  Understanding Replications and Reproductions",
    "section": "",
    "text": "On a different note, Vohs et al. (Vohs et al. 2021) published a study that was different from previous studies in that it did not replicate any previous study but was instead designed to be ideal to test the theory and estimate the average effect size and termed it “paradigmatic replication approach”. Given the present terminology, we do not consider this a replication.↩︎",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Understanding Replications and Reproductions</span>"
    ]
  },
  {
    "objectID": "choosing_study.html",
    "href": "choosing_study.html",
    "title": "3  Choosing the Target Study",
    "section": "",
    "text": "3.1 Determining Reproduction and Replication Value\nWhether a target study is “worth reproducing” or “worth replicating” is highly debated and is suggested to depend on several overlapping factors, including value (sometimes also referred to as impact or relevance), uncertainty, and feasibility (Isager et al. 2023). Below, different suggestions for operationalizing these factors are discussed systematically.\nNote that there is also ongoing discussion about whether or not all studies are generally ‘worth replicating’. One perspective is what is worthy of publication is worthy of replication (Feldman 2025) or, on a different note, what is worthy of publication should be worthy of replication—though this perspective is becoming complicated through the rise of influential preprints and a public-review-curate model to publications. Naturally, a public report of a study is necessary for other researchers to attempt a replication and an available dataset is needed for a reproduction. To take a more fine-grained look at the publication status, several different types of research emerge. An article can be retracted, that is, there is no confidence anymore in its findings due to research misconduct or severe errors. When the data of a study were fabricated and it was thus retracted, a reproduction will not be informative but a replication may inform researchers about the correctness of the hypothesis, unlike the original report. Other reasons (or unclear reasons) for retraction may conversely increase the replication and reproduction value, as the source of a true claim may have become untrustworthy (and not easily citable) due to issues unrelated to its truth (e.g., plagiarism).\nReplicating and reproducing every finding that was ever published appears impossible to achieve, which is why researchers need to make decisions about prioritization. In the following, we discuss criteria by which such a prioritization can occur - restricted to quantitative research.",
    "crumbs": [
      "The Replication Process",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Choosing the Target Study</span>"
    ]
  },
  {
    "objectID": "choosing_study.html#value",
    "href": "choosing_study.html#value",
    "title": "3  Choosing the Target Study",
    "section": "3.2 Value",
    "text": "3.2 Value\nThe original study should be somehow relevant for the replication to have value (e.g., (Karhulahti, Martončik, and Adamkovic 2024)). It may have started a research stream. For example, Jacowitz and Kahneman’s (Jacowitz and Kahneman 1995) studies on anchoring and adjustment were fundamental for how anchoring effects are investigated today, and were therefore replicated by Klein et al. (Klein et al. 2014). Field et al. (Field et al. 2019) propose a method for the selection of replication studies that features the theoretical importance of the original study result. Relevance may be evidenced by many citations as they show that many studies are building on the finding, testing similar hypotheses, or criticizing the study. Note that a study could also be cited as a negative example or study that has not been replicated or retracted for some reason. Isager et al. (IsagerEtAl2021?; Isager et al. 2023) suggest deciding what to replicate based on sample size and citation count (but see (M. Pittelkow, Field, and Ravenzwaaij 2025)). In a Delphi study examining consensus among psychologists that had conducted empirical replications on what should influence the decision of what study to replicate, elements that came up were the importance of the original study for research (as indicated by citations, the phenomenon being over- or understudied, and the impact factor of the journal), the theoretical relevance of the study, and the implications of the original study for practice, policy, or clinical work (M. M. Pittelkow et al. 2023). The relevance of societal impact was also stressed by Bekker (Bekkers 2024), as a study may have a high value for a societal problem (e.g., a new vaccine or a repeated test of a claim that is relevant in the political discourse such as criminality among immigrants).\nFor scientists reproducing or replicating a study because they are interested in building on its findings (including if they wish to build upon their own original findings), their interest to build on it may be a sufficient indicator of its relevance to their research program.",
    "crumbs": [
      "The Replication Process",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Choosing the Target Study</span>"
    ]
  },
  {
    "objectID": "choosing_study.html#uncertainty",
    "href": "choosing_study.html#uncertainty",
    "title": "3  Choosing the Target Study",
    "section": "3.3 Uncertainty",
    "text": "3.3 Uncertainty\nThe more uncertain the original study’s outcome is, the higher the potential of knowledge gained from reproduction and replication. Although no findings are definitive, research reports differ in the strength of the evidence they present (e.g., Registered Reports1 are typically more convincing than non-preregistered studies; (Soderberg et al. 2021)). Similarly, sample size (within a given field) has been proposed as an indicator of evidence strength (IsagerEtAl2021?). Pittelkow et al. (M. M. Pittelkow et al. 2021, 2023) and Field et al. (Field et al. 2019) both argued for using the current strength of evidence in favour of the original claim as an important element that features into choosing a replication target. However, the degree of uncertainty can be uncertain or misjudged: In some areas of research a hypothesis had been claimed to be confirmed hundreds of times and yet, large-scale replication effort could not support the original hypothesis so that after hundreds of studies the existence of the phenomenon was still unknown (e.g., (Friese et al. 2019)). Meta-analyses allow some tests for uncertainty (e.g. via correction of bias, evaluation of risk of bias, or estimates of heterogeneity). Although there are numerous ways to meta-analytically evaluate the expected replicability of a set of claims, none of them is as solid as a well-designed replication attempt (Carter et al. 2019). Other heuristics to estimate robustness, reproducibility and replicability of sets of findings have been proposed: they include the caliper test, relative proximity, or z-curve (Bartoš and Schimmack 2022; see Adler, Röseler, and Schöniger 2023, for an overview and a ShinyApp that combines these tools). Individual findings can be assessed through forensic meta-science tests (for an overview, see (Heathers 2025)), and through the assessment of papers for reporting issues, such as those identified by statcheck (Nuijten and Polanin 2020; DeBruineLakens2025?). Moreover, methods such as sum of p-values (Held, Pawel, and Micheloud 2024) and Bayesian re-analysis can be applied to help determine the degree of evidence for a given effect an original study might contain (Field et al. 2019; M. M. Pittelkow et al. 2021).\nIf the original paper reports multiple studies for the same phenomenon, researchers should check the proportion of significant studies and whether all of them confirm the hypothesis. More studies reduce the overall statistical power (power deflation). Provided the hypothesis is correct, a single study may test it with 90% power, that is, the statistical analysis will indicate the correctness of the hypothesis with a probability of .9. Now, if 10 studies are run with 90% power each, the chances of all of them supporting the hypothesis (even if it is true) are 0.9^10 = .35. For 80%, even finding five significant findings in a row is fairly highly unlikely (0.8^5 = .33). Thus, studies reporting a set of many and only significant findings when each of the studies does not have very high power should be taken with caution (see also (Francis 2012; Schimmack 2012; Lakens and Etz 2017)).\nFor large parts of the literature and given the overall low replicability rate in many fields (though not all, e.g., (Soto 2019)), the mere lack of a reproduction or close replication by independent researchers can be used as an argument for uncertainty (e.g., (M. M. Pittelkow et al. 2023)). If a study has only been replicated by the original authors, it can be indicative of nobody else being interested in the phenomenon (i.e., low replication value) or nobody else being able to provide evidence for it (i.e., high uncertainty). For example, it is possible that reports of failed replications are held back by reviewers due to an aversion to null findings, replications, or findings criticizing their own work.\nAs replications can also be used to probe a phenomenon’s generalizability, a lack of variety in study designs can motivate a replication attempt. If there is reason to assume that a phenomenon is highly dependent on context (e.g., works only for graduate students, with English-speaking people, when people are incentivized, for the chosen stimuli, …), it can be replicated and extended in other contexts. More generally, when background factors are introduced to a study (e.g., there was a positive correlation in study X but researchers suspect it to vanish under condition M), the original finding needs to be replicated in a part of the new study for the argument to work. An added benefit of this is to help avoid later claims of ‘hidden moderators’ in original studies; an argument which has been used previously to refute the validity of replication study results (Zwaan et al. 2018).\nFinally, uncertainty can be the result of a lack of specificity in the original report: If there are details missing that cannot be retrieved anymore (e.g., researchers involved in the original study cannot be reached), a replication can develop, test and share a comprehensive set of materials. For example, Chartrand and Bargh’s (Chartrand and Bargh 1999) seminal study on the chameleon effect requires many materials but none of them are openly available. Accordingly, Pittelkow et al. (M. M. Pittelkow et al. 2023) identified the clarity of the original study protocol as an important element that features into the decision of replication study selection. Reconstructing these materials and documenting a procedure would, thus, be a valuable contribution of a replication study.\nTheoretical contribution\nIn some cases, theories are so vague that a failed replication would likely be criticized for misunderstanding the theory (e.g., (Baumeister and Vohs 2016)). This suggests that the target theory was not well specified. If accepted as a reason not to replicate, it can discourage any form of replication despite the target finding being relevant. Instead, replication researchers can ask original authors for feedback on the study protocol before collecting data to try to ensure that it tests (and then articulates) the intended theory. They can also engage in adversarial collaboration or “red teaming” (e.g., (Cowan et al. 2020)), that is work together with the original authors to design a study that they agree would be able to corroborate the original claim, or to call it into doubt.\nNevertheless, it has been argued that because so many original studies are flawed, the theories built upon them are weak, or contaminated. This, in turn, can lead to flawed replication studies, especially in the case of theory that aims to explain phenomena (Field et al., Volz, Kaznatcheev & van Dongen, 2024), risking a vicious cycle in which successful replications potentially perpetuate flaws across studies.\nAvailability of reproductions and replications\nWhile a single replication (or robustness reproduction) cannot provide conclusive evidence in regard to the veracity of original claims, the first numerical reproduction, and arguably also the first robustness reproduction and replication adds the greatest value in terms of reducing uncertainty. Therefore, the search for existing reproductions and replications is a key part of the selection of a target study.\nAlthough there is no comprehensive database with reproductions yet, researchers can check resources such as the Institute for Replication’s discussion paper series (https://i4replication.org/discussion_paper.html; (Brodeur et al. 2024)), the ReplicationWiki (Höffler 2017), the CODECHECK register (https://codecheck.org.uk/register/, (Nüst and Eglen 2021)), or the Social Science Reproduction Platform (https://www.socialsciencereproduction.org).\nWith regard to replications, researchers can browse the FORRT Replication Database (https://forrt-replications.shinyapps.io/fred_explorer/; (Röseler et al. 2024)), though this does not (yet) provide a replacement for manual searches.",
    "crumbs": [
      "The Replication Process",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Choosing the Target Study</span>"
    ]
  },
  {
    "objectID": "choosing_study.html#potential-researcher-bias-potential-researcher-bias",
    "href": "choosing_study.html#potential-researcher-bias-potential-researcher-bias",
    "title": "3  Choosing the Target Study",
    "section": "3.4 (Potential) Researcher Bias {#(potential)-researcher-bias}",
    "text": "3.4 (Potential) Researcher Bias {#(potential)-researcher-bias}\nResearchers typically work in relatively small communities to investigate the same phenomenon. These researchers are invested in their work and can be influenced by certain researcher biases, such as confirmation bias (the tendency to preferentially seek out, evaluate and recall information that supports one’s existing beliefs; e.g., (Mahoney 1977)) and motivated reasoning (generating post-hoc rationalizations that frame previous decisions in a favourable light; see (Hardwicke and Wagenmakers 2023; Munafò et al. 2020)). In some cases, researchers profit off their work and the (perceived) replicability of their findings may be associated with personal financial gain. Such conflicts of interest should be disclosed, but this is not always the case (see (Heirene et al. 2024)).\nHowever, replication researchers are just as prone to bias as original authors can be. Certain studies are more likely to be chosen for replication than others (see (Pennington 2023; Yarkoni 2013)), and there may be a publication bias in replication studies in favor of nonsignificant findings (Berinsky, Druckman, and Yamamoto 2021), though there is no empirical evidence for this yet. Nevertheless, greater interest in failed replications seems very likely, incentivizing replication researchers to apply questionable research practices (QRPs) so that the results are nonsignificant (“null hacking”, (Protzko 2018; Baumeister, Tice, and Bushman 2022)). The problems of p-hacking and null-hacking can mostly be solved through preregistration and the use of Registered (Replication) Reports (e.g., (Brodeur et al. 2024; Soderberg et al. 2021)). Another type of bias is that researchers may be interested in replicating specific studies because of personal admiration towards a study or envy towards a colleague.",
    "crumbs": [
      "The Replication Process",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Choosing the Target Study</span>"
    ]
  },
  {
    "objectID": "choosing_study.html#feasibility",
    "href": "choosing_study.html#feasibility",
    "title": "3  Choosing the Target Study",
    "section": "3.5 Feasibility",
    "text": "3.5 Feasibility\nReproductions require the original dataset. We recommend that researchers check whether the journal that published the original study has a data editor or reproducibility manager who has done a reproducibility check or provides a replication package. A replication package is a collection of materials to allow reproduction of the original results. Ideally, the dataset in the replication package, or shared separately, adheres to the FAIR criteria (Wilkinson et al. 2016), that is, it should be findable, accessible, interoperable, and reusable. Otherwise, the reproduction author would need to send a data sharing request to the original authors. In any case, they may need to consult with the original authors regarding software versions and code that does not work anymore due to changes in the software.\nWhile original data is not necessary for replications, thorough documentation of the original study is highly beneficial. Moreover, replication researchers should evaluate whether they can achieve the target sample size, which is often a multiple of the original sample size (see section Sample Size Determination). Pittelkow et al. (M. M. Pittelkow et al. 2023) identified the resources available to the replicating team in terms of funding, time, equipment, and (if relevant) previous experience and expertise as important elements that feature into the replication study selection. When choosing a target study, researchers should try to anticipate practical problems, and should restrict their choice of replication target to align with their lab resources in order to prevent ‘secondary’ research waste (Field et al. 2019). Specifically, some studies may be difficult to replicate (e.g., longitudinal studies). Other studies, such as those conducted with the use of highly technical, restricted, or expensive equipment, such as studies involving MRI scanning, might require expertise and knowledge that is not represented in all potential replication research teams (Field et al. 2019).\nMoreover, there are no established standards for replications in some fields yet. In that case, replications may add less to the reduction of uncertainty and replicators need to propose methods. For example, replications with response-surface-analyses are not as established as those with t-tests for two-group study designs. Furthermore, the complexity of the data types can pose challenges for definitions of successful replications, such as in neuroimaging research (e.g., MRI studies) which often implicates outcome variables with an additional spatial component.\n\n\n\n\nAdler, S. J., L. Röseler, and M. K. Schöniger. 2023. “A Toolbox to Evaluate the Trustworthiness of Published Findings.” Journal of Business Research 167: 114189. https://doi.org/10.1016/j.jbusres.2023.114189.\n\n\nBartoš, F., and U. Schimmack. 2022. “Z-Curve 2.0: Estimating Replication Rates and Discovery Rates.” Meta-Psychology 6. https://doi.org/10.15626/MP.2021.2720.\n\n\nBaumeister, R. F., D. M. Tice, and B. J. Bushman. 2022. “A Review of Multisite Replication Projects in Social Psychology: Is It Viable to Sustain Any Confidence in Social Psychology’s Knowledge Base?” Perspectives on Psychological Science 18 (4): 912–35. https://doi.org/10.1177/17456916221121815.\n\n\nBaumeister, R. F., and K. D. Vohs. 2016. “Misguided Effort with Elusive Implications.” Perspectives on Psychological Science 11 (4): 574–75. https://doi.org/10.1177/1745691616652878.\n\n\nBekkers, R. 2024. “Replication Value: A Comment and Alternative.” https://doi.org/10.31234/osf.io/uj5g7.\n\n\nBerinsky, A. J., J. N. Druckman, and T. Yamamoto. 2021. “Publication Biases in Replication Studies.” Political Analysis 29 (3): 370–84. https://doi.org/10.1017/pan.2020.34.\n\n\nBrodeur, A., A. Dreber, F. Hoces de la Guardia, and E. Miguel. 2024. “Reproduction and Replication at Scale.” Nature Human Behaviour 8 (1): 2–3. https://doi.org/10.1038/s41562-023-01807-2.\n\n\nCarter, E. C., F. D. Schönbrodt, W. M. Gervais, and J. Hilgard. 2019. “Correcting for Bias in Psychology: A Comparison of Meta-Analytic Methods.” Advances in Methods and Practices in Psychological Science 2 (2): 115–44. https://doi.org/10.1177/2515245919847196.\n\n\nChartrand, T. L., and J. A. Bargh. 1999. “The Chameleon Effect: The Perception–Behavior Link and Social Interaction.” Journal of Personality and Social Psychology 76 (6): 893. https://doi.org/10.1037/0022-3514.76.6.893.\n\n\nCowan, N., C. Belletier, J. M. Doherty, A. J. Jaroslawska, S. Rhodes, A. Forsberg, M. Naveh-Benjamin, P. Barrouillet, V. Camos, and R. H. Logie. 2020. “How Do Scientific Views Change? Notes from an Extended Adversarial Collaboration.” Perspectives on Psychological Science 15 (4): 1011–25. https://doi.org/10.1177/1745691620906415.\n\n\nErrington, T. M., M. Mathur, C. K. Soderberg, A. Denis, N. Perfito, E. Iorns, and B. A. Nosek. 2021. “Investigating the Replicability of Preclinical Cancer Biology.” eLife 10: e71601. https://doi.org/10.7554/eLife.71601.\n\n\nFeldman, G. 2025. “The Value of Replications Goes Beyond Replicability and Is Associated with the Value of the Research It Replicates: Commentary on Isager Et Al., 2021.” Meta Psychology 9. https://doi.org/10.15626/MP.2024.4326.\n\n\nField, S. M., R. Hoekstra, L. Bringmann, and D. van Ravenzwaaij. 2019. “When and Why to Replicate: As Easy as 1, 2, 3?” Collabra: Psychology 5 (1): 46. https://doi.org/10.1525/collabra.218.\n\n\nFrancis, G. 2012. “Too Good to Be True: Publication Bias in Two Prominent Studies from Experimental Psychology.” Psychonomic Bulletin & Review 19: 151–56. https://doi.org/10.3758/s13423-012-0227-9.\n\n\nFriese, M., D. D. Loschelder, K. Gieseler, J. Frankenbach, and M. Inzlicht. 2019. “Is Ego Depletion Real? An Analysis of Arguments.” Personality and Social Psychology Review 23 (2): 107–31. https://doi.org/10.1177/1088868318762183.\n\n\nHardwicke, T. E., and E. J. Wagenmakers. 2023. “Reducing Bias, Increasing Transparency and Calibrating Confidence with Preregistration.” Nature Human Behaviour 7 (1): 15–26. https://doi.org/10.1038/s41562-022-01497-2.\n\n\nHeathers, J. 2025. “An Introduction to Forensic Metascience.” https://doi.org/10.5281/zenodo.14871843.\n\n\nHeirene, R., D. LaPlante, E. Louderback, B. Keen, M. Bakker, A. Serafimovska, and S. Gainsbury. 2024. “Preregistration Specificity and Adherence: A Review of Preregistered Gambling Studies and Cross-Disciplinary Comparison.” Meta-Psychology 8. https://doi.org/10.15626/MP.2021.2909.\n\n\nHeld, L., S. Pawel, and C. Micheloud. 2024. “The Assessment of Replicability Using the Sum of p-Values.” Royal Society Open Science 11 (8): 240149. https://doi.org/10.1098/rsos.240149.\n\n\nHöffler, J. H. 2017. “ReplicationWiki: Improving Transparency in Social Sciences Research.” D-Lib Magazine 23 (3): 1. https://doi.org/10.1045/march2017-hoeffler.\n\n\nIsager, P. M., R. C. M. van Aert, Š. Bahník, M. J. Brandt, K. A. DeSoto, R. Giner-Sorolla, J. I. Krueger, et al. 2023. “Deciding What to Replicate: A Decision Model for Replication Study Selection Under Resource and Knowledge Constraints.” Psychological Methods 28 (2): 438–51. https://doi.org/10.1037/met0000438.\n\n\nJacowitz, K. E., and D. Kahneman. 1995. “Measures of Anchoring in Estimation Tasks.” Personality and Social Psychology Bulletin 21 (11): 1161–66. https://doi.org/10.1177/01461672952111004.\n\n\nKamermans, K. L., L. Dudda, T. Daikoku, and S. Verheyen. 2025. “The Is-Ought Problem in Deciding What to Replicate: Which Motives Guide Current Replication Practices?” https://doi.org/10.31234/osf.io/6xdy2_v2.\n\n\nKarhulahti, V., M. Martončik, and M. Adamkovic. 2024. “Pre-Replication in Meaningful Science.” https://doi.org/10.31234/osf.io/5gn7m.\n\n\nKlein, R. A., K. A. Ratliff, M. Vianello, R. B. Adams Jr, Š. Bahník, M. J. Bernstein, and B. A. and Nosek. 2014. “Investigating Variation in Replicability.” Social Psychology. https://doi.org/10.1027/1864-9335/a000178.\n\n\nLakens, D., and A. J. Etz. 2017. “Too True to Be Bad: When Sets of Studies with Significant and Nonsignificant Findings Are Probably True.” Social Psychological and Personality Science 8 (8): 875–81. https://doi.org/10.1177/1948550617693058.\n\n\nMahoney, M. J. 1977. “Publication Prejudices: An Experimental Study of Confirmatory Bias in the Peer Review System.” Cognitive Therapy and Research 1: 161–75. https://doi.org/10.1007/BF01173636.\n\n\nMunafò, M. R., C. D. Chambers, A. M. Collins, L. Fortunato, and M. R. Macleod. 2020. “Research Culture and Reproducibility.” Trends in Cognitive Sciences 24 (2): 91–93. https://doi.org/10.1016/j.tics.2019.12.002.\n\n\nNuijten, M. B., and J. R. Polanin. 2020. “‘Statcheck’: Automatically Detect Statistical Reporting Inconsistencies to Increase Reproducibility of Meta‐analyses.” Research Synthesis Methods 11 (5): 574–79. https://doi.org/10.1002/jrsm.1408.\n\n\nNüst, D., and S. J. Eglen. 2021. “CODECHECK: An Open Science Initiative for the Independent Execution of Computations Underlying Research Articles During Peer Review to Improve Reproducibility.” F1000Research 10: 253. https://doi.org/10.12688/f1000research.51738.2.\n\n\nOpen Science Collaboration. 2015. “Estimating the Reproducibility of Psychological Science.” Science 349 (6251): aac4716. https://doi.org/10.1126/science.aac4716.\n\n\nPennington, C. R. 2023. A Student’s Guide to Open Science: Using the Replication Crisis to Reform Psychology. Open University Press.\n\n\nPittelkow, M. M., S. M. Field, P. M. Isager, A. E. van’t Veer, T. Anderson, S. N. Cole, and D. and Van Ravenzwaaij. 2023. “The Process of Replication Target Selection in Psychology: What to Consider?” Royal Society Open Science 10 (2): 210586. https://doi.org/10.1098/rsos.210586.\n\n\nPittelkow, M. M., R. Hoekstra, J. Karsten, and D. van Ravenzwaaij. 2021. “Replication Target Selection in Clinical Psychology: A Bayesian and Qualitative Reevaluation.” Clinical Psychology: Science and Practice 28 (2): 210. https://doi.org/10.1037/cps0000013.\n\n\nPittelkow, M., S. M. Field, and D. van Ravenzwaaij. 2025. “Thinking Beyond RVCN: Addressing the Complexity of Replication Target Selection.” https://doi.org/10.31234/osf.io/6tmyx_v2.\n\n\nProtzko, J. 2018. “Null-Hacking, a Lurking Problem.” https://doi.org/10.31234/osf.io/9y3mp.\n\n\nRöseler, L., L. Kaiser, C. Doetsch, N. Klett, C. Seida, A. Schütz, and Y. and Zhang. 2024. “The Replication Database: Documenting the Replicability of Psychological Science.” Journal of Open Psychology Data 12 (1): 8. https://doi.org/10.5334/jopd.101.\n\n\nSchimmack, U. 2012. “The Ironic Effect of Significant Results on the Credibility of Multiple-Study Articles.” Psychological Methods 17 (4): 551. https://doi.org/10.1037/a0029487.\n\n\nSoderberg, C. K., T. M. Errington, S. R. Schiavone, and and others. 2021. “Initial Evidence of Research Quality of Registered Reports Compared with the Standard Publishing Model.” Nature Human Behaviour 5: 990–97. https://doi.org/10.1038/s41562-021-01142-4.\n\n\nSoto, C. J. 2019. “How Replicable Are Links Between Personality Traits and Consequential Life Outcomes? The Life Outcomes of Personality Replication Project.” Psychological Science 30 (5): 711–27. https://doi.org/10.1177/0956797619831612.\n\n\nWilkinson, M. D., M. Dumontier, I. J. Aalbersberg, G. Appleton, M. Axton, A. Baak, and B. and Mons. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (1): 1–9. https://doi.org/10.1038/sdata.2016.18.\n\n\nYarkoni, T. 2013. “‘What We Can and Can’t Learn from the Many Labs Replication Project’.” Talyarkoni.org/Blog.\n\n\nZwaan, R. A., A. Etz, R. E. Lucas, and M. B. Donnellan. 2018. “Making Replication Mainstream.” Behavioral and Brain Sciences 41: e120. https://doi.org/10.1017/S0140525X17001972.",
    "crumbs": [
      "The Replication Process",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Choosing the Target Study</span>"
    ]
  },
  {
    "objectID": "choosing_study.html#footnotes",
    "href": "choosing_study.html#footnotes",
    "title": "3  Choosing the Target Study",
    "section": "",
    "text": "For registered reports, a journal reviews only the introduction and method and no data have been collected at this point of time. After an initial revision and “in-principle acceptance”, results are collected and the full report submitted. The second round of review is concerned only with the authors’ adherence to the preregistration and success to execute the planned study.↩︎",
    "crumbs": [
      "The Replication Process",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Choosing the Target Study</span>"
    ]
  },
  {
    "objectID": "planning.html",
    "href": "planning.html",
    "title": "4  Planning and Conducting Reproductions and Replications",
    "section": "",
    "text": "5 Planning and Conducting Reproductions and Replications\nPlanning depends on whether the focus is on a certain method or a theory, that is whether the replication will be close or conceptual. Table 2 provides an overview of reproduction and replication types, or more generally “repetitive research” (Schöch 2023), drawn from different resources (e.g., (Dreber and Johannesson 2024); (Hüffmeier, Mazei, and Schultze 2016); for an alternative taxonomy see also (Cortina, Köhler, and Aulisi 2023)). The decision between these types is the first step in planning.\nIn addition, the formation of the replication team is important, as replications can take substantial resources. Notably, repetitive research has successfully been conducted collaboratively with graduate and undergraduate students (e.g., (Boyce et al. 2024; Hawkins et al. 2018; Jekel et al. 2020; Moreau and Wiebels 2023)) and we recommend the use of replication studies to engage students of different levels in conducting and publishing research.\nTable 2\nTypes of repetitive research ordered by reproduction and replication and respective closeness to the original study.",
    "crumbs": [
      "The Replication Process",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Planning and Conducting Reproductions and Replications</span>"
    ]
  },
  {
    "objectID": "planning.html#post-publication-conversations",
    "href": "planning.html#post-publication-conversations",
    "title": "4  Planning and Conducting Reproductions and Replications",
    "section": "5.1 Post Publication Conversations",
    "text": "5.1 Post Publication Conversations\nWhen planning the replication study, additional knowledge should be taken into account such as any discussions of the original finding. There can be other studies citing the original studies, criticizing them, disconfirming their underlying theory, identifying errors, reinterpreting the finding, or making suggestions for replications. All of these might highlight considerations that need to be taken into account when designing a replication study that robustly tests the original claim or its generalisability.\nThus, replication researchers should look for post-publication discussions on the target study such as published comments and reviews, blog posts, or discussions on social media. These can often be found via Altmetric (https://www.altmetric.com) or other tools that allow researchers to quickly identify discussions on social media or news outlets beyond scientific journals (PubPeer, Hypothes.is), or the in-development platform Alphaxiv.org; for a review see (Henriques et al. 2023).",
    "crumbs": [
      "The Replication Process",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Planning and Conducting Reproductions and Replications</span>"
    ]
  },
  {
    "objectID": "planning.html#reproduction-before-replication",
    "href": "planning.html#reproduction-before-replication",
    "title": "4  Planning and Conducting Reproductions and Replications",
    "section": "5.2 Reproduction before Replication",
    "text": "5.2 Reproduction before Replication\nMany features of a replication study rest on the correctness of the original report. A reproduction allows researchers to investigate this by being able to uncover coding errors, fraud, robustness to analytical decisions, and generalizability. To make efficient use of resources, we encourage researchers to investigate the original finding’s reproducibility and robustness first. In other words, ideally, reproductions should take place before planning and conducting a replication study. Depending on the availability of the code and data, these can take several minutes to weeks.\nIf the original code and dataset are available, researchers can try to numerically reproduce the results. Beware, however, that differences in software versions or default settings may lead to slight deviations or require corrections in some cases (for a large-scale test of reproducibility see (Brodeur et al. 2024)). Similarly, the lack of a set seed for random number generators can mean that analyses relying on random numbers (e.g., bootstrapping) cannot be exactly reproduced. If no analysis script is available, analyses need to be recreated from the descriptions in the report (recoding reproduction). In this case, special attention should be paid to processing steps such as exclusion of outliers, transformation of variables, and handling of missing data. However, in many research areas information on these steps is often incomplete (Field et al. 2019); older research tends to be especially limited in terms of the methodological details they provide. In addition, we recommend testing the robustness of the original finding by making small alterations to the data processing and analyses procedure (robustness reproductions). For example, if the analyses were run for a subset of the data (e.g., participants aged 21 to 30 or without outliers ± 3 standard deviations), this subset can be changed (e.g., participants aged 18 to 30 or without outliers ± 2 standard deviations). Here, the initial focus should be on choices that are not determined by the theory that is presented, though this can also be used to explore the generalisability of some aspects of theory. Finally, if the original study was preregistered and the original code is available, reproduction researchers can check whether the original analyses adhere to the preregistered analysis plan.\nIf neither code nor data are available (or shared by the authors), no reproduction is possible. Researchers can still use automated tools to compare reported p-values with those that can be computed from test statistics via the website statcheck.io (where documents may be uploaded), the corresponding R package (Nuijten and Polanin 2020), or papercheck (DeBruineLakens2025?), which is still actively maintained.\nFigure 3",
    "crumbs": [
      "The Replication Process",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Planning and Conducting Reproductions and Replications</span>"
    ]
  },
  {
    "objectID": "planning.html#close-replication-before-conceptual-replication",
    "href": "planning.html#close-replication-before-conceptual-replication",
    "title": "4  Planning and Conducting Reproductions and Replications",
    "section": "5.3 Close replication before conceptual replication",
    "text": "5.3 Close replication before conceptual replication\nIf the goal is to increase the generalizability of a specific finding, we also suggest starting with replications that adhere as close as possible to the original study (e.g., close replications) and only later conduct conceptual replications. Based on Hüffmeier, Mazei, and Schultze (Hüffmeier, Mazei, and Schultze 2016), we propose the typology and order of replication attempts in Figure 3. Importantly, replications at any stage should not compromise any aspects of an original study, but rather (at the latest from the third study stage [constructive replications] onwards) try to improve one or more aspects of the original study, such as “[…] more valid measures, more critical control variables, a more realistic task, a more representative sample, or a design that allows for stronger conclusions regarding causality”, see Köhler & Cortina (Köhler and Cortina 2021, 494). Köhler and Cortina term such replications “constructive replications” and caution against the conduct of “quasi-random” replications that vary features without clear rationale.\nFinally, there may be cases where the sequence of replications is not necessary, or where the context of the replication team requires a focus on generalisability to a specific context (see section The Role of Differences for the Interpretation of Findings).\nFigure 4\n\nNote: This is an adaptation and update of the typology of replication studies by Hüffmeier, Mazei, and Schultze (Hüffmeier, Mazei, and Schultze 2016). The typology is conceptualized as a hierarchy of studies that together help to (i) establish the validity and replicability of new effects, (ii) exclude alternative explanations, (iii) test relevant boundary conditions, and (iv) test generalizability.\n\n\n\n\nBoyce, V., B. Prystawski, A. B. Abutto, E. M. Chen, Z. Chen, H. Chiu, and M. C. and Frank. 2024. “Estimating the Replicability of Psychology Experiments After an Initial Failure to Replicate,” May. https://doi.org/10.31234/osf.io/an3yb.\n\n\nBrodeur, A., A. Dreber, F. Hoces de la Guardia, and E. Miguel. 2024. “Reproduction and Replication at Scale.” Nature Human Behaviour 8 (1): 2–3. https://doi.org/10.1038/s41562-023-01807-2.\n\n\nCortina, J. M., T. Köhler, and L. C. Aulisi. 2023. “Current Reproducibility Practices in Management: What They Are Versus What They Could Be.” Journal of Management Scientific Reports 1 (3-4): 171–205. https://doi.org/10.1177/27550311231202696.\n\n\nDreber, A., and M. Johannesson. 2024. “A Framework for Evaluating Reproducibility and Replicability in Economics.” Economic Inquiry. https://doi.org/10.1111/ecin.13244.\n\n\nField, S. M., R. Hoekstra, L. Bringmann, and D. van Ravenzwaaij. 2019. “When and Why to Replicate: As Easy as 1, 2, 3?” Collabra: Psychology 5 (1): 46. https://doi.org/10.1525/collabra.218.\n\n\nHawkins, R. X., E. N. Smith, C. Au, J. M. Arias, R. Catapano, E. Hermann, and M. C. and Frank. 2018. “Improving the Replicability of Psychological Science Through Pedagogy.” Advances in Methods and Practices in Psychological Science 1 (1): 7–18. https://doi.org/10.1177/2515245917740427.\n\n\nHenriques, S. O., N. Rzayeva, S. Pinfield, and L. Waltman. 2023. “Preprint Review Services: Disrupting the Scholarly Communication Landscape?” https://doi.org/10.31235/osf.io/8c6xm.\n\n\nHüffmeier, J., J. Mazei, and T. Schultze. 2016. “Reconceptualizing Replication as a Sequence of Different Studies: A Replication Typology.” Journal of Experimental Social Psychology 66: 81–92. https://doi.org/10.1016/j.jesp.2015.09.009.\n\n\nJekel, M., S. Fiedler, R. Allstadt Torras, D. Mischkowski, A. R. Dorrough, and A. Glöckner. 2020. “How to Teach Open Science Principles in the Undergraduate Curriculum—the Hagen Cumulative Science Project.” Psychology Learning & Teaching 19 (1): 91–106. https://doi.org/10.1177/1475725719868149.\n\n\nKöhler, T., and J. M. Cortina. 2021. “Play It Again, Sam! An Analysis of Constructive Replication in the Organizational Sciences.” Journal of Management 47 (2): 488–518. https://doi.org/10.1177/0149206319843985.\n\n\nMoreau, D., and K. Wiebels. 2023. “Ten Simple Rules for Designing and Conducting Undergraduate Replication Projects.” PLOS Computational Biology 19 (3): e1010957. https://doi.org/10.1371/journal.pcbi.1010957.\n\n\nNuijten, M. B., and J. R. Polanin. 2020. “‘Statcheck’: Automatically Detect Statistical Reporting Inconsistencies to Increase Reproducibility of Meta‐analyses.” Research Synthesis Methods 11 (5): 574–79. https://doi.org/10.1002/jrsm.1408.\n\n\nSchöch, C. 2023. “Repetitive Research: A Conceptual Space and Terminology of Replication, Reproduction, Revision, Reanalysis, Reinvestigation and Reuse in Digital Humanities.” International Journal of Digital Humanities 5 (2): 373–403. https://doi.org/10.1007/s42803-023-00073-y.",
    "crumbs": [
      "The Replication Process",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Planning and Conducting Reproductions and Replications</span>"
    ]
  },
  {
    "objectID": "execution_reproductions.html",
    "href": "execution_reproductions.html",
    "title": "5  Execution of Reproductions",
    "section": "",
    "text": "5.0.1 Gathering resources\nPrerequisites for reproduction studies are available data and ideally also code. These are usually linked within the manuscript and shared via repositories (e.g., Zenodo, OSF.io, github.com, gitlab.com) or they are part of the supplemental materials that are listed on the article’s website. In special cases, an entire original manuscript may be reproducible and written in Markdown language. Researchers searching for target studies to reproduce can check topfactor.org and filter for Data transparency level 3 (https://topfactor.org/journals?factor=Data+Transparency; will no longer be updated). They can also use the extensive database of economics studies with available data compiled by Sebastian Kranz (https://ejd.econ.mathematik.uni-ulm.de/).\nIf data are not publicly available, researchers can contact the authors of the original study. In this case, we recommend them to adhere to Guide for Accelerating Computational Reproducibility in the Social Sciences (ACRE) guidelines for constructive communication (Berkeley Initiative for Transparency in the Social Sciences 2020).\nWhen re-using data, researchers need to respect licenses. Generally, research data should be licensed openly, that is re-use and alteration should be permitted, likely requiring citation of the original resource (e.g., CC-BY 4.0 Attribution). Note, however, that non-derivative licenses may prohibit reproductions; in that case separate approval would be required from the copyright holder.\nWhen it comes to reporting, Ankel-Peters et al. (Ankel-Peters et al. 2025) provide a table for reporting results from the computational reproduction that includes resource availability (e.g., raw data, cleaning code, analysis code).\n\n\n5.0.2 Contacting Authors\nReproduction authors may have to contact the original authors if there is something missing. It will often be necessary to contact the authors more than once because missing descriptions of details of the original study only become apparent once the replication study is planned. In most cases, the original paper identifies one of the authors as “corresponding author” with an e-mail address. We recommend a quick web search to check if this is the current email address, as researchers frequently change institutions and thus e-mail addresses. Sometimes, it may be most helpful to write to the last authors instead, who tend to have more stable e-mail addresses, or to copy all authors into the email. Templates for asking for materials and sharing replication results in the appendix. Note that original authors may not respond due to institutional changes or not being active in academia anymore.\n\n\n5.0.3 Identification of Claims\nStatistical analyses and their results are always used as a way to evaluate a certain claim. While Ankel-Peters et al. (Ankel-Peters et al. 2025) recommend reproductions to identify “results [that] are essential for the paper’s main argument to hold“, we acknowledge that a reproduction can also focus on secondary results if they are relevant in some other context. In either case, reproduction researchers need to justify the choice of the claim in their report.\n\n\n5.0.4 Preregistration\nPreregistrations contain a description of the planned study or analysis prior to their execution. This way, they can reduce researchers’ ‘degrees of freedom’. In the case of reproductions, they can prevent QRPs (e.g., “null hacking”, (Bryan, Yeager, and O’Brien 2019); “gotcha bias”, (Berinsky, Druckman, and Yamamoto 2021)) as long as the entire analysis plan is preregistered (Brodeur et al. 2024) and the data have not yet been accessed. While a numerical reproduction with available code does not require preregistration, we recommend a priori specification of all further planned analyses.\nIt should be noted that a preregistered analysis plan or analysis script is much easier to create with access to data and reproductions are impossible with unavailable data, pre-registration cannot exclude the risk of authors having already looked at the data, yet making fraudulent claims regarding data access in a pre-registration is evidently academic misconduct. How much weight readers and reviewers will give to a pre-registration based on data that could have been accessed already will differ, but generating it is a way to keep ourselves accountable and produce robust reproductions.\n\n\n5.0.5 Deviations\nTo increase trust in the reported results, reproduction researchers need to report them in a transparent way, in a possible pre-registration and the final report. Ideally, all changes to the original procedure are explained, justified, and hypotheses about their expected effect on the outcomes are reported. Note that some journals’ publishing reproductions require adherence to special requirements such a Registered Report format (e.g., Journal of Open Psychology Data) or including a minimum of two independent reproductions (e.g., Journal of Robustness Reports).\n\n\n5.0.6 Analysis\nThe main part of the reproduction is the analysis. Factors that are potentially relevant for reproduction success include the software of the machine that is running the code as well as versions of the software and additional packages or plug-ins. For example, users of the open source software R can get a comprehensive overview of the program version and their machine using the function sessionInfo(), which should be included in supplementary materials. For python users, a package has been developed to run a similar function session_info.show() (https://gitlab.com/joelostblom/session_info).\nApart from a numerical reproduction where the same code is used, reproduction researchers can explore alternative ways that should and should not affect the results, test new hypotheses or theories, and run exploratory analyses. Their report should be clearly structured to discern these methods. Finally, for statistical analyses, the reproduction report should include reproducibility indicators (Dreber and Johannesson 2024) that summarize statistical significance and relative effect sizes across the original and reproduction results. Ankel-Peters et al. (Ankel-Peters et al. 2025) recommend a visual summary of these indicators in the form of a reproducibility dashboard and specification curves (e.g., (Simonsohn, Simmons, and Nelson 2020), see also (Mazei, Hüffmeier, and Schultze 2025)). We strongly recommend reproduction researchers to consult the respective resources for further details.\n\n\n5.0.7 Discussion\nThe discussion section should include a clear evaluation of the reproduction success on different levels (Ankel-Peters et al. 2025). Researchers should report possible reasons for failure (e.g., objective coding errors, changes in software packages) and the role of differences between the original and the reproduction studies’ results with respect to their conclusions. Finally, if the original authors provided comments, the reproduction report should include a discussion of them.\n\n\n\n\nAnkel-Peters, J., A. Brodeur, A. Dreber, M. Johannesson, F. Neubauer, and J. Rose. 2025. “A Protocol for Structured Robustness Reproductions and Replicability Assessments.” Q Open, qoaf004. https://doi.org/10.1093/qopen/qoaf004.\n\n\nBerinsky, A. J., J. N. Druckman, and T. Yamamoto. 2021. “Publication Biases in Replication Studies.” Political Analysis 29 (3): 370–84. https://doi.org/10.1017/pan.2020.34.\n\n\nBerkeley Initiative for Transparency in the Social Sciences. 2020. “Guide for Advancing Computational Reproducibility in the Social Sciences.” https://bitss.github.io/ACRE/.\n\n\nBrodeur, A., A. Dreber, F. Hoces de la Guardia, and E. Miguel. 2024. “Reproduction and Replication at Scale.” Nature Human Behaviour 8 (1): 2–3. https://doi.org/10.1038/s41562-023-01807-2.\n\n\nBryan, C. J., D. S. Yeager, and J. M. O’Brien. 2019. “Replicator Degrees of Freedom Allow Publication of Misleading Failures to Replicate.” Proceedings of the National Academy of Sciences 116 (51): 25535–45. https://doi.org/10.1073/pnas.1910951116.\n\n\nDreber, A., and M. Johannesson. 2024. “A Framework for Evaluating Reproducibility and Replicability in Economics.” Economic Inquiry. https://doi.org/10.1111/ecin.13244.\n\n\nMazei, J., J. Hüffmeier, and T. Schultze. 2025. “Specification Curve and Reproducibility Dashboards for Social Science Research: Recommendations for Implementation.” Advances in Methods and Practices in Psychological Science.\n\n\nSimonsohn, U., J. P. Simmons, and L. D. Nelson. 2020. “Specification Curve Analysis.” Nature Human Behaviour 4: 1208–14. https://doi.org/10.1038/s41562-020-0912-z.",
    "crumbs": [
      "The Replication Process",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Execution of Reproductions</span>"
    ]
  },
  {
    "objectID": "execution_replications.html",
    "href": "execution_replications.html",
    "title": "[6  Preregistration and Registered (Replication) Reports]{#preregistration-and-registered-(replication)-reports .quarto-section-identifier}",
    "section": "",
    "text": "6.0.1 Sample Size Determination\nFor replication studies, power analyses or other types of sample size justification can be simpler than for studies testing entirely new hypotheses because there already is a study that did what one is planning, with a result that one can refer to. However, we advise against simply using the original study’s sample size. While the maxim for most decisions is “stay as close as possible to the original study”, sample sizes of replication studies usually need to be larger. To be informative, replication failure should provide evidence for a null hypothesis or a substantially smaller effect size, which requires a larger sample. While a general tutorial for sample size justification is provided by Lakens (Lakens 2022), we briefly present approaches that are fit for replication studies.\nAs a pair of original and replication studies is usually concerned with multiple effect sizes (e.g., for different scales/items/groups/hypotheses), their number and individual power need to be considered carefully. If the interpretation will rely on the significance of all effect sizes, the total power will be smaller than the power for each individual test. To get along with limited resources, researchers may choose one single effect size and argue that it is central, or clearly specify other methods for aggregation across results (e.g., testing multivariate models).",
    "crumbs": [
      "The Replication Process",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>**Preregistration and Registered (Replication) Reports**</span>"
    ]
  },
  {
    "objectID": "execution_replications.html#footnotes",
    "href": "execution_replications.html#footnotes",
    "title": "[6  Preregistration and Registered (Replication) Reports]{#preregistration-and-registered-(replication)-reports .quarto-section-identifier}",
    "section": "",
    "text": "Note that a two-tailed test could be applied as well. Given that the original study has a clear effect and direction, one-tailed gives the original authors the benefit of the doubt.↩︎",
    "crumbs": [
      "The Replication Process",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>**Preregistration and Registered (Replication) Reports**</span>"
    ]
  },
  {
    "objectID": "discussion.html",
    "href": "discussion.html",
    "title": "7  Discussion",
    "section": "",
    "text": "7.0.0.1 Defining and Determining Replication Success\nThere is no strong consensus yet on what constitutes a replication success and some approaches can be biased (e.g., Schauer & Hedges, 2021) or imprecise (Patil et al., 2016b). Like in classical null hypothesis significance testing (NHST), replication researchers face the trade-off between dichotomizing something that is not dichotomous (success vs. failure) and making a clear decision about the outcome. On the one hand this is a question about statistical choices and their interpretation, namely how to compare original and replication effect sizes (or p-values) and how to interpret differences. On the other hand, it is a more complex question about how to interpret a mixed pattern of results, where some results are consistent across original and replication, while others are not. Here, it is important for replication researchers to specify which effects are of primary interest in their pre-registration, and how they will aggregate results, noting that requiring multiple effects to yield the same result will reduce statistical power.\nBelow, we present different approaches to assessing replication success as summarized by Heyard et al. (2025; see also Muradchanian et al., 2021; Röseler & Wallrich, 2024; Errington et al., 2021, Table 1).\nTable 3\nCriteria to operationalize replication success (excerpt from Heyard et al., 2025)\n\n\n\nSuccess Criteria Table\n\n\n\n\n\n\n\nName\nQuestion answered\nType of Reproducibility investigated\n\n\n\n\nBayes Factor: Independent Jeffreys-Zellner-Siow BF test\n“What is the evidence for the effect being present or absent in light of a replication attempt, given that we know relatively little about the expected effect size beforehand?”\nDifferent data - same analysis\n\n\nBayes Factor: Equality-of-effect-size BF test\n“What is the evidence for the effect size in the replication attempt being equal vs. unequal to the effect size in the original study?”\nDifferent data - same analysis\n\n\nBayes Factor: Fixed-effect meta-analysis BF Test\n“When pooling all data, what is the evidence for the effect being present vs. absent?”\nDifferent data - same analysis\n\n\nReplication Bayes factor\n“What is the evidence for the effect from the replication attempt being comparable to what was found in the original study, or absent?” - “Are the replication results more consistent with the original study or with a null effect?”\nDifferent data - same analysis\n\n\nSignificance criterion\n“Do the original and replication study both find a statistical significant effect in the same direction?”\nDifferent data - same analysis; Same data - different analysis; Different data - different analysis\n\n\nDifference in effect size\n“To which degree do the effects from a replication study mirror the original”\nDifferent data - same analysis; Same data - different analysis; Different data - different analysis\n\n\nConfidence interval: original effect in replication 95% CI\n“Given an original effect size, (what is the probability that) does a repetition of the experiment, with an independent sample of participants, produce(s) a CI that overlaps with the original effect?”\nDifferent data - same analysis\n\n\nConfidence interval: replication effect in original 95%CI\n“Given an effect size and 95% CI, (what is the probability that) does a repetition of the experiment, with an independent sample of participants, give(s) an effect that falls within the original CI?”\nDifferent data - same analysis\n\n\nPrediction interval: replication effect in original 95% prediction interval\n“Do the findings from the replication study align with a reasonable expectation, given the observed variation in the original study and replication study?” - “Are the replication estimates statistically consistent with the original estimates?”\nDifferent data - same analysis\n\n\nSmall Telescopes\n“Are the replication results consistent with an effect size big enough to have been detectable in the original?”\nDifferent data - same analysis\n\n\nMeta-analysis\n“Given an original-replication study pair, does the pooled effect align with that of the original study?” - “Given a set of replications, is the effect size reproducible across studies?”\nDifferent data - same analysis; Same data - different analysis; Different data - different analysis\n\n\nEquivalence testing\n“For the replication of an original null finding, does the replication study find an effect that is equally negligible?” - “Are the results from the replication statistically equivalent to the results of the original study?”\nDifferent data - same analysis\n\n\nMinimum effect testing\n“Is the replication effect size significantly different from a minimal effect size of interest, required to support the original study?”\nDifferent data - same analysis\n\n\nCausal replication framework\n“How can a replication failure be interpreted, from a causal perspective”\nDifferent data - same analysis; Different data - different analysis\n\n\nText-based machine learning model to estimate reproducibility\n“Given the text of an original paper, what is the probability of replication success?”\nDifferent data - same analysis\n\n\nPrediction market\n“What do the participants in a prediction market predict as the probability that the original findings will replicate?”\nDifferent data - same analysis\n\n\nPresence/Absence of elements ensuring reproducibility,via proxies\n“Do the design, methods and reporting of the original paper align with community standards of reproducible and transparent research?”\nSame data - same analysis; Same data - different analysis; Different data - same analysis; Different data - different analysis\n\n\nQuantified reproducibility assessment, QRA\n“After performing multiple measurements of an object, what is the precision of the measured quantity obtained?”\nSame data - same analysis; Different data - same analysis; Same data - different analysis; Different data - different analysis\n\n\nJaccard similarity coefficient\n“By what extent do the results of two (or more) fMRI experiments overlap?”\nSame data - same analysis\n\n\nSceptical \\(p\\)-value\n“To what extent are the results of a replication study in conflict with the beliefs of a sceptic of the original study?”\nDifferent data - same analysis\n\n\nModified Brinley plot\n“Given a pre-specified desired effect and multiple replications, what is the share of replications that, represented graphically, achieve the desired effect?”\nSame data - same analysis; Same data - different analysis; Different data - same analysis; Different data - different analysis\n\n\nLikelihood-based approach for reproducibility\n“Given a theoretically interesting effect size derived from the original study, what is the evidence for or against replicating this effect?”\nDifferent data - same analysis\n\n\nBayesian mixture model for reproducibility rate\n“Given the results (\\(p\\)-values) from a set of original and replication studies, what is the rate of reproducibility, and how is it related to certain aspects of the experiments?”\nDifferent data - same analysis\n\n\nUnified framework for estimating the credibility of published research\n“For a specific published research work, what is the evidence for its credibility measured on four different dimensions: method and data transparency, analytic reproducibility, analytic robustness and effect reproducibility?”\nSame data - same analysis; Same data - different analysis; Different data - same analysis; Different data - different analysis\n\n\nReproducibility scale of workflow execution - Tonkaz\n“Given a certain original research paper with results based on computation, can the workflow to generate the results be executed and verified?”\nSame data - same analysis; Same data - different analysis; Different data - same analysis; Different data - different analysis\n\n\nMean relative effect size\n“What is the average ratio of replication study effects to original study effects?”\nDifferent data - same analysis; Same data - same analysis\n\n\nCorrelation between effects\n“Do the replication studies and the original studies produce effects that are correlated?”\nDifferent data - same analysis; Same data - same analysis\n\n\nFragility Index\n“Given the results of an original study were significant, what is the smallest change in the original data that is needed to deem the results non-significant? and vice-versa for original null results” - “How fragile are the original results to small changes in the underlying data?”\nSame data - different analysis\n\n\nExternally standardized residuals\n“Is the original study consistent with the replication(s)?” - “Are all studies included in a meta-analysis replicable?”\nDifferent data - same analysis; Same data - different analysis\n\n\nSnapshot hybrid\n“After replicating an original study, what is the evidence for a null, small, medium or large effect?”\nDifferent data - same analysis\n\n\nBayesian Evidence Synthesis\n“Given several conceptual replications with substantial diversity in data, design and methods but investigating the same theory, what is the evidence underlying a certain theory of interest?”\nDifferent data - different analysis\n\n\nDesign analysis\n“Given the results of an original study and an effect of a hypothetical replication study, what is the probability of the estimate being in the wrong direction, and what is the factor by which the magnitude of the effect is overestimated?”\nDifferent data - same analysis\n\n\nReproducibility Maps\n“For fMRI research, how many and which of the truly active voxels were strongly reproduced?”\nSame data - same analysis; Same data - different analysis\n\n\nContinuously cumulating meta-analytic approach\n“Given subsequent replications that were performed to date, what is the current evidence for an effect?”\nDifferent data - same analysis\n\n\nCorrespondence test\n“To what extent does the effect size from the replication study differ or is equivalent to that of the original study?”\nDifferent data - same analysis\n\n\nZ-curve\n“Do all studies combined provide credible evidence for a phenomenon?”\nDifferent data - same analysis\n\n\nCross-validation methods\n“To what extent can the stability of a result be trusted, and to what extent can the result be generalized?”\nDifferent data - same analysis\n\n\nNetwork Comparison Test, NCT\n“Given two network structures, how similar are they to each other?”\nSame data - same analysis; Different data - same analysis\n\n\nLeave-one-out error\n“Given a deep learning model, how generalizable are its results?”\nDifferent data - same analysis\n\n\nSubjective reproducibility assessment\n“Does the replication team consider the replication as successful?” - “To what extent does the replication team trust in the reproducibility of a finding?”\nDifferent data - same analysis\n\n\nI squared - \\(I^2\\)\n“Given a set of replications, to what extent is the total variation across study results due to heterogeneity?” - “How consistent are the results across replications?”\nDifferent data - same analysis; Different data - different analysis\n\n\nCredibility analysis\n“How credible are the results of a study, in a Bayesian framework?”\nDifferent data - same analysis\n\n\nConsistency of original with replications, \\(P_{\\mbox{orig}}\\)\n“To what extent are the replication effect sizes consistent with the effect size of an original study?”\nDifferent data - same analysis; Different data - different analysis\n\n\nProportion of population effects agreeing in direction with the original, \\(\\hat{P}_{&gt;0}\\)\n“To what extent do the replication effect sizes agree with the sign found in the original study?”\nDifferent data - same analysis; Different data - different analysis\n\n\nRepliCATS\n“How reliable do experts believe the claims from an original finding are?”\nDifferent data - same analysis\n\n\nRepeAT - Repeatability Assessment Tool\n“Does the presented research align with community standards of reproducible biomedical research, using electronic health records?”\nSame data - same analysis; Different data - same analysis\n\n\nP interval\n“Given the results of an original study, what is the range of \\(p\\)-values a replication (following the same design) would lie in with 80% probability?”\nDifferent data - same analysis\n\n\nRipetaScore\n“Given certain trust in research, reproducibility and professionalism quality indicators, how high does a paper score?”\nSame data - same analysis; Different data - same analysis\n\n\nBland-Altman Plot\n“Do the effects estimated in several original-replication study pairs agree with each other?” - “How good is the agreement between repeated measures/studies?”\nSame data - same analysis; Same data - different analysis; Different data - same analysis; Different data - different analysis\n\n\nSceptical Bayes Factor\n“In light of the replication data, at which level of evidence can an advocate of the original study convince a sceptic?”\nDifferent data - same analysis\n\n\n\n\n\nNote. For reference and descriptions see the full table by Heyard et al. (2025, Table 4) also available at http://rachelheyard.com/reproducibility_metrics/#table. Reused in accordance with the CC-BY license of the published article and the OSF-project (https://osf.io/sbcy3).\n\n\n7.0.0.2 Interpreting Divergent Results (Replication Failures) {#interpreting-divergent-results-(replication-failures)}\nWhen replications succeed, the original claim gains further credence (as long as the methods are sound). However, when replications fail, many explanations and interpretations can be advanced, which need to be carefully considered and discussed in a report. While replication failure can highlight issues with statistical conclusion validity in the original studies (John et al., 2012; Nelson et al., 2018; Simmons et al., 2011), other explanations need to be considered, including issues with internal, external, and construct validity in both original and replication studies (Fabrigar et al., 2020; Vazire et al., 2022). For example, internal validity is threatened when attrition rates differ between experimental conditions in original or replication studies, creating potential confounds in the interpretation of treatment effects (Zhou & Fishbach, 2016). Construct validity is threatened when original or replication studies use unvalidated ad-hoc measures, fail to employ validated manipulations of the target construct, or when differences in sample characteristics between original and replication studies mean that manipulations and measures do not work as intended (Fabrigar et al., 2020; Fiedler et al., 2021; Flake & Fried, 2020). External validity is threatened when original findings do not generalize to the specifics of the replication study due to person and context differences between studies that moderate the effect. Thus, before making statements about the original finding’s robustness and generalizability, replication researchers need to critically discuss potential methodological shortcomings in both original studies and replication attempts that limit statistical conclusion, internal, external, and construct validity.\n\n7.0.0.2.1 Hidden Moderator Account\nOne challenge for replication researchers is the identification of hidden/unknown confounds that may influence or bias the phenomenon under study. Each study has a set of potential extraneous or unknown moderator variables that is unique to it. These may seem trivial, such as the brightness of an experimental laboratory, or important, such as a cultural difference between the replicating and original studies. Yet even seemingly trivial differences could potentially change results. Often statistical and methodological choices are made to circumvent or attenuate these issues. However, for some paradigms, these variables could be unknown to the original researcher (Fiedler, 2011). These are referred to in the literature as unknown moderators, background variables, hidden moderators or fringe variables. While they are always a way to reject unpleasant replication results, they can potentially bias replications, which highlights that a single replication is never entirely conclusive (though it might raise enough doubts that researchers do not see the value in addressing the remaining uncertainty). It should be noted that the same argument could be applied to raise doubts about any original study, questioning whether the effect is really due to the hypothesised cause or due to some hidden moderator or background variable. Clearly a skeptic who stops at that level would not be taken very seriously, so that it is important to move conversations about replication failure beyond general suspicion of hidden moderators.\nBargh (2006) suggested that the evidence generated by empirical findings far outweighs the resources of (social) psychology to conceptualize and understand the mechanisms underlying their effects. Therefore, boundary conditions are not easily specified, which can impact both direct and conceptual replication success. Replication failure indicates that the original claim does not generalise to the setting of the replication. Whether that generalises to the setting of the original study needs to be considered in light of theory, and might be a legitimate matter of contention.\n\n\n\n7.0.0.3 The Role of Differences for the Interpretation of Findings\nEach replication outcome should be evaluated in the light of its closeness, which is why all deviations with the respective reasons and, if possible, their potential impact on the results should be discussed. Existing theories may help assess whether a deviation should affect the outcomes. For example, most psychological theories are agnostic towards age so that a different distribution of participants’ age will be unproblematic in most cases. Researchers may choose to evaluate replications from both phenomenon-focused / inductive and theory-focused / deductive views. Different types of interpretations are listed in Figure 5 and integrated from previous accounts by Borgstede and Scholz (2021) and Freese and Peterson (2017, Figure 3).\nFigure 5\nInterpretation of replication outcomes depend on similarity of closeness and results as well as the view (inductive vs. deductive).\n\nNote.\n\nInductive or phenomenon-oriented views assume minimal generalizability of the original finding. For example, they cannot cast doubts on the original finding unless the replication is highly similar to the original study.\n\nDeductive or theory-oriented views assume maximal generality of a theory. For example, different results (i.e., replication failures) cast doubts on the theory regardless of the replication type.\n\n\n\n7.0.0.4 Comments from the Original Study’s Authors {#comments-from-the-original-study’s-authors}\nIf the replication results do not converge with the original results, replication researchers can reach out to the original study’s authors and ask for a comment that they can publish together with the replication report. A template for asking for a comment is in the appendix. Note that some journals (e.g., Journal of Replications and Comments in Economics) require such statements at the time of submission.",
    "crumbs": [
      "The Replication Process",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discussion</span>"
    ]
  },
  {
    "objectID": "publishing.html",
    "href": "publishing.html",
    "title": "8  Communicating and Publishing",
    "section": "",
    "text": "The final step of replication research is publishing and communicating the results. Researchers should adhere to best practices of transparency and openness promotion guidelines (TOP, 2025; Grant et al., 2024) and to the reporting standards of their respective field (e.g., JARS standards for reporting psychology replications, https://apastyle.apa.org/jars/quant-table-6.pdf). For example, they should report a link to the pre-registration, analysis plan, and analysis script, share all materials and data (if possible in light of ethical and legal limitations) under an open license (see also Janz & Freese, 2021), and report methods and results comprehensively. Finally, in writing the report, reproduction and replication authors should be mindful of their language. Ideally, being replicated would be an honor for authors since other researchers deem their findings important but a failed replication could potentially harm the reputation of the original and increase distrust towards them among their peers. We recommend a descriptive and impersonal language. When criticizing bad documentation, no access to data, or brevity in methods replication authors should keep in mind the historical context of the original publication. For example, sharing data was much more difficult in the 1990s and not required in many areas until recently.\nThe journals that published the original studies are often also chosen by authors for publication in accordance with the pottery-barn-rule (Srivastava, 2012). However, in our experience, many journals reject replications due to their lack of novelty. We list several options for writing and publishing the report in Table 4. These are non-exclusive, that is, researchers can choose multiple of them. An overview of active journals that exclusively publish replications is in Table 5.\nTable 4\nReporting and communicating reproductions and replications.\n\n\n\nType\nDescription\n\n\n\n\nFORRT Replication Database\nThis open and collaborative database contains thousands of replication findings and makes them visible. Anyone can enter results using a guided survey (https://t1p.de/fred_submit).\n\n\nPubPeer\nResearchers can comment on the original study and say that there is a replication attempt, describe the outcome, and provide links/references/DOIs to the replication(s). Researchers checking pubpeer.com or using the browser plug-in that automatically highlights studies for which there are comments will see your comment.\n\n\nManuscript (required for Preprint and Journal Article)\nManuscripts are mostly used as they are the traditional form of a research article. For judgment and decision making, there are useful examples by Feldman (2024). For reproducibility analyses the I4R Replication Report Template (https://osf.io/j2qrx) can be used. Moreover, Röseler et al. (2025, https://osf.io/brxtd) provide general templates for reproductions and replications.\n\n\nPreprint\nWe recommend publishing a report in the form of a traditional or standardized manuscript as a preprint. This secures open access and makes the report visible, citable, and commentable. There are many preprint servers across the social sciences (e.g., PsyArxiv, SOCARXIV, SportRxiv, MediArXiv, MindRxiv, EdArXiv, AfricArXiv, or MetaArXiv). In some countries, researchers have a legal right for a secondary publication of their research (green open access). Be aware that preprints are faster in terms of publication than journal articles, but are usually not peer-reviewed.\n\n\nJournal article\nMost researchers have to “play by the rules”, that is, publish or perish (Bakker et al., 2012; Koole & Lakens, 2012). While some have argued for a pottery barn rule (https://thehardestscience.com/2012/09/27/a-pottery-barn-rule-for-scientific-journals/) where journals that published the original finding have to publish respective replication attempts, many journals are not (yet) interested in replications. Notable exceptions are listed in the appendix. This is why journals dedicated to replications have emerged (see Table 5). Moreover, researchers can submit their preprint to a PCI community (see https://peercommunityin.org/current-pcis/), which is a preprint peer-review service. Several journals are PCI-friendly, which means that they publish articles recommended by the respective PCI. Many institutions and libraries recommend adding a CC-BY disclaimer on journal submissions that give the researchers the right to use the accepted manuscript as they like or choosing Diamond Open Access journals that are defined by no fees for publishing and reading research.\n\n\n\nTable 5\nActive journals dedicated to reproductions and replications.\n\n\n\nJournal name\nCommercial status\nOwners\nDisciplines\nArticle types\nWebsite\n\n\n\n\nJournal of Comments and Replications in Economics\nNon-commercial, diamond OA\nZBW\nEconomics\nReplications, Reproductions and comments research\nhttps://jcr-econ.org\n\n\nReplication Research\nNon-commercial, diamond OA\nMünster Center for Open Science and FORRT\nMultidisciplinary\nReproductions, Replications, Conceptual articles\nhttps://replicationresearch.org\n\n\nJournal of Open Psychology Data\nCommercial, Gold OA (APCs: 450 pounds)\nUbiquity Press\nPsychology\nReproductions (only as Registered Reports)\nhttps://openpsychologydata.metajnl.com\n\n\nJournal of Robustness Reports\nNon-commercial, diamond OA\nSciPost\nMultidisciplinary\nAt least two independent reproductions are required, limited to 500 words\nhttps://scipost.org/JRobustRep\n\n\nRescience C\nNon-commercial, diamond OA\nOlivia Guest, Benoît Girard, Konrad Hinsen, Nicolas P. Rougier\nMultidisciplinary\nReproductions\nhttps://rescience.github.io\n\n\nJournal of Management Scientific Reports\nCommercial (subscription based)\nSage\nManagement\nReplications, reproductions, related methods\nhttps://smgmt.org/jomsr/\n\n\nJournal of Reproducibility in Neuroscience\nNon-commercial, diamond OA\nCenter of Trial and Error\nNeuroscience\nReplications, Comments, Reviews, conceptual articles\nhttps://jrn.trialanderror.org\n\n\nRescience X\nNon-commercial, diamond OA\nEtienne B. Roesch\nMultidisciplinary\nReplications (Experiments)\nhttp://rescience.org/x\n\n\nAIS Transactions on Replication Research\nNon-commercial, diamond OA\nAssociation for Information Systems (?)\nInformation Systems\nExact, Methodological, Conceptual Replications\nhttps://aisel.aisnet.org/trr/",
    "crumbs": [
      "Advanced Topics and Applications",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Communicating and Publishing</span>"
    ]
  },
  {
    "objectID": "field_specific_mri.html",
    "href": "field_specific_mri.html",
    "title": "9  Field-Specific Replication Challenges: An example from MRI research",
    "section": "",
    "text": "9.0.1 Introduction\nWhile the principles of reproducibility and replication apply across scientific disciplines, certain fields face distinct methodological and practical challenges. Neuroimaging research, particularly MRI-based studies, is one example where field-specific complexities cause specific challenges for data sharing, reproducibility and replicability. Other fields may have different specialized requirements on these topics. Generally, false-positive findings are likely driven by a combination of low statistical power, a high number of researcher degrees of freedom and statistical tests, and biased motivation towards obtaining positive (i.e., significant) results (Ioannidis, 2005). Most of these factors are arguably aggravated in MRI studies, making replication research in this field particularly relevant albeit challenging. In addition, the analyzed data and obtained findings are characterized by a three-dimensional spatial component (or four dimensions in case of functional MRI studies (fMRI) in combination with time series data), which further complicates the matter. In the following we summarize the inherent peculiarities of replication research in the field of neuroimaging.\n\n\n9.0.2 Researcher Degrees of Freedom\nBrain imaging comes with a massive number of researcher degrees of freedom along the preprocessing and analysis pipelines. Preprocessing steps include for example motion correction procedures, spatial normalization and smoothing, with additional steps necessary for some imaging modalities, such as temporal signal filtering for fMRI. For each of these steps a multitude of parameter options and toolboxes are available. It has been shown that different preprocessing toolboxes can lead to fundamentally different results, even when aiming to harmonize all parameters (Zhou et al., 2002), and that different teams analyzing the same dataset can arrive at different final conclusions dependent on the used pipeline (Botvinik-Nezer et al., 2020). Furthermore, a large variety of operationalizations of neurobiological targets is available. For example, cerebral gray matter structure could be investigated as voxel-wise gray matter, segmentation-based regional cortical surface, thickness or gyrification.\nAnalysis-wise, the high number of researcher degrees of freedom is mainly a consequence of the multidimensional data structure. Basically, the central question is where in the brain to look for effects and how to define significance in the face of a large number of tests. There is an immensely high number of single data points represented by spatial units in the obtained individual images (e.g., two-dimensional pixels or three-dimensional voxels). Analysis is often done utilizing mass-univariate approaches where a statistical model is calculated separately for each of these spatial units. For example, in cerebral MRI research the analysis of 400k voxels is common. To avoid false-positive findings, region-of-interests (ROIs) are often defined or the analysis is restricted to a smaller region in the brain (i.e., small volume correction) to narrow down the search space and unique methods to correct for multiple testing are applied (Han et al., 2019). This again results in a multitude of options, such as the anatomical vs. functional definition of a ROI based on several different atlases and a variety of voxel-based or cluster-based inference methods to choose from. Botvinik-Nezer et al. (2020) gave the same fMRI dataset (raw data and preprocessed data), along with predefined hypotheses to 70 independent analysis teams and observed substantial variation in obtained results, attributable to variability in the analysis pipelines (in fact, none of the 70 teams used the same pipeline). Even when the same code and data is available the reproducibility of MRI analysis can be challenging (Leehr et al., 2024).\n\n\n9.0.3 Sample Size Justification\nThe gold standard for sample size justification is a power analysis. In neuroimaging this is complicated by the outlined mass-univariate three-dimensional data structure. Any power analysis would need to incorporate assumptions about the covariance structure of all data points, as well as the spatial extent and distribution of statistical effects, and the method to correct for multiple tests. While these numerous tests are not independent from another, the extent of their spatial covariance structure is difficult to assess and depends on preprocessing steps, such as image smoothing but is also on the data and the specific research question. Due to the high number of single data points, the obtained result is not a single statistical estimate with an effect size but rather a highly individual three-dimensional distribution of effect sizes around a peak localization. Simulation-based power analysis approaches have been previously suggested to address this problem. However, valid simulations require assumptions about valid spatial distributions of effects (contingent on regional anatomical peculiarities and on the specific research question), often difficult to assess and many developed power analysis tools have been discontinued. To date the utilization of power analysis is extremely rare in MRI research.\nWithout proper power estimation, justifying sample size becomes challenging. As in other fields of research the statistical power ultimately depends on the expected effect size. Recent large-scale investigations in the domain of mental health neuroimaging suggest that maximum underlying effect sizes are very small across various neuroimaging modalities (below 2% explained variance; Marek et al., 2022; Winter et al., 2022) and could require thousands of individuals to obtain robust and replicable statistical estimates (Marek et al., 2022). In contrast, given the labor-intensive and costly nature of MRI assessments, most MRI studies tend to have small sample sizes, making them likely underpowered (Button et al., 2013). Smaller samples may be suitable however, for research questions where the neurobiological effect sizes are expected to be larger, such as in psychosis research or when using highly individually tailored or within-subject designs (Lynch et al, 2024; Marek et al., 2022; Rosenberg & Finn, 2022; Spisak et al., 2023).\n\n\n9.0.4 Criteria of Replication Success\nRegarding the definition of replication success, the three-dimensional data structure requires special attention when defining replication success. In addition to other possible definitions, it has to be defined where in the brain the criteria of replication success should be met. As discussed above, there is not only one effect size but rather a 3D map with an effect size for each spatial unit (e.g., voxel). Goltermann & Altegoer (2025) describe a variety of potential criteria focusing on statistical significance in accordance with different spatial definitions revolving around the original finding. These include significance either at the peak voxel location (where the effect in the original study had the largest effect size), or in a ROI that can be defined in terms of spatial proximity to this peak voxel (for example a 15mm sphere with the peak voxel as a center) or in terms of an anatomically defined region where the original effect was found (for example anywhere in the hippocampus). Another possibility is the definition of a ROI directly deducted from the original results mask, if available (i.e., the original thresholded mask). Each of these spatial definitions comes with important limitations. For example, the meaning of proximity could be judged very different in different locations in the brain, as some anatomically or functionally defined structures may vary in size and distinctiveness (e.g., comparing the small and clearly-defined amygdala with a large and difficult to define dorsolateral prefrontal cortex). Thus, it may be necessary to combine several criteria in a systematic and/or subjective manner.\nIt should be noted that these criteria apply to voxel-based analyses. For other neuroimaging techniques, such as segmentation-based MRI analysis, diffusion tensor imaging (white matter integrity), or functional connectivity metrics, other criteria for replication success may be necessary.\n\n\n9.0.5 Open Science Practices in Neuroimaging\nWhile suggestions on open science practices and replication studies are not fundamentally different from other research areas, their necessity for neuroimaging studies could be even more pressing and there are some peculiarities to consider. Due to the high number of researcher degrees of freedom the utilization of automated preprocessing pipelines is highly advisable (e.g., Esteban et al., 2019), ideally in combination with containerized toolbox environments for preprocessing and analysis (Renton et al., 2024). In face of reproducibility challenges the transparent publication of preprocessing and analysis scripts becomes even more vital. While the publication of data is advised whenever possible, this can be difficult when sensitive patient data is included and whenever anonymization is difficult. For example, while this is currently subject of debate, MRI-derived brain scans may retain fingerprint-like identifiable features, even when removing the face from the image (Jwa et al., 2024, Abramian & Eklund, 2019). When the publication of raw data is not possible, comprehensive statistical brain maps (i.e., the statistical results in each voxel) should be made publicly available in non-thresholded form (Taylor et al., 2023) and/or data can be published in aggregated form (e.g., summarized for one brain region). Preregistrations can and should be used to make the exploitation of researcher degrees of freedom more transparent. To facilitate preregistrations in neuroimaging, there are multiple templates available. To incorporate all the specifics coming with MRI studies Beyer, Flannery et al. (2021) developed a fMRI specific template, which can be assessed here: https://doi.org/10.23668/psycharchives.5121. For replication research, preregistrations should contain a definition of replication success criteria that take into consideration the spatial dimension of results. Overall, open science practices and replications are still extremely rare in neuroimaging research despite their pressing relevance. Finally, there are also unique tensions to be navigated between open science practices in neuroimaging and the ongoing climate crisis, for example the sustainability of data sharing (see Puhlmann et al., 2025 for a perspective).",
    "crumbs": [
      "Advanced Topics and Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Field-Specific Replication Challenges: An example from MRI research</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "10  Conclusion",
    "section": "",
    "text": "10.1 Reproductions and Replications Checklist",
    "crumbs": [
      "Conclusion and Checklist",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#reproductions-and-replications-checklist",
    "href": "conclusion.html#reproductions-and-replications-checklist",
    "title": "10  Conclusion",
    "section": "",
    "text": "Justify choice of target study and claims\n\nChoose a reproduction/replication type that aligns with your aims\n\nGather and review all relevant materials\n\nReproduce before you replicate, where possible\n\nDiscuss all updates, changes, and extensions of the original materials (as close as possible, as updated as necessary)\n\nPreregister your study and analysis plan\n\nPredetermine conditions for success and failure\n\nUse balanced language when describing the outcomes\n\nCarefully evaluate outcomes and potential reasons for divergences\n\nReport your research comprehensively and openly accessible",
    "crumbs": [
      "Conclusion and Checklist",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Adler, S. J., L. Röseler, and M. K. Schöniger. 2023. “A Toolbox to\nEvaluate the Trustworthiness of Published Findings.” Journal\nof Business Research 167: 114189. https://doi.org/10.1016/j.jbusres.2023.114189.\n\n\nAnkel-Peters, J., A. Brodeur, A. Dreber, M. Johannesson, F. Neubauer,\nand J. Rose. 2025. “A Protocol for Structured Robustness\nReproductions and Replicability Assessments.” Q Open,\nqoaf004. https://doi.org/10.1093/qopen/qoaf004.\n\n\nAnkel-Peters, J., N. Fiala, and F. Neubauer. 2023. “Do Economists\nReplicate?” Journal of Economic Behavior &\nOrganization 212: 219–32. https://doi.org/10.1016/j.jebo.2023.05.009.\n\n\nBartoš, F., and U. Schimmack. 2022. “Z-Curve 2.0: Estimating\nReplication Rates and Discovery Rates.” Meta-Psychology\n6. https://doi.org/10.15626/MP.2021.2720.\n\n\nBaumeister, R. F., D. M. Tice, and B. J. Bushman. 2022. “A Review\nof Multisite Replication Projects in Social Psychology: Is It Viable to\nSustain Any Confidence in Social Psychology’s Knowledge Base?”\nPerspectives on Psychological Science 18 (4): 912–35. https://doi.org/10.1177/17456916221121815.\n\n\nBaumeister, R. F., and K. D. Vohs. 2016. “Misguided Effort with\nElusive Implications.” Perspectives on Psychological\nScience 11 (4): 574–75. https://doi.org/10.1177/1745691616652878.\n\n\nBekkers, R. 2024. “Replication Value: A Comment and\nAlternative.” https://doi.org/10.31234/osf.io/uj5g7.\n\n\nBennett, E. A. 2021. “Open Science from a Qualitative, Feminist\nPerspective: Epistemological Dogmas and a Call for Critical\nExamination.” Psychology of Women Quarterly 45 (4):\n448–56. https://doi.org/10.1177/03616843211036460.\n\n\nBerinsky, A. J., J. N. Druckman, and T. Yamamoto. 2021.\n“Publication Biases in Replication Studies.” Political\nAnalysis 29 (3): 370–84. https://doi.org/10.1017/pan.2020.34.\n\n\nBerkeley Initiative for Transparency in the Social Sciences. 2020.\n“Guide for Advancing Computational Reproducibility in the Social\nSciences.” https://bitss.github.io/ACRE/.\n\n\nBlock, J., and A. Kuckertz. 2018. “Seven Principles of Effective\nReplication Studies: Strengthening the Evidence Base of Management\nResearch.” Management Review Quarterly 68 (4): 355–59.\nhttps://doi.org/10.1007/s11301-018-0149-3.\n\n\nBorgstede, M., and M. Scholz. 2021. “Quantitative and Qualitative\nApproaches to Generalization and Replication–a Representationalist\nView.” Frontiers in Psychology 12: 605191. https://doi.org/10.3389/fpsyg.2021.605191.\n\n\nBoyce, V., B. Prystawski, A. B. Abutto, E. M. Chen, Z. Chen, H. Chiu,\nand M. C. and Frank. 2024. “Estimating the Replicability of\nPsychology Experiments After an Initial Failure to Replicate,”\nMay. https://doi.org/10.31234/osf.io/an3yb.\n\n\nBrandt, M. J., H. IJzerman, A. Dijksterhuis, F. J. Farach, J. Geller, R.\nGiner-Sorolla, and A. and Van’t Veer. 2014. “The Replication\nRecipe: What Makes for a Convincing Replication?” Journal of\nExperimental Social Psychology 50: 217–24. https://doi.org/10.1016/j.jesp.2013.10.005.\n\n\nBrodeur, A., A. Dreber, F. Hoces de la Guardia, and E. Miguel. 2024.\n“Reproduction and Replication at Scale.” Nature Human\nBehaviour 8 (1): 2–3. https://doi.org/10.1038/s41562-023-01807-2.\n\n\nBryan, C. J., D. S. Yeager, and J. M. O’Brien. 2019. “Replicator\nDegrees of Freedom Allow Publication of Misleading Failures to\nReplicate.” Proceedings of the National Academy of\nSciences 116 (51): 25535–45. https://doi.org/10.1073/pnas.1910951116.\n\n\nCalder, B. J., L. W. Phillips, and A. M. Tybout. 1981. “Designing\nResearch for Application.” Journal of Consumer Research\n8 (2): 197–207. https://doi.org/10.1086/208856.\n\n\nCarter, E. C., F. D. Schönbrodt, W. M. Gervais, and J. Hilgard. 2019.\n“Correcting for Bias in Psychology: A Comparison of Meta-Analytic\nMethods.” Advances in Methods and Practices in Psychological\nScience 2 (2): 115–44. https://doi.org/10.1177/2515245919847196.\n\n\nChartrand, T. L., and J. A. Bargh. 1999. “The Chameleon Effect:\nThe Perception–Behavior Link and Social Interaction.” Journal\nof Personality and Social Psychology 76 (6): 893. https://doi.org/10.1037/0022-3514.76.6.893.\n\n\nClarke, B., P. Y. (K.) Lee, S. R. Schiavone, M. Rhemtulla, and S.\nVazire. 2024. “The Prevalence of Direct Replication Articles in\nTop-Ranking Psychology Journals.” American Psychologist.\nhttps://doi.org/10.1037/amp0001385.\n\n\nCole, N. L., S. Ulpts, A. Bochynska, E. Kormann, M. Good, B. Leitner,\nand T. Ross-Hellauer. 2024. “Reproducibility and Replicability of\nQualitative Research: An Integrative Review of Concepts, Barriers and\nEnablers.” https://doi.org/10.31222/osf.io/n5zkw_v1.\n\n\nCortina, J. M., T. Köhler, and L. C. Aulisi. 2023. “Current\nReproducibility Practices in Management: What They Are Versus What They\nCould Be.” Journal of Management Scientific Reports 1\n(3-4): 171–205. https://doi.org/10.1177/27550311231202696.\n\n\nCowan, N., C. Belletier, J. M. Doherty, A. J. Jaroslawska, S. Rhodes, A.\nForsberg, M. Naveh-Benjamin, P. Barrouillet, V. Camos, and R. H. Logie.\n2020. “How Do Scientific Views Change? Notes from an Extended\nAdversarial Collaboration.” Perspectives on Psychological\nScience 15 (4): 1011–25. https://doi.org/10.1177/1745691620906415.\n\n\nDreber, A., and M. Johannesson. 2024. “A Framework for Evaluating\nReproducibility and Replicability in Economics.” Economic\nInquiry. https://doi.org/10.1111/ecin.13244.\n\n\nDunlap, K. 1926. “The Experimental Methods of Psychology.”\nIn Psychologies of 1925, edited by C. Murchison, 331–51. Clark\nUniversity Press. https://doi.org/10.1037/11020-022.\n\n\nErrington, T. M., M. Mathur, C. K. Soderberg, A. Denis, N. Perfito, E.\nIorns, and B. A. Nosek. 2021. “Investigating the Replicability of\nPreclinical Cancer Biology.” eLife 10: e71601. https://doi.org/10.7554/eLife.71601.\n\n\nFeldman, G. 2025. “The Value of Replications Goes Beyond\nReplicability and Is Associated with the Value of the Research It\nReplicates: Commentary on Isager Et Al., 2021.” Meta\nPsychology 9. https://doi.org/10.15626/MP.2024.4326.\n\n\nField, S. M., R. Hoekstra, L. Bringmann, and D. van Ravenzwaaij. 2019.\n“When and Why to Replicate: As Easy as 1, 2, 3?”\nCollabra: Psychology 5 (1): 46. https://doi.org/10.1525/collabra.218.\n\n\nFrancis, G. 2012. “Too Good to Be True: Publication Bias in Two\nProminent Studies from Experimental Psychology.” Psychonomic\nBulletin & Review 19: 151–56. https://doi.org/10.3758/s13423-012-0227-9.\n\n\nFriese, M., D. D. Loschelder, K. Gieseler, J. Frankenbach, and M.\nInzlicht. 2019. “Is Ego Depletion Real? An Analysis of\nArguments.” Personality and Social Psychology Review 23\n(2): 107–31. https://doi.org/10.1177/1088868318762183.\n\n\nHardwicke, T. E., and E. J. Wagenmakers. 2023. “Reducing Bias,\nIncreasing Transparency and Calibrating Confidence with\nPreregistration.” Nature Human Behaviour 7 (1): 15–26.\nhttps://doi.org/10.1038/s41562-022-01497-2.\n\n\nHawkins, R. X., E. N. Smith, C. Au, J. M. Arias, R. Catapano, E.\nHermann, and M. C. and Frank. 2018. “Improving the Replicability\nof Psychological Science Through Pedagogy.” Advances in\nMethods and Practices in Psychological Science 1 (1): 7–18. https://doi.org/10.1177/2515245917740427.\n\n\nHeathers, J. 2025. “An Introduction to Forensic\nMetascience.” https://doi.org/10.5281/zenodo.14871843.\n\n\nHeirene, R., D. LaPlante, E. Louderback, B. Keen, M. Bakker, A.\nSerafimovska, and S. Gainsbury. 2024. “Preregistration Specificity\nand Adherence: A Review of Preregistered Gambling Studies and\nCross-Disciplinary Comparison.” Meta-Psychology 8. https://doi.org/10.15626/MP.2021.2909.\n\n\nHeld, L., S. Pawel, and C. Micheloud. 2024. “The Assessment of\nReplicability Using the Sum of p-Values.” Royal Society Open\nScience 11 (8): 240149. https://doi.org/10.1098/rsos.240149.\n\n\nHenriques, S. O., N. Rzayeva, S. Pinfield, and L. Waltman. 2023.\n“Preprint Review Services: Disrupting the Scholarly Communication\nLandscape?” https://doi.org/10.31235/osf.io/8c6xm.\n\n\nHeroux, Michael A., Lorena A. Barba, Manish Parashar, Victoria Stodden,\nand Michela Taufer. 2018. “Toward a Compatible Reproducibility\nTaxonomy for Computational and Computing Sciences.” https://doi.org/10.2172/1481626.\n\n\nHöffler, J. H. 2017. “ReplicationWiki: Improving Transparency in\nSocial Sciences Research.” D-Lib Magazine 23 (3): 1. https://doi.org/10.1045/march2017-hoeffler.\n\n\nHuang, F. L., and A. B. Huang. 2024. “Replication Studies Using\nSecondary or Nonexperimental Datasets.” School Psychology\nReview, 1–15. https://doi.org/10.1080/2372966X.2024.2346781.\n\n\nHüffmeier, J., J. Mazei, and T. Schultze. 2016. “Reconceptualizing\nReplication as a Sequence of Different Studies: A Replication\nTypology.” Journal of Experimental Social Psychology 66:\n81–92. https://doi.org/10.1016/j.jesp.2015.09.009.\n\n\nHummel, T., and J. Manner. 2024. “A Literature Review on\nReproducibility Studies in Computer Science.” In Proceedings\nof the 16th ZEUS Workshop on Services and Their Composition (ZEUS\n2024)(CEUR). Vol. 3673.\n\n\nIsager, P. M., R. C. M. van Aert, Š. Bahník, M. J. Brandt, K. A. DeSoto,\nR. Giner-Sorolla, J. I. Krueger, et al. 2023. “Deciding What to\nReplicate: A Decision Model for Replication Study Selection Under\nResource and Knowledge Constraints.” Psychological\nMethods 28 (2): 438–51. https://doi.org/10.1037/met0000438.\n\n\nJacowitz, K. E., and D. Kahneman. 1995. “Measures of Anchoring in\nEstimation Tasks.” Personality and Social Psychology\nBulletin 21 (11): 1161–66. https://doi.org/10.1177/01461672952111004.\n\n\nJekel, M., S. Fiedler, R. Allstadt Torras, D. Mischkowski, A. R.\nDorrough, and A. Glöckner. 2020. “How to Teach Open Science\nPrinciples in the Undergraduate Curriculum—the Hagen Cumulative Science\nProject.” Psychology Learning & Teaching 19 (1):\n91–106. https://doi.org/10.1177/1475725719868149.\n\n\nKamermans, K. L., L. Dudda, T. Daikoku, and S. Verheyen. 2025.\n“The Is-Ought Problem in Deciding What to Replicate: Which Motives\nGuide Current Replication Practices?” https://doi.org/10.31234/osf.io/6xdy2_v2.\n\n\nKarhulahti, V., M. Martončik, and M. Adamkovic. 2024.\n“Pre-Replication in Meaningful Science.” https://doi.org/10.31234/osf.io/5gn7m.\n\n\nKing, G. 1995. “Replication, Replication.” PS:\nPolitical Science & Politics 28 (3): 444–52. https://doi.org/10.2307/420301.\n\n\nKlein, R. A., K. A. Ratliff, M. Vianello, R. B. Adams Jr, Š. Bahník, M.\nJ. Bernstein, and B. A. and Nosek. 2014. “Investigating Variation\nin Replicability.” Social Psychology. https://doi.org/10.1027/1864-9335/a000178.\n\n\nKöhler, T., and J. M. Cortina. 2021. “Play It Again, Sam! An\nAnalysis of Constructive Replication in the Organizational\nSciences.” Journal of Management 47 (2): 488–518. https://doi.org/10.1177/0149206319843985.\n\n\nLakens, D. 2022. “Sample Size Justification.” Collabra:\nPsychology 8 (1): 33267. https://doi.org/10.1525/collabra.33267.\n\n\n———. 2024. “When and How to Deviate from a\nPreregistration.” Collabra: Psychology 10 (1). https://doi.org/10.1525/collabra.117094.\n\n\nLakens, D., and A. J. Etz. 2017. “Too True to Be Bad: When Sets of\nStudies with Significant and Nonsignificant Findings Are Probably\nTrue.” Social Psychological and Personality Science 8\n(8): 875–81. https://doi.org/10.1177/1948550617693058.\n\n\nLandy, J. F., M. L. Jia, I. L. Ding, D. Viganola, W. Tierney, A. Dreber,\nand and Crowdsourcing Hypothesis Tests Collaboration. 2020.\n“Crowdsourcing Hypothesis Tests: Making Transparent How Design\nChoices Shape Research Results.” Psychological Bulletin\n146 (5): 451. https://doi.org/10.1037/bul0000220.\n\n\nLash, T. L., L. J. Collin, and M. E. Van Dyke. 2018. “The\nReplication Crisis in Epidemiology: Snowball, Snow Job, or Winter\nSolstice?” Current Epidemiology Reports 5: 175–83.\n\n\nLeBel, E. P., R. J. McCarthy, B. D. Earp, M. Elson, and W. Vanpaemel.\n2018. “A Unified Framework to Quantify the Credibility of\nScientific Findings.” Advances in Methods and Practices in\nPsychological Science 1 (3): 389–402. https://doi.org/10.1177/2515245918787489.\n\n\nMahoney, M. J. 1977. “Publication Prejudices: An Experimental\nStudy of Confirmatory Bias in the Peer Review System.”\nCognitive Therapy and Research 1: 161–75. https://doi.org/10.1007/BF01173636.\n\n\nMakel, M. C., J. A. Plucker, and B. Hegarty. 2012. “Replications\nin Psychology Research: How Often Do They Really Occur?”\nPerspectives on Psychological Science 7 (6): 537–42. https://doi.org/10.1177/1745691612460688.\n\n\nMazei, J., J. Hüffmeier, and T. Schultze. 2025. “Specification\nCurve and Reproducibility Dashboards for Social Science Research:\nRecommendations for Implementation.” Advances in Methods and\nPractices in Psychological Science.\n\n\nMcManus, K. 2024. “Replication Studies in Second Language\nAcquisition Research: Definitions, Issues, Resources, and Future\nDirections: Introduction to the Special Issue.” Studies in\nSecond Language Acquisition 46 (5): 1299–319. https://doi.org/10.1017/S0272263124000652.\n\n\nMiłkowski, M., W. M. Hensel, and M. Hohol. 2018. “Replicability or\nReproducibility? On the Replication Crisis in Computational Neuroscience\nand Sharing Only Relevant Detail.” Journal of Computational\nNeuroscience 45 (3): 163–72. https://doi.org/10.1007/s10827-018-0702-z.\n\n\nMoreau, D., and K. Wiebels. 2023. “Ten Simple Rules for Designing\nand Conducting Undergraduate Replication Projects.” PLOS\nComputational Biology 19 (3): e1010957. https://doi.org/10.1371/journal.pcbi.1010957.\n\n\nMunafò, M. R., C. D. Chambers, A. M. Collins, L. Fortunato, and M. R.\nMacleod. 2020. “Research Culture and Reproducibility.”\nTrends in Cognitive Sciences 24 (2): 91–93. https://doi.org/10.1016/j.tics.2019.12.002.\n\n\nNosek, B. A., and T. M. Errington. 2020. “What Is\nReplication?” PLoS Biology 18 (3): e3000691. https://doi.org/10.1371/journal.pbio.3000691.\n\n\nNuijten, M. B., and J. R. Polanin. 2020.\n“‘Statcheck’: Automatically Detect Statistical\nReporting Inconsistencies to Increase Reproducibility of\nMeta‐analyses.” Research Synthesis Methods 11 (5):\n574–79. https://doi.org/10.1002/jrsm.1408.\n\n\nNüst, D., and S. J. Eglen. 2021. “CODECHECK: An Open Science\nInitiative for the Independent Execution of Computations Underlying\nResearch Articles During Peer Review to Improve Reproducibility.”\nF1000Research 10: 253. https://doi.org/10.12688/f1000research.51738.2.\n\n\nOpen Science Collaboration. 2015. “Estimating the Reproducibility\nof Psychological Science.” Science 349 (6251): aac4716.\nhttps://doi.org/10.1126/science.aac4716.\n\n\nPatil, P., R. D. Peng, and J. T. Leek. 2016. “A Statistical\nDefinition for Reproducibility and Replicability.”\nBioRxiv, 066803. https://doi.org/10.1101/066803.\n\n\nPennington, C. R. 2023. A Student’s Guide to Open Science: Using the\nReplication Crisis to Reform Psychology. Open University Press.\n\n\nPerry, T., R. Morris, and R. Lea. 2022. “A Decade of Replication\nStudy in Education? A Mapping Review (2011–2020).”\nEducational Research and Evaluation 27 (1-2): 12–34. https://doi.org/10.1080/13803611.2021.2022315.\n\n\nPittelkow, M. M., S. M. Field, P. M. Isager, A. E. van’t Veer, T.\nAnderson, S. N. Cole, and D. and Van Ravenzwaaij. 2023. “The\nProcess of Replication Target Selection in Psychology: What to\nConsider?” Royal Society Open Science 10 (2): 210586. https://doi.org/10.1098/rsos.210586.\n\n\nPittelkow, M. M., R. Hoekstra, J. Karsten, and D. van Ravenzwaaij. 2021.\n“Replication Target Selection in Clinical Psychology: A Bayesian\nand Qualitative Reevaluation.” Clinical Psychology: Science\nand Practice 28 (2): 210. https://doi.org/10.1037/cps0000013.\n\n\nPittelkow, M., S. M. Field, and D. van Ravenzwaaij. 2025.\n“Thinking Beyond RVCN: Addressing the Complexity of Replication\nTarget Selection.” https://doi.org/10.31234/osf.io/6tmyx_v2.\n\n\nPownall, M. 2022. “Is Replication Possible for Qualitative\nResearch?” https://doi.org/10.31234/osf.io/dwxeg.\n\n\nProtzko, J. 2018. “Null-Hacking, a Lurking Problem.” https://doi.org/10.31234/osf.io/9y3mp.\n\n\nRöseler, L., L. Kaiser, C. Doetsch, N. Klett, C. Seida, A. Schütz, and\nY. and Zhang. 2024. “The Replication Database: Documenting the\nReplicability of Psychological Science.” Journal of Open\nPsychology Data 12 (1): 8. https://doi.org/10.5334/jopd.101.\n\n\nSchimmack, U. 2012. “The Ironic Effect of Significant Results on\nthe Credibility of Multiple-Study Articles.” Psychological\nMethods 17 (4): 551. https://doi.org/10.1037/a0029487.\n\n\nSchmidt, S. 2009. “Shall We Really Do It Again? The Powerful\nConcept of Replication Is Neglected in the Social Sciences.”\nReview of General Psychology 13 (2): 90–100. https://doi.org/10.1037/a0015108.\n\n\nSchöch, C. 2023. “Repetitive Research: A Conceptual Space and\nTerminology of Replication, Reproduction, Revision, Reanalysis,\nReinvestigation and Reuse in Digital Humanities.”\nInternational Journal of Digital Humanities 5 (2): 373–403. https://doi.org/10.1007/s42803-023-00073-y.\n\n\nSimonsohn, U. 2015. “Small Telescopes: Detectability and the\nEvaluation of Replication Results.” Psychological\nScience 26 (5): 559–69. https://doi.org/10.1177/0956797614567341.\n\n\nSimonsohn, U., J. P. Simmons, and L. D. Nelson. 2020.\n“Specification Curve Analysis.” Nature Human\nBehaviour 4: 1208–14. https://doi.org/10.1038/s41562-020-0912-z.\n\n\nSoderberg, C. K., T. M. Errington, S. R. Schiavone, and and others.\n2021. “Initial Evidence of Research Quality of Registered Reports\nCompared with the Standard Publishing Model.” Nature Human\nBehaviour 5: 990–97. https://doi.org/10.1038/s41562-021-01142-4.\n\n\nSoto, C. J. 2019. “How Replicable Are Links Between Personality\nTraits and Consequential Life Outcomes? The Life Outcomes of Personality\nReplication Project.” Psychological Science 30 (5):\n711–27. https://doi.org/10.1177/0956797619831612.\n\n\nSyed, M. 2023. “Replication or Generalizability? How Flexible\nInferences Uphold Unfounded Universal Claims,” May. https://doi.org/10.31234/osf.io/znv5r.\n\n\nTsang, E. W., and K. M. Kwan. 1999. “Replication and Theory\nDevelopment in Organizational Science: A Critical Realist\nPerspective.” Academy of Management Review 24 (4):\n759–80. https://doi.org/10.2307/259353.\n\n\nUrminsky, O., and B. J. Dietvorst. 2024. “Taking the Full Measure:\nIntegrating Replication into Research Practice to Assess\nGeneralizability.” Journal of Consumer Research 51 (1):\n157–68. https://doi.org/10.1093/jcr/ucae007.\n\n\nVoelkl, B., R. Heyard, D. Fanelli, K. E. Wever, L. Held, Z. Maniadis,\nand H. and Würbel. 2025. “Defining Reproducibility.” https://doi.org/10.17605/OSF.IO/BR9SP.\n\n\nVohs, K. D., B. J. Schmeichel, S. Lohmann, Q. F. Gronau, A. J. Finley,\nS. E. Ainsworth, J. L. Alquist, et al. 2021. “A Multisite\nPreregistered Paradigmatic Test of the Ego-Depletion Effect.”\nPsychological Science 32 (10): 1566–81. https://doi.org/10.1177/0956797621989733.\n\n\nWilkinson, M. D., M. Dumontier, I. J. Aalbersberg, G. Appleton, M.\nAxton, A. Baak, and B. and Mons. 2016. “The FAIR Guiding\nPrinciples for Scientific Data Management and Stewardship.”\nScientific Data 3 (1): 1–9. https://doi.org/10.1038/sdata.2016.18.\n\n\nWillroth, E. C., and O. E. Atherton. 2024. “Best Laid Plans: A\nGuide to Reporting Preregistration Deviations.” Advances in\nMethods and Practices in Psychological Science 7 (1):\n25152459231213802. https://doi.org/10.1177/25152459231213802.\n\n\nYarkoni, T. 2013. “‘What We Can and Can’t Learn from the\nMany Labs Replication Project’.”\nTalyarkoni.org/Blog.\n\n\nZwaan, R. A., A. Etz, R. E. Lucas, and M. B. Donnellan. 2018.\n“Making Replication Mainstream.” Behavioral and Brain\nSciences 41: e120. https://doi.org/10.1017/S0140525X17001972.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "contributions.html",
    "href": "contributions.html",
    "title": "Appendix A — Author Contributions",
    "section": "",
    "text": "No.\nAuthor (last name, first name)\nORCID-ID\nContribution (CRediT)\nAffiliation\n\n\n\n\n1\nRöseler, Lukas* ✉\nhttps://orcid.org/0000-0002-6446-1901\nConceptualization, Project Administration, Writing – original draft, Writing – review & editing\nMünster Center for Open Science, University of Münster\n\n\n2\nWallrich, Lukas* ✉\nhttps://orcid.org/0000-0003-2121-5177\nWriting – original draft, Writing – review & editing\nBirkbeck Business School, University of London\n\n\n3\nHartmann, Helena\nhttps://orcid.org/0000-0002-1331-6683\nWriting – original draft, Writing – review & editing\nDepartment for Neurology and Center for Translational Neuro- and Behavioral Sciences (C-TNBS), University Hospital Essen\n\n\n4\nHüffmeier, Joachim\nhttps://orcid.org/0000-0002-0490-7035\nWriting - review & editing\nTU Dortmund University\n\n\n5\nGoltermann, Janik\nhttps://orcid.org/0000-0003-3087-1002\nWriting - review & editing\nUniversity of Münster, Institute for Translational Psychiatry\n\n\n6\nCharlotte R. Pennington\nhttps://orcid.org/0000-0002-5259-642X\nWriting - review & editing\nSchool of Psychology, Aston University, Birmingham\n\n\n7\nVeronica Boyce\nhttps://orcid.org/0000-0002-8890-2775\nWriting - review & editing\nDepartment of Psychology, Stanford University\n\n\n8\nSarahanne M. Field\nhttps://orcid.org/0000-0001-7874-1261\nWriting, review & editing\nDepartment of pedagogy, University of Groningen\n\n\n9\nMerle-Marie Pittelkow\nhttps://orcid.org/0000-0002-7487-7898\nWriting, review & editing\nBerlin Institute of Health at Charité - Universitätsmedizin Berlin\n\n\n10\nDon van Ravenzwaaij\nhttps://orcid.org/0000-0002-5030-4091\nWriting, review & editing\nDepartment of psychology, University of Groningen\n\n\n11\nPriya Silverstein\nhttps://orcid.org/0000-0003-0095-339X\nWriting, review & editing\nCenter for Neuroscience and Cell Biology, University of Coimbra; Institute for Globally Distributed Open Research and Education\n\n\n12\nLuisa Altegoer\nhttps://orcid.org/0000-0001-8466-7328\nWriting - review & editing\nUniversity of Münster, Institute for Translational Psychiatry\n\n\n13\nAzevedo, Flavio✉\nhttps://orcid.org/0000-0001-9000-8513\nWriting – original draft, Writing – review & editing\nUniversity of Utrecht, Department of Interdisciplinary Social Science\n\n\n\n*shared first authorship\n\nB Potential Conflicts of Interest\nA large proportion of the authors are members of FORRT, an organization dedicated to integrating open and reproducible science into higher education. LR, LW, FA, and JG are inaugural editors of the in-development journal Replication Research (https://replicationresearch.org). Besides their conviction of the value of replications, their current project’s success relies on researchers conducting reproductions and replications. LR is the managing director of an institutional open science center and proponent of repetitive research. The authors declare that they have no further potential conflicts of interest.\n\n\nC Funding\nLR received funding from the University of Münster and the ‘Landesinitiative opennaccess.nrw’. LW and LR received funding from ‘UK Research and Innovation’. LW, HH, and FA received funding from the ‘Nederlandse Organisatie voor Wetenschappelijk Onderzoek’. JG received funding by ‘Innovative Medizinische Forschung’ (IMF) of the medical faculty of the University of Münster (GO122301). HH was supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - Project-ID 422744262 - TRR 289 (gefördert durch die Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 422744262 - TRR 289).\n\n\nD Acknowledgments\nThis work is an initiative from The Framework for Open and Reproducible Research Training (FORRT; https://forrt.org), and all core-team authors are active members of FORRT’s Replication Hub (https://forrt.org/replication-hub).\nWe thank Patrick Smela for valuable ideas on defining replication success and Abel Brodeur for suggestions about definitions and the relationship between reproducibility and replicability.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Author Contributions</span>"
    ]
  },
  {
    "objectID": "appendix_templates.html",
    "href": "appendix_templates.html",
    "title": "Appendix B — Asking for materials and data",
    "section": "",
    "text": "Dear [name of author(s)],\nwe are conducting replication research using some of your research. Specifically, we [brief name of the phenomenon and study that was replicated]. [We do this because … e.g., your research addresses a very important question.] Can you please send us the following materials to help us design a replication as close as possible to your original study?\n\n[list of required materials/data/code]\n[list of required materials/data/code]\nCitation of original study: [add citation] We are looking forward to your responses! Thank you [Your name] Asking for comments on an experimental paradigm Dear [name of authors], We are planning a replication of some of your research. Specifically we are aiming to replicate your study [study details and citation]. [we are interested in these findings because …] I’m writing to share a mock-up of the replication to get your feedback on whether this paradigm accurately captures the design of your study. Please let me know if you have any comments or concerns that you’d like to share. Here’s a link to my paradigm. Any insights you have into details that differ from your own study would be much appreciated. I will be replicating your experiment on [planned recruitment sample]. [I know this is a deviation from the original population you tested, and I will note this sample decision prominently in any writeups.]\n\nThanks again,\n[Your name]\n\nC Asking for comments on replication results\nDear [name of author(s)],\nwe have conducted replication research using some of your research. Specifically, we [brief name of the phenomenon and study that was replicated]. In our study [description of results]. We want to provide you with an opportunity to comment on these findings. We plan to publish the replication report via [paper or publication platform, e.g., FORRT’s Replication Hub], which asks replication studies to be submitted alongside comments from the authors of the original study. Your comment – if you choose to give one – will be part of the report.\n- Citation of original study: [add citation]\n- Replication study:[add link to document or attach it to the e-mail]\nWe are looking forward to your responses!\nThank you\n[Your name]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Asking for materials and data</span>"
    ]
  }
]