[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Handbook on Conducting Reproduction and Replication Studies",
    "section": "",
    "text": "1 Abstract\nThe practice of repeatedly testing published results with the same data (reproduction) or new data (replication) is currently gaining traction in the social sciences, owing to multiple failures to reproduce and replicate published findings. Along with increased skepticism have come guidelines for the repeated testing of hypotheses from various disciplines and fields. This guide aims to enable researchers to conduct high-quality reproductions and replications across social science disciplines. First we summarize recent developments, then provide a comprehensive guide to carrying out reproductions and replications, and finally present an example for how guidance needs to be tailored for specific fields. Our guide covers the entire research process: choosing a target study, deciding between different types of reproductions and replications, planning and running the new study, analyzing the results, discussing outcomes in the light of potential differences, and publishing a report.\nKeywords: replication, repetitive research, reproducibility, meta-science, meta-research, open science, open research, open scholarship",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Abstract</span>"
    ]
  },
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "2  Background",
    "section": "",
    "text": "“The proof established by the test must have a specific form, namely, repeatability. The issue of the experiment must be a statement of the hypothesis, the conditions of test, and the results, in such form that another experimenter, from the description alone, may be able to repeat the experiment. Nothing is accepted as proof, in psychology or in any other science, which does not conform to this requirement.” – (Dunlap 1926)\nRepeatability is the cornerstone of many sciences: A majority of the scientific progress rests on the successful accumulation of evidence for claims through reproduction and replications to establish robust discoveries. Reproductions and replications, that is repeated testing of a hypothesis with the same (reproduction) or different (replication) data, are necessary.\nCumulative science without repetition is costly. The aim of this guide is to empower researchers to conduct high-quality reproductions and replications and thereby contribute to making their fields of research more cumulative and robust. Issues of replicability have been discussed across many disciplines, such as psychology ((Open Science Collaboration 2015)), economics ((Dreber and Johannesson 2024)), biology ((Errington et al. 2021)), marketing ((Urminsky and Dietvorst 2024)), linguistics ((McManus 2024)), computer science ((Hummel and Manner 2024)) and epidemiology ((Lash, Collin, and Van Dyke 2018)) and the number of replications has been rising sharply (see Figure 1).\nFigure 1\nNumber of replication studies by year of publication based on the FORRT Replication Database (FReD, (Röseler et al. 2024)) based on the version from July 16, 2025. Code to reproduce the figure: https://osf.io/dznrb.\n![][image1](img/bVY_Image_1.jpeg)\nWhile the number of replication and reproduction studies has increased, the overall proportion of them is still very small, with reviews finding yearly replication rates of up to 1% ((Perry, Morris, and Lea 2022)). Moreover, much of the guidance on replications is being developed actively ((Clarke et al. 2024)) and in narrow parts of science, which leads to fragmentation, siloing, and potentially inconsistent information.\nHere we attempt to integrate useful guidelines (e.g., (Block and Kuckertz 2018; Jekel et al. 2020)) into a comprehensive overview that allows diverse fields to profit from each other. In sum, this guide provides information about the entire process of research allowing researchers at all career stages to plan, conduct, and publish reproduction and replication studies. We limit our scope to quantitative research, given that the concept of reproducibility and replicability itself is highly contested among qualitative researchers (see Makel, Plucker, and Hegarty (2012); Cole et al. (2024); Pownall (2022); Bennett (2021)).\n\n\n\n\nBennett, E. A. 2021. “Open Science from a Qualitative, Feminist Perspective: Epistemological Dogmas and a Call for Critical Examination.” Psychology of Women Quarterly 45 (4): 448–56. https://doi.org/10.1177/03616843211036460.\n\n\nBlock, J., and A. Kuckertz. 2018. “Seven Principles of Effective Replication Studies: Strengthening the Evidence Base of Management Research.” Management Review Quarterly 68 (4): 355–59. https://doi.org/10.1007/s11301-018-0149-3.\n\n\nClarke, B., P. Y. (K.) Lee, S. R. Schiavone, M. Rhemtulla, and S. Vazire. 2024. “The Prevalence of Direct Replication Articles in Top-Ranking Psychology Journals.” American Psychologist. https://doi.org/10.1037/amp0001385.\n\n\nCole, N. L., S. Ulpts, A. Bochynska, E. Kormann, M. Good, B. Leitner, and T. Ross-Hellauer. 2024. “Reproducibility and Replicability of Qualitative Research: An Integrative Review of Concepts, Barriers and Enablers.” https://doi.org/10.31222/osf.io/n5zkw_v1.\n\n\nDreber, A., and M. Johannesson. 2024. “A Framework for Evaluating Reproducibility and Replicability in Economics.” Economic Inquiry. https://doi.org/10.1111/ecin.13244.\n\n\nDunlap, K. 1926. “The Experimental Methods of Psychology.” In Psychologies of 1925, edited by C. Murchison, 331–51. Clark University Press. https://doi.org/10.1037/11020-022.\n\n\nErrington, T. M., M. Mathur, C. K. Soderberg, A. Denis, N. Perfito, E. Iorns, and B. A. Nosek. 2021. “Investigating the Replicability of Preclinical Cancer Biology.” eLife 10: e71601. https://doi.org/10.7554/eLife.71601.\n\n\nHummel, T., and J. Manner. 2024. “A Literature Review on Reproducibility Studies in Computer Science.” In Proceedings of the 16th ZEUS Workshop on Services and Their Composition (ZEUS 2024)(CEUR). Vol. 3673.\n\n\nJekel, M., S. Fiedler, R. Allstadt Torras, D. Mischkowski, A. R. Dorrough, and A. Glöckner. 2020. “How to Teach Open Science Principles in the Undergraduate Curriculum—the Hagen Cumulative Science Project.” Psychology Learning & Teaching 19 (1): 91–106. https://doi.org/10.1177/1475725719868149.\n\n\nLash, T. L., L. J. Collin, and M. E. Van Dyke. 2018. “The Replication Crisis in Epidemiology: Snowball, Snow Job, or Winter Solstice?” Current Epidemiology Reports 5: 175–83.\n\n\nMakel, M. C., J. A. Plucker, and B. Hegarty. 2012. “Replications in Psychology Research: How Often Do They Really Occur?” Perspectives on Psychological Science 7 (6): 537–42. https://doi.org/10.1177/1745691612460688.\n\n\nMcManus, K. 2024. “Replication Studies in Second Language Acquisition Research: Definitions, Issues, Resources, and Future Directions: Introduction to the Special Issue.” Studies in Second Language Acquisition 46 (5): 1299–319. https://doi.org/10.1017/S0272263124000652.\n\n\nOpen Science Collaboration. 2015. “Estimating the Reproducibility of Psychological Science.” Science 349 (6251): aac4716. https://doi.org/10.1126/science.aac4716.\n\n\nPerry, T., R. Morris, and R. Lea. 2022. “A Decade of Replication Study in Education? A Mapping Review (2011–2020).” Educational Research and Evaluation 27 (1-2): 12–34. https://doi.org/10.1080/13803611.2021.2022315.\n\n\nPownall, M. 2022. “Is Replication Possible for Qualitative Research?” https://doi.org/10.31234/osf.io/dwxeg.\n\n\nRöseler, L., L. Kaiser, C. Doetsch, N. Klett, C. Seida, A. Schütz, and Y. and Zhang. 2024. “The Replication Database: Documenting the Replicability of Psychological Science.” Journal of Open Psychology Data 12 (1): 8. https://doi.org/10.5334/jopd.101.\n\n\nUrminsky, O., and B. J. Dietvorst. 2024. “Taking the Full Measure: Integrating Replication into Research Practice to Assess Generalizability.” Journal of Consumer Research 51 (1): 157–68. https://doi.org/10.1093/jcr/ucae007.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "planning.html",
    "href": "planning.html",
    "title": "5  Planning and Conducting Reproductions and Replications",
    "section": "",
    "text": "6 Planning and Conducting Reproductions and Replications\nPlanning depends on whether the focus is on a certain method or a theory, that is whether the replication will be close or conceptual. Table 6.1 provides an overview of reproduction and replication types, or more generally “repetitive research” ((Schöch 2023)), drawn from different resources (e.g., (Dreber and Johannesson 2024; Hüffmeier, Mazei, and Schultze 2016); for an alternative taxonomy see also (Cortina, Köhler, and Aulisi 2023)). The decision between these types is the first step in planning.\nIn addition, the formation of the replication team is important, as replications can take substantial resources. Notably, repetitive research has successfully been conducted collaboratively with graduate and undergraduate students (e.g., (Boyce et al. 2024; Hawkins et al. 2018; Jekel et al. 2020; Moreau and Wiebels 2023)) and we recommend the use of replication studies to engage students of different levels in conducting and publishing research.",
    "crumbs": [
      "The Replication Process",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Planning and Conducting Reproductions and Replications</span>"
    ]
  },
  {
    "objectID": "planning.html#sec-post-publication-conversations",
    "href": "planning.html#sec-post-publication-conversations",
    "title": "5  Planning and Conducting Reproductions and Replications",
    "section": "6.1 Post Publication Conversations",
    "text": "6.1 Post Publication Conversations\nWhen planning the replication study, additional knowledge should be taken into account such as any discussions of the original finding. There can be other studies citing the original studies, criticizing them, disconfirming their underlying theory, identifying errors, reinterpreting the finding, or making suggestions for replications. All of these might highlight considerations that need to be taken into account when designing a replication study that robustly tests the original claim or its generalisability.\nThus, replication researchers should look for post-publication discussions on the target study such as published comments and reviews, blog posts, or discussions on social media. These can often be found via Altmetric (https://www.altmetric.com) or other tools that allow researchers to quickly identify discussions on social media or news outlets beyond scientific journals (PubPeer [https://pubpeer.com], Hypothes.is [https://web.hypothes.is]), or the in-development platform Alphaxiv.org [https://www.alphaxiv.org/]; for a review see (Henriques et al. 2023)).",
    "crumbs": [
      "The Replication Process",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Planning and Conducting Reproductions and Replications</span>"
    ]
  },
  {
    "objectID": "planning.html#sec-reproduction-before-replication",
    "href": "planning.html#sec-reproduction-before-replication",
    "title": "5  Planning and Conducting Reproductions and Replications",
    "section": "6.2 Reproduction before Replication",
    "text": "6.2 Reproduction before Replication\nMany features of a replication study rest on the correctness of the original report. A reproduction allows researchers to investigate this by being able to uncover coding errors, fraud, robustness to analytical decisions, and generalizability. To make efficient use of resources, we encourage researchers to investigate the original finding’s reproducibility and robustness first. In other words, ideally, reproductions should take place before planning and conducting a replication study. Depending on the availability of the code and data, these can take several minutes to weeks.\nIf the original code and dataset are available, researchers can try to numerically reproduce the results. Beware, however, that differences in software versions or default settings may lead to slight deviations or require corrections in some cases (for a large-scale test of reproducibility see Brodeur et al., 2024). Similarly, the lack of a set seed for random number generators can mean that analyses relying on random numbers (e.g., bootstrapping) cannot be exactly reproduced. If no analysis script is available, analyses need to be recreated from the descriptions in the report (recoding reproduction). In this case, special attention should be paid to processing steps such as exclusion of outliers, transformation of variables, and handling of missing data. However, in many research areas information on these steps is often incomplete (Field et al., 2019); older research tends to be especially limited in terms of the methodological details they provide. In addition, we recommend testing the robustness of the original finding by making small alterations to the data processing and analyses procedure (robustness reproductions). For example, if the analyses were run for a subset of the data (e.g., participants aged 21 to 30 or without outliers ± 3 standard deviations), this subset can be changed (e.g., participants aged 18 to 30 or without outliers ± 2 standard deviations). Here, the initial focus should be on choices that are not determined by the theory that is presented, though this can also be used to explore the generalisability of some aspects of theory. Finally, if the original study was preregistered and the original code is available, reproduction researchers can check whether the original analyses adhere to the preregistered analysis plan.\nIf neither code nor data are available (or shared by the authors), no reproduction is possible. Researchers can still use automated tools to compare reported p-values with those that can be computed from test statistics via the website statcheck.io (where documents may be uploaded) or the corresponding R package ((Nuijten and Polanin 2020)).",
    "crumbs": [
      "The Replication Process",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Planning and Conducting Reproductions and Replications</span>"
    ]
  },
  {
    "objectID": "planning.html#sec-close-replication-before-conceptual-replication",
    "href": "planning.html#sec-close-replication-before-conceptual-replication",
    "title": "5  Planning and Conducting Reproductions and Replications",
    "section": "6.3 Close replication before conceptual replication",
    "text": "6.3 Close replication before conceptual replication\nIf the goal is to increase the generalizability of a specific finding, we also suggest starting with replications that adhere as close as possible to the original study (e.g., close replications) and only later conduct conceptual replications. Based on (Hüffmeier, Mazei, and Schultze 2016), we propose the typology and order of replication attempts in Figure 6.1. Note that there may be cases where the sequence of replications is not necessary, or where the context of the replication team requires a focus on generalisability to a specific context (see ?sec-interpretation).\n\n\n\n\n\n\nFigure 6.1: Sequence of replications from exact replications to conceptual replications under field conditions. Note: This an adaptation and update of the typology of replication studies by (Hüffmeier, Mazei, and Schultze 2016). The typology is conceptualized as a hierarchy of studies that together help to (i) establish the validity and replicability of new effects, (ii) exclude alternative explanations, (iii) test relevant boundary conditions, and (iv) test generalizability. Importantly, replications at any stage should not compromise any aspects of an original study, but rather (at the latest from the third study stage [constructive replications] onwards) try to improve one or more aspects of the original study, such as “[…] more valid measures, more critical control variables, a more realistic task, a more representative sample, or a design that allows for stronger conclusions regarding causality”, see (Köhler and Cortina 2021, 494)). (Köhler and Cortina 2021) term such replications “constructive replications” and caution against the conduct of “quasi-random” replications that vary features without clear rationale.\n\n\n\nAt the start, you will need to collect all available materials and data, and contact the original authors if there is something missing. It will often be necessary to contact the authors more than once because missing descriptions of details of the original study only become apparent once the replication study is planned. In most cases, the original paper identifies one of the authors as “corresponding author” with an e-mail address. We recommend a quick web search to check if this is the current email address, as researchers frequently change institutions and thus e-mail addresses. Sometimes, it may be most helpful to write to the last authors instead, who tend to have more stable e-mail addresses, or to copy all authors into the email Templates for asking for materials and sharing replication results are in the appendix. Note that original authors may not respond due to institutional changes or not being active in academia anymore.\n\n\n\n\nBoyce, V., B. Prystawski, A. B. Abutto, E. M. Chen, Z. Chen, H. Chiu, and M. C. and Frank. 2024. “Estimating the Replicability of Psychology Experiments After an Initial Failure to Replicate,” May. https://doi.org/10.31234/osf.io/an3yb.\n\n\nCortina, J. M., T. Köhler, and L. C. Aulisi. 2023. “Current Reproducibility Practices in Management: What They Are Versus What They Could Be.” Journal of Management Scientific Reports 1 (3-4): 171–205. https://doi.org/10.1177/27550311231202696.\n\n\nDreber, A., and M. Johannesson. 2024. “A Framework for Evaluating Reproducibility and Replicability in Economics.” Economic Inquiry. https://doi.org/10.1111/ecin.13244.\n\n\nHawkins, R. X., E. N. Smith, C. Au, J. M. Arias, R. Catapano, E. Hermann, and M. C. and Frank. 2018. “Improving the Replicability of Psychological Science Through Pedagogy.” Advances in Methods and Practices in Psychological Science 1 (1): 7–18. https://doi.org/10.1177/2515245917740427.\n\n\nHenriques, S. O., N. Rzayeva, S. Pinfield, and L. Waltman. 2023. “Preprint Review Services: Disrupting the Scholarly Communication Landscape?” https://doi.org/10.31235/osf.io/8c6xm.\n\n\nHüffmeier, J., J. Mazei, and T. Schultze. 2016. “Reconceptualizing Replication as a Sequence of Different Studies: A Replication Typology.” Journal of Experimental Social Psychology 66: 81–92. https://doi.org/10.1016/j.jesp.2015.09.009.\n\n\nJekel, M., S. Fiedler, R. Allstadt Torras, D. Mischkowski, A. R. Dorrough, and A. Glöckner. 2020. “How to Teach Open Science Principles in the Undergraduate Curriculum—the Hagen Cumulative Science Project.” Psychology Learning & Teaching 19 (1): 91–106. https://doi.org/10.1177/1475725719868149.\n\n\nKöhler, T., and J. M. Cortina. 2021. “Play It Again, Sam! An Analysis of Constructive Replication in the Organizational Sciences.” Journal of Management 47 (2): 488–518. https://doi.org/10.1177/0149206319843985.\n\n\nMoreau, D., and K. Wiebels. 2023. “Ten Simple Rules for Designing and Conducting Undergraduate Replication Projects.” PLOS Computational Biology 19 (3): e1010957. https://doi.org/10.1371/journal.pcbi.1010957.\n\n\nNuijten, M. B., and J. R. Polanin. 2020. “‘Statcheck’: Automatically Detect Statistical Reporting Inconsistencies to Increase Reproducibility of Meta‐analyses.” Research Synthesis Methods 11 (5): 574–79. https://doi.org/10.1002/jrsm.1408.\n\n\nSchöch, C. 2023. “Repetitive Research: A Conceptual Space and Terminology of Replication, Reproduction, Revision, Reanalysis, Reinvestigation and Reuse in Digital Humanities.” International Journal of Digital Humanities 5 (2): 373–403. https://doi.org/10.1007/s42803-023-00073-y.",
    "crumbs": [
      "The Replication Process",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Planning and Conducting Reproductions and Replications</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bennett, E. A. 2021. “Open Science from a Qualitative, Feminist\nPerspective: Epistemological Dogmas and a Call for Critical\nExamination.” Psychology of Women Quarterly 45 (4):\n448–56. https://doi.org/10.1177/03616843211036460.\n\n\nBlock, J., and A. Kuckertz. 2018. “Seven Principles of Effective\nReplication Studies: Strengthening the Evidence Base of Management\nResearch.” Management Review Quarterly 68 (4): 355–59.\nhttps://doi.org/10.1007/s11301-018-0149-3.\n\n\nBoyce, V., B. Prystawski, A. B. Abutto, E. M. Chen, Z. Chen, H. Chiu,\nand M. C. and Frank. 2024. “Estimating the Replicability of\nPsychology Experiments After an Initial Failure to Replicate,”\nMay. https://doi.org/10.31234/osf.io/an3yb.\n\n\nClarke, B., P. Y. (K.) Lee, S. R. Schiavone, M. Rhemtulla, and S.\nVazire. 2024. “The Prevalence of Direct Replication Articles in\nTop-Ranking Psychology Journals.” American Psychologist.\nhttps://doi.org/10.1037/amp0001385.\n\n\nCole, N. L., S. Ulpts, A. Bochynska, E. Kormann, M. Good, B. Leitner,\nand T. Ross-Hellauer. 2024. “Reproducibility and Replicability of\nQualitative Research: An Integrative Review of Concepts, Barriers and\nEnablers.” https://doi.org/10.31222/osf.io/n5zkw_v1.\n\n\nCortina, J. M., T. Köhler, and L. C. Aulisi. 2023. “Current\nReproducibility Practices in Management: What They Are Versus What They\nCould Be.” Journal of Management Scientific Reports 1\n(3-4): 171–205. https://doi.org/10.1177/27550311231202696.\n\n\nDreber, A., and M. Johannesson. 2024. “A Framework for Evaluating\nReproducibility and Replicability in Economics.” Economic\nInquiry. https://doi.org/10.1111/ecin.13244.\n\n\nDunlap, K. 1926. “The Experimental Methods of Psychology.”\nIn Psychologies of 1925, edited by C. Murchison, 331–51. Clark\nUniversity Press. https://doi.org/10.1037/11020-022.\n\n\nErrington, T. M., M. Mathur, C. K. Soderberg, A. Denis, N. Perfito, E.\nIorns, and B. A. Nosek. 2021. “Investigating the Replicability of\nPreclinical Cancer Biology.” eLife 10: e71601. https://doi.org/10.7554/eLife.71601.\n\n\nHawkins, R. X., E. N. Smith, C. Au, J. M. Arias, R. Catapano, E.\nHermann, and M. C. and Frank. 2018. “Improving the Replicability\nof Psychological Science Through Pedagogy.” Advances in\nMethods and Practices in Psychological Science 1 (1): 7–18. https://doi.org/10.1177/2515245917740427.\n\n\nHenriques, S. O., N. Rzayeva, S. Pinfield, and L. Waltman. 2023.\n“Preprint Review Services: Disrupting the Scholarly Communication\nLandscape?” https://doi.org/10.31235/osf.io/8c6xm.\n\n\nHüffmeier, J., J. Mazei, and T. Schultze. 2016. “Reconceptualizing\nReplication as a Sequence of Different Studies: A Replication\nTypology.” Journal of Experimental Social Psychology 66:\n81–92. https://doi.org/10.1016/j.jesp.2015.09.009.\n\n\nHummel, T., and J. Manner. 2024. “A Literature Review on\nReproducibility Studies in Computer Science.” In Proceedings\nof the 16th ZEUS Workshop on Services and Their Composition (ZEUS\n2024)(CEUR). Vol. 3673.\n\n\nJekel, M., S. Fiedler, R. Allstadt Torras, D. Mischkowski, A. R.\nDorrough, and A. Glöckner. 2020. “How to Teach Open Science\nPrinciples in the Undergraduate Curriculum—the Hagen Cumulative Science\nProject.” Psychology Learning & Teaching 19 (1):\n91–106. https://doi.org/10.1177/1475725719868149.\n\n\nKöhler, T., and J. M. Cortina. 2021. “Play It Again, Sam! An\nAnalysis of Constructive Replication in the Organizational\nSciences.” Journal of Management 47 (2): 488–518. https://doi.org/10.1177/0149206319843985.\n\n\nLash, T. L., L. J. Collin, and M. E. Van Dyke. 2018. “The\nReplication Crisis in Epidemiology: Snowball, Snow Job, or Winter\nSolstice?” Current Epidemiology Reports 5: 175–83.\n\n\nMakel, M. C., J. A. Plucker, and B. Hegarty. 2012. “Replications\nin Psychology Research: How Often Do They Really Occur?”\nPerspectives on Psychological Science 7 (6): 537–42. https://doi.org/10.1177/1745691612460688.\n\n\nMcManus, K. 2024. “Replication Studies in Second Language\nAcquisition Research: Definitions, Issues, Resources, and Future\nDirections: Introduction to the Special Issue.” Studies in\nSecond Language Acquisition 46 (5): 1299–319. https://doi.org/10.1017/S0272263124000652.\n\n\nMoreau, D., and K. Wiebels. 2023. “Ten Simple Rules for Designing\nand Conducting Undergraduate Replication Projects.” PLOS\nComputational Biology 19 (3): e1010957. https://doi.org/10.1371/journal.pcbi.1010957.\n\n\nNuijten, M. B., and J. R. Polanin. 2020.\n“‘Statcheck’: Automatically Detect Statistical\nReporting Inconsistencies to Increase Reproducibility of\nMeta‐analyses.” Research Synthesis Methods 11 (5):\n574–79. https://doi.org/10.1002/jrsm.1408.\n\n\nOpen Science Collaboration. 2015. “Estimating the Reproducibility\nof Psychological Science.” Science 349 (6251): aac4716.\nhttps://doi.org/10.1126/science.aac4716.\n\n\nPerry, T., R. Morris, and R. Lea. 2022. “A Decade of Replication\nStudy in Education? A Mapping Review (2011–2020).”\nEducational Research and Evaluation 27 (1-2): 12–34. https://doi.org/10.1080/13803611.2021.2022315.\n\n\nPownall, M. 2022. “Is Replication Possible for Qualitative\nResearch?” https://doi.org/10.31234/osf.io/dwxeg.\n\n\nRöseler, L., L. Kaiser, C. Doetsch, N. Klett, C. Seida, A. Schütz, and\nY. and Zhang. 2024. “The Replication Database: Documenting the\nReplicability of Psychological Science.” Journal of Open\nPsychology Data 12 (1): 8. https://doi.org/10.5334/jopd.101.\n\n\nSchöch, C. 2023. “Repetitive Research: A Conceptual Space and\nTerminology of Replication, Reproduction, Revision, Reanalysis,\nReinvestigation and Reuse in Digital Humanities.”\nInternational Journal of Digital Humanities 5 (2): 373–403. https://doi.org/10.1007/s42803-023-00073-y.\n\n\nUrminsky, O., and B. J. Dietvorst. 2024. “Taking the Full Measure:\nIntegrating Replication into Research Practice to Assess\nGeneralizability.” Journal of Consumer Research 51 (1):\n157–68. https://doi.org/10.1093/jcr/ucae007.",
    "crumbs": [
      "References"
    ]
  }
]